{"tldr_summary":"Nick Bostrom's 1998 paper 'How Long Before Superintelligence?' became the intellectual foundation for the entire AI safety field, even though it was originally just trying to forecast timelines. Bostrom analyzed hardware trends (Moore's Law was still doubling every 18 months), estimated brain computational requirements, and examined paths to superintelligence through AI, brain emulation, or biological enhancement. His central estimate landed around 2033, with a range stretching from 2000 to beyond mid-century. What made Bostrom's analysis distinctive was his focus on recursive self-improvement—once you have human-level AI, it could rapidly bootstrap itself to superintelligence in days or hours. This 'intelligence explosion' scenario terrified enough people that it spawned organizations like the Future of Humanity Institute and convinced billionaires to fund AI alignment research. Bostrom's later book 'Superintelligence' (2014) expanded these ideas and became required reading in Silicon Valley. His rigorous philosophical approach—defining terms precisely, considering multiple pathways, acknowledging uncertainty—set the template for serious AI forecasting.","criteria_definition":"An intellect much smarter than the best human brains in practically every field, including scientific creativity, general wisdom and social skills.","confidence_level":"Moderate confidence with wide range acknowledging significant uncertainty in recursive self-improvement dynamics","source_name":"How Long Before Superintelligence? - Journal of Future Studies (1998)","source_url":"https://nickbostrom.com/superintelligence","concept_keys":["superintelligence","recursive-self-improvement","intelligence-explosion","alignment"]}