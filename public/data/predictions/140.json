{"tldr_summary":"Noam Chomsky, the legendary linguist who revolutionized our understanding of human language, remains deeply skeptical that current AI approaches can lead to artificial general intelligence. His core argument: neural networks lack the structured, rule-based architecture that underlies human cognition and language. Chomsky's theory of universal grammar posits that humans have innate linguistic structures—a 'language organ' that enables children to learn any language from limited examples. Current AI systems, by contrast, are statistical pattern matchers that lack genuine understanding or the ability to generate truly novel grammatical structures. While GPT-4 can produce fluent text, Chomsky argues it's fundamentally mimicry without comprehension—a 'stochastic parrot' as some critics label it. His position implies AGI won't emerge from scaling current architectures, no matter how many parameters or training tokens you throw at them. Instead, a fundamental paradigm shift toward symbolic reasoning and structured representations would be required. This makes Chomsky one of the most prominent AGI skeptics in academia, suggesting timelines measured in many decades or longer—if current approaches can reach AGI at all. His cognitive science credentials lend weight to the critique, even as AI capabilities continue surprising experts.","criteria_definition":"AI systems achieving human-like cognition and language understanding through current neural network architectures","confidence_level":"Strong skepticism that current neural network approaches can achieve genuine intelligence or language understanding","source_name":"Chomsky interviews","source_url":"https://en.wikipedia.org/wiki/Noam_Chomsky","concept_keys":["agi","scaling-hypothesis"]}