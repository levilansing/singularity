{"tldr_summary":"Nick Bostrom's 'Superintelligence: Paths, Dangers, Strategies' became the text that launched a thousand AI safety careers when it dropped in 2014. The Oxford philosopher argues that once we crack human-level AI, superintelligence could follow 'within years to decades' through recursive self-improvement—machines designing better machines in an intelligence explosion. His timeline clusters around mid-century (2040-2100, best guess 2050) but the book's real impact was making AI risk respectable. Elon Musk tweeted it was scarier than nukes. Bill Gates called it important. The paperclip maximizer thought experiment—an AI that tiles the universe with paperclips because we forgot to specify we wanted other things too—became shorthand for alignment failure. Bostrom's scenarios range from singleton world governments to humanity's extinction, all delivered in careful philosophical prose that made existential risk sound less like sci-fi and more like a problem requiring serious technical work.","criteria_definition":"Machine superintelligence surpassing human cognitive performance across virtually all domains","confidence_level":"Substantial uncertainty acknowledged with wide probability distributions across decades","source_name":"Superintelligence: Paths Dangers Strategies - Oxford University Press (2014)","source_url":"https://en.wikipedia.org/wiki/Nick_Bostrom","concept_keys":["superintelligence","recursive-self-improvement","intelligence-explosion","alignment"]}