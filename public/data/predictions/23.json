{"tldr_summary":"Ajeya Cotra's original biological anchors framework estimated median transformative AI at 2050 by calculating evolution's total compute expenditure to produce human brains. The methodology was simultaneously rigorous and absurd: carefully counting every neuron and synapse across evolutionary history, then extrapolating when we'd match that compute artificially. It became the rationalist community's canonical AGI forecast because it was quantitative and argued its assumptions explicitly rather than vibes. The 2050 date assumed scaling laws would continue but not accelerate, algorithmic progress would be moderate, and compute prices would follow historical trends. Then GPT-3 happened, followed by GPT-4, and suddenly scaling laws were exceeding expectations. Cotra revised to 2040 by 2022, a decade closer, acknowledging that progress was outpacing her original conservative assumptions. The biological anchors approach treats AGI as inevitable once we match nature's compute bill, which critics argue ignores efficiency differences between evolution's random walk and directed engineering. But at least Cotra's framework makes falsifiable predictions we can check as compute trends evolve.","criteria_definition":"Transformative AI via biological anchors: compute matching evolution and brain development","confidence_level":"50% by 2050 initially; revised to 2040 by 2022","source_name":"Open Philanthropy Biological Anchors Report (2020)","source_url":"https://www.lesswrong.com/posts/KrJfoZzpSDpnrv9va/draft-report-on-ai-timelines","concept_keys":["transformative-ai","scaling-hypothesis","biological-anchors"]}