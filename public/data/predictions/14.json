{"tldr_summary":"Ege Erdil, a top forecaster on Metaculus with a track record of accurate predictions, throws cold water on the AGI party with a 2073 median - decades beyond the consensus. His contrarian stance argues most forecasters are anchoring on exponential extrapolations that ignore diminishing returns and fundamental barriers we haven't even identified yet. While everyone else compressed timelines from 2050 to 2030 after ChatGPT, Erdil barely budged. His argument: scaling laws will hit walls, transformers have inherent limitations, and the field systematically underestimates how hard true intelligence is. Professional forecasters typically beat experts at predictions because they account for base rates and avoid motivated reasoning, which makes Erdil's skepticism worth taking seriously. He's essentially betting that the current AI boom looks more like mobile apps in 2010 than the internet in 1995 - lots of hype, real utility, but not civilizational transformation. Someone has to be the adult in the room, and Erdil volunteered.","criteria_definition":"Transformative AI; argues most forecasters anchor on exponential extrapolations ignoring diminishing returns","confidence_level":"Conservative median; more skeptical than most forecasters","source_name":"LessWrong discussion","source_url":"https://forum.effectivealtruism.org/posts/SYtwChBTs6xkocBSP/when-do-experts-think-human-level-ai-will-be-created","concept_keys":["transformative-ai","accelerating-change","scaling-hypothesis"]}