{"tldr_summary":"Eliezer Yudkowsky, the AI safety researcher who literally invented the term \"friendly AI\" in 2001, has become the movement's most apocalyptic voice. His \"p(doom)\"—probability of existential catastrophe from AI—ranges from 10% on optimistic days to approaching 100% depending on how the question is framed. Unlike other researchers hedging with 5% or 20% estimates, Yudkowsky argues we're fundamentally unprepared for AGI's arrival sometime between 2025-2030, advocating for an immediate worldwide moratorium on frontier AI development. His position: we don't know how to align superintelligent systems, recursive self-improvement could happen extremely fast once triggered, and by the time we realize we've lost control it will be too late to course-correct. While mainstream AI researchers dismiss him as alarmist, Yudkowsky points out he's been consistently right about capability timelines since the 1990s—just ask anyone who laughed at his 2008 predictions about deep learning. His p(doom) has only increased as capabilities advance faster than alignment research. When the guy who founded the field says we're probably doomed, it's worth considering he might know something.","criteria_definition":"Artificial general intelligence capable of recursive self-improvement leading to existential catastrophe without alignment breakthroughs","confidence_level":"Very high confidence (personal p(doom) ranges from 10% to near-100% depending on context) that AGI poses existential risk","source_name":"Multiple 2024-2025 interviews","source_url":"https://en.wikipedia.org/wiki/P(doom)","concept_keys":["event-horizon","recursive-self-improvement","alignment"]}