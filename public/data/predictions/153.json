{"tldr_summary":"Roman Yampolskiy holds one of the highest P(doom) estimates in the AI safety communityâ€”nearly 100% probability that AI leads to existential catastrophe. His position appears on Wikipedia's P(doom) page alongside more moderate estimates like Geoffrey Hinton's 10-20% and Yoshua Bengio's 50%. Yampolskiy's near-certainty stems from his work on AI containment and control problems, where he's concluded that controlling superintelligent systems is fundamentally unsolvable. Unlike researchers who see alignment as difficult but tractable, Yampolskiy argues we're building something we can't control and won't be able to stop once it reaches critical capability thresholds. His book 'AI: Unexplainable, Unpredictable, Uncontrollable' lays out the case for why technical solutions to alignment are likely insufficient. The estimate puts him at the extreme pessimistic end even among AI safety researchers, most of whom cluster between 5-50%. Whether his near-certainty reflects deeper insight into control problems or excessive catastrophizing remains hotly debated, but it's consistent with his other predictions about rapid AGI timelines and massive unemployment.","criteria_definition":"Probability of existentially catastrophic outcomes from artificial intelligence including human extinction or permanent disempowerment","confidence_level":"Near-certainty expressed as 'nearly 100%' probability of existential catastrophe","source_name":"AI extinction risk estimates 2025","source_url":"https://en.wikipedia.org/wiki/P(doom)","concept_keys":["superintelligence","economic-singularity","event-horizon","alignment"]}