{"tldr_summary":"Vincent Müller and Nick Bostrom's 2016 survey of AI researchers captured a moment when AGI still felt comfortably distant: 50% probability by 2040, 90% by 2075. The survey asked experts when 'high-level machine intelligence' would match or exceed human cognitive performance across the board. Their median estimates suggested a generation or two before humanity would face this challenge—plenty of time for careful safety work and societal preparation. Fast forward to 2026, and these timelines look charmingly conservative. Current Metaculus forecasts put 50% probability around 2033, and some AI lab leaders are talking about late 2020s. The survey has become a historical artifact documenting how rapidly expert opinion shifted post-2022. What changed? GPT-3, GPT-4, scaling laws that kept delivering, and the sudden realization that 'human-level' might not require solving consciousness or replicating biological neurons—just really good pattern matching at scale. The 2016 survey represents the 'before times' when AI researchers could still imagine AGI as their grandchildren's problem, not their own career's climax.","criteria_definition":"High-level machine intelligence matching or exceeding human cognitive performance","confidence_level":"Survey-based aggregate of expert opinions; 50% confidence by 2040, 90% by 2075","source_name":"Muller & Bostrom survey (550)","source_url":"https://research.aimultiple.com/artificial-general-intelligence-singularity-timing/","concept_keys":["agi","scaling-hypothesis","alignment","survey-drift"]}