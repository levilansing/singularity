{"tldr_summary":"Geoffrey Hinton isn't just warning about AI risk in vague terms—he's putting numbers on it: 10-20% chance of human extinction within 30 years, a probability that's been creeping upward from his earlier 10% estimate. To put this in perspective, that's higher than most estimates for nuclear war this century and orders of magnitude higher than asteroid impact risk. Hinton's reasoning centers on the fundamental challenge of controlling entities smarter than us, especially once malicious actors get involved. The extinction mechanism isn't some sci-fi robot army—it's misaligned superintelligence pursuing goals incompatible with human survival, or bad actors weaponizing AI without adequate safeguards. By late 2024, Hinton was warning the risk is 'greater than ever,' suggesting his probability estimates may still be rising. The credibility boost from having a Nobel laureate issue this warning cannot be overstated. If Hinton is even half right, AI represents the single greatest existential threat humanity faces.","criteria_definition":"Singularity/extinction risk quantified at 10-20% probability within 30-year window","confidence_level":"10-20% probability estimate of extinction within 30 years, increased from earlier 10% figure","source_name":"Multiple 2023 interviews","source_url":"https://x.com/geoffreyhinton/status/1653687894534504451","concept_keys":["event-horizon"]}