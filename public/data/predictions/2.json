{"tldr_summary":"Released April 3, 2025 by Daniel Kokotajlo (ex-OpenAI), Scott Alexander, Thomas Larsen, Eli Lifland, and Romeo Dean, AI 2027 is a month-by-month timeline where everything goes sideways fast: AI struggles with adoption in mid-2025, achieves superhuman coding by March 2027, superhuman AI research by August, and full superintelligence by December. The scenario includes China stealing model weights in February 2027, whistleblowers revealing misalignment in October, and humanity collectively panicking as AI agents recursively improve themselves into godhood. Explicitly framed as one possible future for predictive accuracy not advocacy, making it either the most detailed AGI roadmap ever published or the most elaborate thought experiment in AI safety. The authors give it as their modal year while acknowledging longer median timelines when accounting for uncertainties.","criteria_definition":"Recursive self-improvement: Superhuman Coder (Mar 27) to Superhuman AI Researcher (Aug 27) to Superintelligence (Dec 27)","confidence_level":"Modal estimate (longer medians when accounting for uncertainty)","source_name":"AI 2027 (Kokotajlo et al.)","source_url":"https://ai-2027.com/","concept_keys":["superintelligence","recursive-self-improvement","economic-singularity","alignment"]}