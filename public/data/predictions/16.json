{"tldr_summary":"Paul Christiano, who led alignment research at OpenAI before founding the Alignment Research Center, gives 15% probability to transformative AI by 2030 while openly admitting he has different numbers every day. His refreshing honesty about uncertainty - half a significant figure confidence - cuts through the false precision plaguing AI predictions. Christiano's focus on slow takeoff scenarios where AI gradually automates research rather than exploding overnight colors his more measured timeline. He's one of the few giving actual probability distributions instead of point estimates, acknowledging that anyone claiming to know whether AGI arrives in 2027 or 2045 is probably fooling themselves. His 30% by 2033 represents someone deeply embedded in AI safety who recognizes the technology is advancing faster than expected while maintaining epistemic humility. The field needs more researchers willing to say I don't really know instead of confidently proclaiming dates they pulled from exponential curves and vibes.","criteria_definition":"Transformative AI; admits his numbers have half a significant figure confidence at best","confidence_level":"15% by 2030; admits different numbers every day","source_name":"EA Forum and LessWrong","source_url":"https://forum.effectivealtruism.org/posts/SYtwChBTs6xkocBSP/when-do-experts-think-human-level-ai-will-be-created","concept_keys":["transformative-ai","soft-takeoff","accelerating-change","alignment"]}