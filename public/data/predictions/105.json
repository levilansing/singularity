{"tldr_summary":"Katja Grace's blockbuster 2023 survey (published January 2024) of 2,778 AI researchers who published in top-tier venues revealed the most dramatic timeline acceleration ever recorded: the median HLMI prediction jumped from 2060 to 2047 in just one year, with 10% odds by 2027. But the real story is the existential dread baked into the numbers. Between 38% and 51% of respondents gave at least 10% probability to advanced AI causing human extinction. Even among net optimists (68.3% thought good outcomes more likely than bad), nearly half gave 5%+ odds to extinction scenarios. The survey captured AI researchers grappling with whiplash: their own technology accelerating faster than expected while their confidence in controlling it eroded. More than half expressed \"substantial\" or \"extreme\" concern about misinformation, authoritarian control, and inequality. The field disagrees on whether faster or slower progress helps, but broadly agrees safety research needs prioritization. It's the scientific equivalent of building a rocket while realizing mid-flight you forgot the brakes.","criteria_definition":"High-level machine intelligence outperforming humans in every possible task when science continues undisrupted","confidence_level":"50% confidence at 2047 median, but 38-51% give at least 10% chance to human extinction outcomes; high uncertainty reflected in wide distribution","source_name":"Thousands of AI Authors on the Future of AI - arXiv 2401.02843 (2024)","source_url":"https://arxiv.org/abs/2401.02843","concept_keys":["agi","accelerating-change","alignment","survey-drift"]}