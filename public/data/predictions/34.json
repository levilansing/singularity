{"tldr_summary":"When the world's most famous physicist warns that artificial intelligence could spell the end of humanity, people listen. Stephen Hawking's December 2014 BBC interview and co-authored op-ed didn't predict when superintelligent AI would arrive—he carefully avoided timelines—but argued that whenever it does, it represents an existential threat unless we solve the control problem first. 'Success in creating AI would be the biggest event in human history,' Hawking wrote, 'Unfortunately, it might also be the last, unless we learn how to avoid the risks.' His intervention moved AI safety from fringe concern to mainstream conversation overnight. Here was someone who'd spent decades thinking about black holes and the universe's fate, and he was worried about AI. Hawking's warnings carried extra weight because he relied on AI himself—his speech system was primitive by today's standards but kept him communicating for decades. He understood both AI's promise and peril viscerally. No timeline, but maximum gravitas.","criteria_definition":"Superintelligent AI that could potentially end human civilization if not properly controlled","confidence_level":"No specific timeline given, emphasis on existential risk rather than timing","source_name":"Co-authored article with Russell Tegmark and Wilczek (2014)","source_url":"https://en.wikipedia.org/wiki/Technological_singularity","concept_keys":["superintelligence","alignment","industry-academia-divergence"]}