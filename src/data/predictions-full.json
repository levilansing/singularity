[
  {
    "id": 1,
    "predictor_name": "AAAI Presidential Panel",
    "predictor_type": "Survey",
    "prediction_date": "2025-03-07",
    "predicted_date_low": "2025-01-01",
    "predicted_date_high": "2030-12-31",
    "predicted_date_best": "2027-01-01",
    "predicted_year_low": 2025,
    "predicted_year_high": 2030,
    "predicted_year_best": 2027,
    "prediction_type": "AGI",
    "confidence_level": "76% of 475 respondents skeptical of scaling approaches",
    "confidence_label": "Survey Says Skeptical",
    "confidence_type": "low",
    "concept_keys": [
      "agi",
      "scaling-hypothesis",
      "survey-drift"
    ],
    "criteria_definition": "AGI via scaling current approaches; 76% of 475 AI researchers say scaling alone won't yield AGI",
    "source_name": "AAAI 2025 Presidential Panel Report",
    "source_url": "https://aaai.org/wp-content/uploads/2025/03/AAAI-2025-PresPanel-Report-Digital-3.7.25.pdf",
    "headline": "Academic AI Society Polls 475 Researchers: 76% Say Just Scaling Up Won't Get Us to AGI",
    "headline_slug": "ai-society-polls-researchers-on-agi-scaling",
    "tldr_summary": "Published March 7, 2025 by AAAI Presidential Panel (chaired by Francesca Rossi with 25 AI researchers and 475 survey respondents), this report threw cold water on the scaling hypothesis: 76% of respondents said simply scaling current AI approaches will not yield AGI. The comprehensive 17-chapter report covered the future of AI research across multiple domains, but the AGI finding landed like a bomb in the middle of Silicon Valley's race to pour billions into ever-larger models. The academic consensus: we need fundamentally new approaches, not just bigger GPUs. The report comes as frontier labs bet their entire roadmaps on scaling laws continuing indefinitely.",
    "target_date": "2027-01-01T00:00:00Z"
  },
  {
    "id": 2,
    "predictor_name": "AI 2027 Project",
    "predictor_type": "Individual",
    "prediction_date": "2025-04-03",
    "predicted_date_low": "2026-03-01",
    "predicted_date_high": "2027-12-31",
    "predicted_date_best": "2027-08-01",
    "predicted_year_low": 2026,
    "predicted_year_high": 2027,
    "predicted_year_best": 2027,
    "prediction_type": "Superintelligence",
    "confidence_level": "Modal estimate (longer medians when accounting for uncertainty)",
    "confidence_label": "Confident But Hedging",
    "confidence_type": "medium",
    "concept_keys": [
      "superintelligence",
      "recursive-self-improvement",
      "economic-singularity",
      "alignment"
    ],
    "criteria_definition": "Recursive self-improvement: Superhuman Coder (Mar 27) to Superhuman AI Researcher (Aug 27) to Superintelligence (Dec 27)",
    "source_name": "AI 2027 (Kokotajlo et al.)",
    "source_url": "https://ai-2027.com/",
    "headline": "Ex-OpenAI Researcher Publishes Month-by-Month AGI Apocalypse Calendar: Superhuman Coder March, ASI December",
    "headline_slug": "openai-researcher-predicts-agi-apocalypse-timeline",
    "tldr_summary": "Released April 3, 2025 by Daniel Kokotajlo (ex-OpenAI), Scott Alexander, Thomas Larsen, Eli Lifland, and Romeo Dean, AI 2027 is a month-by-month timeline where everything goes sideways fast: AI struggles with adoption in mid-2025, achieves superhuman coding by March 2027, superhuman AI research by August, and full superintelligence by December. The scenario includes China stealing model weights in February 2027, whistleblowers revealing misalignment in October, and humanity collectively panicking as AI agents recursively improve themselves into godhood. Explicitly framed as one possible future for predictive accuracy not advocacy, making it either the most detailed AGI roadmap ever published or the most elaborate thought experiment in AI safety. The authors give it as their modal year while acknowledging longer median timelines when accounting for uncertainties.",
    "target_date": "2027-08-01T00:00:00Z"
  },
  {
    "id": 3,
    "predictor_name": "AI Frontiers",
    "predictor_type": "Survey",
    "prediction_date": "2025-10-22",
    "predicted_date_low": "2026-12-31",
    "predicted_date_high": "2030-12-31",
    "predicted_date_best": "2028-12-31",
    "predicted_year_low": 2026,
    "predicted_year_high": 2030,
    "predicted_year_best": 2028,
    "prediction_type": "AGI",
    "confidence_level": "50% by end of 2028, 80% by end of 2030 (one author estimate)",
    "confidence_label": "Probabilistically Hedging",
    "confidence_type": "medium",
    "concept_keys": [
      "agi",
      "alignment",
      "survey-drift"
    ],
    "criteria_definition": "AGI Score 95%+ across 10 cognitive abilities (knowledge, reasoning, memory, visual/auditory processing) via Cattell-Horn-Carroll framework",
    "source_name": "AI Frontiers - AGIs Last Bottlenecks",
    "source_url": "https://ai-frontiers.org/articles/agis-last-bottlenecks",
    "headline": "Researchers Grade AI on Actual Intelligence Test: GPT-4 Fails With 27%, GPT-5 Gets Gentleman's C at 57%",
    "headline_slug": "researchers-test-ai-intelligence-capabilities",
    "tldr_summary": "Published October 22, 2025 by AI Frontiers (authors Adam Khoja, Laura Hiscott, and Dan Hendrycks from Center for AI Safety), this report built a real AGI report card using 10 cognitive abilities from Cattell-Horn-Carroll intelligence theory. Current scores: GPT-4 at 27% and GPT-5 at 57%, with improvements mainly from bigger context windows and better multimodal support. The brutal finding: continual learning (long-term memory storage) scores absolute zero across all models and requires a major breakthrough, not just engineering. Other bottlenecks include visual reasoning, hallucinations, and spatial navigation. One author estimates 50% odds of hitting their 95% AGI threshold by end of 2028 and 80% by 2030, which seems optimistic given current models just barely reached passing grades. The article notes most gaps are tractable with business-as-usual research except that pesky memory problem.",
    "target_date": "2028-12-31T00:00:00Z"
  },
  {
    "id": 4,
    "predictor_name": "Matthew Barnett / Epoch AI",
    "predictor_type": "Individual",
    "prediction_date": "2024-01-09",
    "predicted_date_low": "2030-01-01",
    "predicted_date_high": "2050-12-31",
    "predicted_date_best": "2033-01-01",
    "predicted_year_low": 2030,
    "predicted_year_high": 2050,
    "predicted_year_best": 2033,
    "prediction_type": "Transformative AI",
    "confidence_level": "Depends on scaling laws and compute assumptions",
    "confidence_label": "Model Says Maybe",
    "confidence_type": "low",
    "concept_keys": [
      "transformative-ai",
      "scaling-hypothesis"
    ],
    "criteria_definition": "Transformative AI via Direct Approach framework: extrapolating training loss trends to predict when models reach transformative capability",
    "source_name": "Epoch AI blog - Direct Approach Interactive Model (Q1 2025)",
    "source_url": "https://epoch.ai/blog/direct-approach-interactive-model",
    "headline": "Epoch AI Builds Interactive Doomsday Calculator: Adjust Your Assumptions, Get Your Custom Apocalypse Date",
    "headline_slug": "epoch-ai-launches-interactive-doomsday-calculator",
    "tldr_summary": "Launched January 9, 2024 by Matthew Barnett and colleagues at Epoch AI, the Direct Approach Interactive Model lets users play with assumptions about compute scaling, algorithmic progress, and training efficiency to forecast their personal transformative AI timeline. The median lands around 2033 assuming current scaling trends hold, but the model is explicitly adjustable because nobody actually knows the right parameters. Unlike surveys asking experts to guess, this uses neural scaling laws to bound compute needed for a transformative model, then extrapolates forward based on compute trends. It's either rigorous quantitative forecasting or sophisticated garbage-in-garbage-out depending on whether scaling laws continue or hit a wall. The interactive version went live in 2024 after the original Direct Approach paper from 2023, giving everyone a chance to forecast their own singularity.",
    "target_date": "2033-01-01T00:00:00Z"
  },
  {
    "id": 5,
    "predictor_name": "Andrew Critch",
    "predictor_type": "Individual",
    "prediction_date": "2025-01-02",
    "predicted_date_low": "2025-01-01",
    "predicted_date_high": "2028-12-31",
    "predicted_date_best": "2026-12-31",
    "predicted_year_low": 2025,
    "predicted_year_high": 2028,
    "predicted_year_best": 2026,
    "prediction_type": "AGI",
    "confidence_level": "45% by end of 2026",
    "confidence_label": "Confidently Concerned",
    "confidence_type": "high",
    "concept_keys": [
      "agi",
      "alignment"
    ],
    "criteria_definition": "AGI broadly defined; assigns 45% probability by end of 2026",
    "source_name": "X/Twitter post and EA Forum discussion (2024)",
    "source_url": "https://forum.effectivealtruism.org/posts/SYtwChBTs6xkocBSP/when-do-experts-think-human-level-ai-will-be-created",
    "headline": "AI Safety Researcher Gives 45% Odds AGI Arrives by End of 2026, Presumably Spends 2025 Prepping Bunker",
    "headline_slug": "ai-safety-researcher-forecasts-agi-arrival",
    "tldr_summary": "Posted January 2, 2025 on the EA Forum, Andrew Critch estimated 45% probability of AGI by end of 2026, making him among the most aggressive forecasters in AI safety circles. This timeline appeared in a comprehensive post surveying expert AGI predictions, where Critch's near-term odds stood out even in a field that had dramatically shortened timelines post-ChatGPT. His rationale centers on autoregressive transformers already exhibiting general reasoning capabilities with gains compounding faster than anticipated. The prediction comes from someone working on cooperative AI at UC Berkeley, so this is not uninformed doomerism - this is informed doomerism. His 45% by 2026 implies roughly even odds we have AGI before the next US presidential term ends.",
    "target_date": "2026-12-31T00:00:00Z"
  },
  {
    "id": 6,
    "predictor_name": "Francois Chollet",
    "predictor_type": "Individual",
    "prediction_date": "2024-06-15",
    "predicted_date_low": "2038-01-01",
    "predicted_date_high": "2048-12-31",
    "predicted_date_best": "2043-01-01",
    "predicted_year_low": 2038,
    "predicted_year_high": 2048,
    "predicted_year_best": 2043,
    "prediction_type": "AGI",
    "confidence_level": "Skeptical of current LLM approaches achieving AGI",
    "confidence_label": "Skeptically Contrarian",
    "confidence_type": "low",
    "concept_keys": [
      "agi",
      "scaling-hypothesis",
      "industry-academia-divergence"
    ],
    "criteria_definition": "Human-level AI with broad adaptability; skeptical current path gets us there",
    "source_name": "EA Forum discussion (~2024)",
    "source_url": "https://forum.effectivealtruism.org/posts/xkNjpGNfnAYmkFz3s/on-january-1-2030-there-will-be-no-agi-and-agi-will-still",
    "headline": "Keras Creator Throws Shade at AGI Hype: Current LLMs Won't Get Us There Before 2040s",
    "headline_slug": "keras-creator-challenges-agi-hype-timelines",
    "tldr_summary": "Francois Chollet, creator of Keras and the ARC benchmark for measuring general intelligence, argues LLMs are sophisticated memorization engines that lack genuine reasoning and generalization. His timeline of 2038-2048 reflects deep skepticism that scaling transformers solves abstraction and world modeling. Chollet's ARC benchmark remains largely unsolved even by frontier models like GPT-4 and Claude, which he considers evidence we're not as close as the hype suggests. His position: current architectures hit fundamental limits around reasoning and adaptability that require entirely new approaches, not just more parameters. This puts him firmly in the skeptic camp, predicting AGI two decades out while others cluster around 2027-2033. The EA Forum post summarizing expert predictions positioned him as a prominent counterweight to accelerationist timelines.",
    "target_date": "2043-01-01T00:00:00Z"
  },
  {
    "id": 7,
    "predictor_name": "Gary Marcus",
    "predictor_type": "Individual",
    "prediction_date": "2023-01-22",
    "predicted_date_low": "2060-01-01",
    "predicted_date_high": "2100-12-31",
    "predicted_date_best": "2080-01-01",
    "predicted_year_low": 2060,
    "predicted_year_high": 2100,
    "predicted_year_best": 2080,
    "prediction_type": "AGI",
    "confidence_level": "Conditionally optimistic (requires abandoning LLMs for neurosymbolic AI)",
    "confidence_label": "Skeptically Optimistic",
    "confidence_type": "low",
    "concept_keys": [
      "agi",
      "scaling-hypothesis",
      "industry-academia-divergence"
    ],
    "criteria_definition": "Flexible general intelligence with resourcefulness and reliability comparable to humans; requires neurosymbolic approaches not scaled LLMs",
    "source_name": "Marcus on AI Substack (2024-2025)",
    "source_url": "https://garymarcus.substack.com/p/agi-will-not-happen-in-your-lifetime",
    "headline": "NYU Professor Predicts AGI Takes 75 Years, Calls LLMs Bullshit at Scale, Elon Still Hasn't Responded to Bet",
    "headline_slug": "gary-marcus-calls-out-llm-limitations",
    "tldr_summary": "Published January 22, 2023, Gary Marcus predicts AGI quite possibly before the end of this century, meaning you will be dead first. In conversation with Grady Booch, Marcus characterizes modern LLMs as systems that demonstrably produce bullshit at scale and argues the field wastes resources on an approach that probably is not on the right path. He offered Elon Musk a bet defining AGI as intelligence with resourcefulness and reliability comparable to humans, explicitly ruling out today's LLMs (still no word from Elon). Marcus says we need to pivot to neurosymbolic AI that combines neural networks with symbolic reasoning. Booch notes evolution took 300 million years to develop human cognition and it is never just a matter of software. The article title is sarcastic - Marcus does think AGI will eventually happen, just not via current methods and not in time for anyone's retirement party.",
    "target_date": "2080-01-01T00:00:00Z"
  },
  {
    "id": 8,
    "predictor_name": "Louis Rosenberg",
    "predictor_type": "Individual",
    "prediction_date": "2024-09-15",
    "predicted_date_low": "2028-01-01",
    "predicted_date_high": "2032-12-31",
    "predicted_date_best": "2030-01-01",
    "predicted_year_low": 2028,
    "predicted_year_high": 2032,
    "predicted_year_best": 2030,
    "prediction_type": "Singularity",
    "confidence_level": "Moderate (revised from earlier 2050 estimate)",
    "confidence_label": "Revised Upward Dramatically",
    "confidence_type": "medium",
    "concept_keys": [
      "event-horizon"
    ],
    "criteria_definition": "Singularity: AI exceeding human abilities broadly across domains",
    "source_name": "AIMultiple analysis",
    "source_url": "https://research.aimultiple.com/artificial-general-intelligence-singularity-timing/",
    "headline": "VR Pioneer Slashes Singularity Estimate From 2050 to 2030 After Watching GPT-4, Calls It Course Correction",
    "headline_slug": "vr-pioneer-accelerates-singularity-prediction",
    "tldr_summary": "Louis Rosenberg, who built the first functional augmented reality system at Air Force Research Lab, dramatically revised his singularity prediction from around 2050 down to 2030 after observing post-2020 AI progress. His original conservative estimate assumed AGI required breakthroughs we couldn't envision, hedging toward later dates. Then GPT-3 and GPT-4 happened and he pulled his timeline 20 years closer. This revision exemplifies the field-wide timeline compression following the transformer revolution - what seemed like science fiction in 2019 became engineering timelines by 2024. Rosenberg's definition focuses on AI exceeding human abilities broadly across domains rather than philosophical questions about consciousness. His 20-year acceleration represents one of the largest expert timeline revisions on record, though whether this reflects genuine insight or hype contagion remains hotly debated.",
    "target_date": "2030-01-01T00:00:00Z"
  },
  {
    "id": 9,
    "predictor_name": "Ray Kurzweil",
    "predictor_type": "Individual",
    "prediction_date": "2024-06-15",
    "predicted_date_low": "2029-01-01",
    "predicted_date_high": "2029-12-31",
    "predicted_date_best": "2029-12-31",
    "predicted_year_low": 2029,
    "predicted_year_high": 2029,
    "predicted_year_best": 2029,
    "prediction_type": "AGI",
    "confidence_level": "High (refusing to budge since 1999)",
    "confidence_label": "Stubbornly Consistent",
    "confidence_type": "high",
    "concept_keys": [
      "agi",
      "accelerating-change",
      "turing-test",
      "industry-academia-divergence"
    ],
    "criteria_definition": "AGI: machines passing Turing test; computational power reaching threshold of human brain (trillion operations/sec)",
    "source_name": "The Singularity Is Nearer book and TED Talk (2024)",
    "source_url": "https://www.popularmechanics.com/science/a70171717/2045-singularity-ray-kurzweil-predictions/",
    "headline": "76-Year-Old Kurzweil Publishes 'Singularity Is Nearer,' Doubles Down on Same 2029 Prediction From 25 Years Ago",
    "headline_slug": "kurzweil-doubles-down-on-2029-ai-milestone",
    "tldr_summary": "Published in his 2024 book The Singularity Is Nearer, Ray Kurzweil maintains his signature 2029 AGI prediction made way back in 1999. Now 76, Google's director of engineering insists human-level AI arrives when technology hits a trillion calculations per second - which he pegs to 2029. His track record on narrow predictions (smartphone ubiquity, computer vision progress) lends credibility, though critics note his timeline hasn't budged despite decades of shifting AI landscapes. Kurzweil's methodology relies on his Law of Accelerating Returns: exponential compute trends make AGI inevitable by 2029. In TED talks and interviews promoting the book, he says he's sticking with his five years prediction. The man who correctly forecast the death of the Soviet Union and rise of the internet now promises AGI arrives on schedule regardless of setbacks, skeptics, or AI winters. His refusal to adjust the date is either prophetic conviction or stubborn commitment to a 25-year-old guess.",
    "target_date": "2029-12-31T00:00:00Z"
  },
  {
    "id": 10,
    "predictor_name": "Ray Kurzweil",
    "predictor_type": "Individual",
    "prediction_date": "2024-06-15",
    "predicted_date_low": "2045-01-01",
    "predicted_date_high": "2045-12-31",
    "predicted_date_best": "2045-12-31",
    "predicted_year_low": 2045,
    "predicted_year_high": 2045,
    "predicted_year_best": 2045,
    "prediction_type": "Singularity",
    "confidence_level": "Certain (predicting since 2005)",
    "confidence_label": "Transhumanist Certain",
    "confidence_type": "certain",
    "concept_keys": [
      "event-horizon",
      "industry-academia-divergence"
    ],
    "criteria_definition": "Singularity: human-AI merger via brain nanobots; millionfold intelligence expansion; Epoch 5",
    "source_name": "The Singularity Is Nearer book and Science Friday interview (2024)",
    "source_url": "https://www.popularmechanics.com/science/a70171717/2045-singularity-ray-kurzweil-predictions/",
    "headline": "Kurzweil Reaffirms 2045 Brain-Nanobot Merger: Your Neocortex Gets Cloud Backup, Immortality Included",
    "headline_slug": "kurzweil-predicts-brain-nanobot-merger",
    "tldr_summary": "Ray Kurzweil's 2045 singularity prediction remains unchanged from his 2005 book: human brains will merge with AI through nanobots inserted into capillaries, creating brain-machine interfaces that multiply intelligence a millionfold. We'll be a combination of our natural intelligence and our cybernetic intelligence, all rolled into one, he promises. The Singularity Is Nearer (2024) reaffirms every detail of his original vision - nanobots, immortality through medical nanotech, transcendence of biological limits. Critics call it science fiction; Kurzweil calls it extrapolation. His 2045 date represents Epoch 5 in his framework where machine intelligence becomes infinitely more powerful than all human intelligence combined. The pitch hasn't changed in 20 years, just the distance to the goalpost. Whether Kurzweil lives to see his merger (he'd be 97) depends on whether his anti-aging protocols work as advertised.",
    "target_date": "2045-12-31T00:00:00Z"
  },
  {
    "id": 11,
    "predictor_name": "Tom Davidson",
    "predictor_type": "Individual",
    "prediction_date": "2024-05-15",
    "predicted_date_low": "2027-01-01",
    "predicted_date_high": "2100-12-31",
    "predicted_date_best": "2040-01-01",
    "predicted_year_low": 2027,
    "predicted_year_high": 2100,
    "predicted_year_best": 2040,
    "prediction_type": "Transformative AI",
    "confidence_level": "Model-based estimate with wide uncertainty",
    "confidence_label": "Modeled Uncertainty",
    "confidence_type": "low",
    "concept_keys": [
      "transformative-ai",
      "scaling-hypothesis",
      "economic-singularity",
      "alignment",
      "industry-academia-divergence"
    ],
    "criteria_definition": "Transformative AI via compute-centric framework; uses GWP and compute trends to model labor automation timelines",
    "source_name": "Open Philanthropy takeoff speeds model - Alignment Forum",
    "source_url": "https://www.alignmentforum.org/posts/Gc9FGtdXhK9sCSEYu/what-a-compute-centric-framework-says-about-ai-takeoff",
    "headline": "Open Philanthropy Researcher Models AI Timelines With Economics, Gets 2040 Plus or Minus Everything",
    "headline_slug": "tom-davidson-ai-timelines-economic-model",
    "tldr_summary": "Tom Davidson built a compute-centric framework that treats transformative AI as an economic modeling problem: when does compute scaling enable full labor automation? The model's best guess lands at 2040, but the confidence intervals are hilariously wide - the aggressive preset hits 2027 while the conservative preset says never. This captures the field's fundamental uncertainty: if you believe scaling laws continue indefinitely, AGI is around the corner; if you think we hit diminishing returns, it could be centuries away. Davidson's framework is rigorous quantitative forecasting that essentially says we're parameterizing our ignorance with math. Open Philanthropy uses this model to allocate hundreds of millions in AI safety funding, which means someone's billion-dollar decisions rest on assumptions about compute trends nobody can verify.",
    "target_date": "2040-01-01T00:00:00Z"
  },
  {
    "id": 12,
    "predictor_name": "Yann LeCun",
    "predictor_type": "Individual",
    "prediction_date": "2024-08-15",
    "predicted_date_low": "2035-01-01",
    "predicted_date_high": "2060-12-31",
    "predicted_date_best": "2045-01-01",
    "predicted_year_low": 2035,
    "predicted_year_high": 2060,
    "predicted_year_best": 2045,
    "prediction_type": "AGI",
    "confidence_level": "Skeptical of near-term AGI; decades away",
    "confidence_label": "Skeptically Dismissive",
    "confidence_type": "low",
    "concept_keys": [
      "agi",
      "accelerating-change",
      "scaling-hypothesis",
      "industry-academia-divergence"
    ],
    "criteria_definition": "AGI requires world models, self-supervised learning, and cognitive architectures that current LLMs lack; proposes Advanced Machine Intelligence instead",
    "source_name": "AI Business and CNBC interviews (2024)",
    "source_url": "https://aibusiness.com/responsible-ai/lecun-debunks-agi-hype-says-it-is-decades-away",
    "headline": "Meta's Chief AI Scientist Calls AGI Hype Complete BS, Says Decades Away Via New Architecture",
    "headline_slug": "yann-lecun-debunks-agi-hype",
    "tldr_summary": "Yann LeCun stands as the industry's loudest AGI skeptic, calling current hype complete BS and arguing LLMs fundamentally lack world models and true understanding. As Meta's chief AI scientist and a Turing Award winner, he has the credentials to be contrarian. LeCun proposes Advanced Machine Intelligence instead of AGI, estimates it's decades away, and doubts autoregressive transformers will ever get us there. His vision requires new architectures with self-supervised learning that build internal world models - basically admitting we need to go back to the drawing board. While other Turing Award winners like Hinton and Bengio shortened their timelines to 5-20 years, LeCun dug in his heels at 2035-2060. He's either the only one seeing clearly or the only one missing the exponential curve. His 2045 estimate puts AGI roughly when flying cars were supposed to arrive.",
    "target_date": "2045-01-01T00:00:00Z"
  },
  {
    "id": 13,
    "predictor_name": "Daniel Kokotajlo",
    "predictor_type": "Individual",
    "prediction_date": "2023-06-15",
    "predicted_date_low": "2025-01-01",
    "predicted_date_high": "2030-12-31",
    "predicted_date_best": "2027-01-01",
    "predicted_year_low": 2025,
    "predicted_year_high": 2030,
    "predicted_year_best": 2027,
    "prediction_type": "Transformative AI",
    "confidence_level": "Median 2027 estimate",
    "confidence_label": "Insider Information Confident",
    "confidence_type": "high",
    "concept_keys": [
      "transformative-ai",
      "scaling-hypothesis",
      "economic-singularity",
      "alignment"
    ],
    "criteria_definition": "Transformative AI; radical economic transformation and automation",
    "source_name": "LessWrong discussion and AI 2027 project",
    "source_url": "https://ai-2027.com/",
    "headline": "Ex-OpenAI Governance Researcher Rage-Quits Over Safety, Predicts TAI by 2027 Anyway",
    "headline_slug": "daniel-kokotajlo-predicts-tai-by-2027",
    "tldr_summary": "Daniel Kokotajlo left OpenAI's governance team and immediately launched AI 2027, a project centered on transformative AI arriving within four years of his departure. His insider perspective makes this especially unsettling - he saw OpenAI's internal roadmaps and scaling curves, concluded AGI was imminent, then decided the company wasn't taking safety seriously enough to stick around. So he quit and started publicly warning that we're racing toward capabilities nobody can control. His 2027 median represents what someone with direct access to frontier lab progress thinks is realistic, which is either incredibly informed or incredibly alarming. Probably both. The project argues companies are building toward AGI with safety as an afterthought, and Kokotajlo's resignation adds credibility to the urgency. When insiders leave cushy jobs to sound the alarm, you listen.",
    "target_date": "2027-01-01T00:00:00Z"
  },
  {
    "id": 14,
    "predictor_name": "Ege Erdil",
    "predictor_type": "Individual",
    "prediction_date": "2023-08-20",
    "predicted_date_low": "2040-01-01",
    "predicted_date_high": "2120-12-31",
    "predicted_date_best": "2073-01-01",
    "predicted_year_low": 2040,
    "predicted_year_high": 2120,
    "predicted_year_best": 2073,
    "prediction_type": "Transformative AI",
    "confidence_level": "Conservative median; more skeptical than most forecasters",
    "confidence_label": "Contrarian Skeptic",
    "confidence_type": "low",
    "concept_keys": [
      "transformative-ai",
      "accelerating-change",
      "scaling-hypothesis"
    ],
    "criteria_definition": "Transformative AI; argues most forecasters anchor on exponential extrapolations ignoring diminishing returns",
    "source_name": "LessWrong discussion",
    "source_url": "https://forum.effectivealtruism.org/posts/SYtwChBTs6xkocBSP/when-do-experts-think-human-level-ai-will-be-created",
    "headline": "Superforecaster Bucks AGI Hype Entirely: Not Until 2073, Scaling Hits Wall Hard",
    "headline_slug": "ege-erdil-challenges-agi-timeline",
    "tldr_summary": "Ege Erdil, a top forecaster on Metaculus with a track record of accurate predictions, throws cold water on the AGI party with a 2073 median - decades beyond the consensus. His contrarian stance argues most forecasters are anchoring on exponential extrapolations that ignore diminishing returns and fundamental barriers we haven't even identified yet. While everyone else compressed timelines from 2050 to 2030 after ChatGPT, Erdil barely budged. His argument: scaling laws will hit walls, transformers have inherent limitations, and the field systematically underestimates how hard true intelligence is. Professional forecasters typically beat experts at predictions because they account for base rates and avoid motivated reasoning, which makes Erdil's skepticism worth taking seriously. He's essentially betting that the current AI boom looks more like mobile apps in 2010 than the internet in 1995 - lots of hype, real utility, but not civilizational transformation. Someone has to be the adult in the room, and Erdil volunteered.",
    "target_date": "2073-01-01T00:00:00Z"
  },
  {
    "id": 15,
    "predictor_name": "Jürgen Schmidhuber",
    "predictor_type": "Individual",
    "prediction_date": "2023-06-10",
    "predicted_date_low": "2048-01-01",
    "predicted_date_high": "2050-12-31",
    "predicted_date_best": "2050-01-01",
    "predicted_year_low": 2048,
    "predicted_year_high": 2050,
    "predicted_year_best": 2050,
    "prediction_type": "Singularity",
    "confidence_level": "Moderate estimate; notes singularity timeline might be perception bias",
    "confidence_label": "Philosophically Uncertain",
    "confidence_type": "low",
    "concept_keys": [
      "event-horizon",
      "scaling-hypothesis"
    ],
    "criteria_definition": "Singularity: AGI emergence with frequency of notable events appearing to accelerate",
    "source_name": "Various interviews (~2023)",
    "source_url": "https://en.wikipedia.org/wiki/Technological_singularity",
    "headline": "LSTM Inventor Warns Singularity Predictions Might Just Be Humans Thinking They're Special",
    "headline_slug": "j-rgen-schmidhuber-warns-on-singularity-predictions",
    "tldr_summary": "Jürgen Schmidhuber, who invented LSTMs (the neural architecture behind early language models), suggests singularity predictions might suffer from temporal parochialism - every generation thinks they're living through history's most important moment. He notes the frequency of notable events seems to accelerate as we approach the present, but that's partly our near-sighted view privileging recent developments. Older predictions from the 1970s saying AGI by 1985 look ridiculous now; will 2027 predictions look equally silly by 2040? Despite this metacognitive caution, Schmidhuber still puts the singularity around 2050 and muses about AI eventually colonizing the solar system, then galaxy, then universe over tens of billions of years. His philosophical humility about timeline predictions combined with cosmic-scale speculation captures the field's tension between epistemological modesty and transhumanist ambition. Maybe we're experiencing something unprecedented, or maybe we're just the latest generation to think so.",
    "target_date": "2050-01-01T00:00:00Z"
  },
  {
    "id": 16,
    "predictor_name": "Paul Christiano",
    "predictor_type": "Individual",
    "prediction_date": "2023-09-12",
    "predicted_date_low": "2028-01-01",
    "predicted_date_high": "2045-12-31",
    "predicted_date_best": "2033-01-01",
    "predicted_year_low": 2028,
    "predicted_year_high": 2045,
    "predicted_year_best": 2033,
    "prediction_type": "Transformative AI",
    "confidence_level": "15% by 2030; admits different numbers every day",
    "confidence_label": "Cautiously Uncertain",
    "confidence_type": "none",
    "concept_keys": [
      "transformative-ai",
      "soft-takeoff",
      "accelerating-change",
      "alignment"
    ],
    "criteria_definition": "Transformative AI; admits his numbers have half a significant figure confidence at best",
    "source_name": "EA Forum and LessWrong",
    "source_url": "https://forum.effectivealtruism.org/posts/SYtwChBTs6xkocBSP/when-do-experts-think-human-level-ai-will-be-created",
    "headline": "Alignment Researcher Admits His AGI Predictions Have Half a Significant Figure of Confidence",
    "headline_slug": "ai-researcher-paul-christianos-probability-puzzle",
    "tldr_summary": "Paul Christiano, who led alignment research at OpenAI before founding the Alignment Research Center, gives 15% probability to transformative AI by 2030 while openly admitting he has different numbers every day. His refreshing honesty about uncertainty - half a significant figure confidence - cuts through the false precision plaguing AI predictions. Christiano's focus on slow takeoff scenarios where AI gradually automates research rather than exploding overnight colors his more measured timeline. He's one of the few giving actual probability distributions instead of point estimates, acknowledging that anyone claiming to know whether AGI arrives in 2027 or 2045 is probably fooling themselves. His 30% by 2033 represents someone deeply embedded in AI safety who recognizes the technology is advancing faster than expected while maintaining epistemic humility. The field needs more researchers willing to say I don't really know instead of confidently proclaiming dates they pulled from exponential curves and vibes.",
    "target_date": "2033-01-01T00:00:00Z"
  },
  {
    "id": 17,
    "predictor_name": "Shane Legg",
    "predictor_type": "Individual",
    "prediction_date": "2023-11-08",
    "predicted_date_low": "2025-01-01",
    "predicted_date_high": "2037-12-31",
    "predicted_date_best": "2028-01-01",
    "predicted_year_low": 2025,
    "predicted_year_high": 2037,
    "predicted_year_best": 2028,
    "prediction_type": "AGI",
    "confidence_level": "50% by 2028; 80% by 2037; maintained since 2011",
    "confidence_label": "Stubbornly Consistent",
    "confidence_type": "high",
    "concept_keys": [
      "agi",
      "scaling-hypothesis",
      "industry-academia-divergence"
    ],
    "criteria_definition": "Minimal AGI: artificial agent reliably performing full range of cognitive tasks of average human; coined the term AGI",
    "source_name": "DeepMind official podcast (late 2023)",
    "source_url": "https://80000hours.org/2025/03/when-do-experts-expect-agi-to-arrive/",
    "headline": "DeepMind Co-Founder Who Coined AGI Gives 50-50 Odds for 2028, Held Line Since 2011",
    "headline_slug": "deepmind-co-founders-consistent-agi-prediction",
    "tldr_summary": "Shane Legg, who literally coined the term artificial general intelligence and co-founded DeepMind, has maintained 50% odds on AGI by 2028 since way back in 2011. His consistency across 12 years of hype cycles and AI winters lends serious credibility - this is not someone adjusting forecasts based on headlines. As DeepMind's chief AGI scientist working on Gemini, he has inside knowledge of capability scaling and architectural limitations. Legg's definition is concrete: an AI that reliably handles the full range of human cognitive tasks, not philosophical questions about consciousness. His 80% by 2037 upper bound suggests even if 2028 misses, he expects AGI within 15 years maximum. The fact that someone this embedded in frontier AI research has barely moved his timeline despite AlphaGo, GPT-3, and everything since suggests either remarkable foresight in 2011 or stubborn commitment to an old prediction. Either way, we're close enough to 2028 to find out if the man who named AGI can predict it.",
    "target_date": "2028-01-01T00:00:00Z"
  },
  {
    "id": 18,
    "predictor_name": "Yoshua Bengio",
    "predictor_type": "Individual",
    "prediction_date": "2023-06-24",
    "predicted_date_low": "2028-01-01",
    "predicted_date_high": "2043-12-31",
    "predicted_date_best": "2035-01-01",
    "predicted_year_low": 2028,
    "predicted_year_high": 2043,
    "predicted_year_best": 2035,
    "prediction_type": "Superintelligence",
    "confidence_level": "95% confidence within 5-20 years; shortened from decades or centuries",
    "confidence_label": "High Confidence Alarmed",
    "confidence_type": "certain",
    "concept_keys": [
      "superintelligence",
      "recursive-self-improvement",
      "alignment",
      "industry-academia-divergence"
    ],
    "criteria_definition": "Superhuman intelligence emergence; 95% confidence interval 5-20 years post-ChatGPT",
    "source_name": "FAQ on Catastrophic AI Risks - yoshuabengio.org (2023)",
    "source_url": "https://yoshuabengio.org/2023/06/24/faq-on-catastrophic-ai-risks/",
    "headline": "Deep Learning Pioneer Bengio Slashes Timeline From Decades to 5-20 Years With 95% Confidence",
    "headline_slug": "bengio-slashes-superintelligence-timeline",
    "tldr_summary": "Yoshua Bengio, Turing Award winner and deep learning pioneer, compressed his superintelligence timeline from decades or centuries down to 5-20 years with 95% confidence after watching GPT-4 and friends. This represents one of the most dramatic timeline revisions from a founding father of modern AI. Bengio's pivot from dismissing AI risk to leading safety research and radically shortening estimates shows how ChatGPT scrambled even the experts' priors. His warning: once AGI emerges, superintelligence could follow within months through recursive self-improvement, not years. The man who helped create deep learning now spends his time warning about its potential to kill everyone, which should probably concern us. His 95% confidence interval spanning 2028-2043 captures genuine uncertainty about exact timing while maintaining alarm about the narrow window. When the person who literally invented the technology says we might have 5 years, that's not doomerism - that's the engineer reading the blueprints.",
    "target_date": "2035-01-01T00:00:00Z"
  },
  {
    "id": 19,
    "predictor_name": "Ajeya Cotra",
    "predictor_type": "Individual",
    "prediction_date": "2022-06-15",
    "predicted_date_low": "2030-01-01",
    "predicted_date_high": "2070-12-31",
    "predicted_date_best": "2040-01-01",
    "predicted_year_low": 2030,
    "predicted_year_high": 2070,
    "predicted_year_best": 2040,
    "prediction_type": "Transformative AI",
    "confidence_level": "50% by 2040 (revised from 2050); compute-based analysis",
    "confidence_label": "Analytically Hedging",
    "confidence_type": "medium",
    "concept_keys": [
      "transformative-ai",
      "scaling-hypothesis",
      "biological-anchors",
      "economic-singularity"
    ],
    "criteria_definition": "Transformative AI via biological anchors: estimates compute to match evolutionary and neural development",
    "source_name": "Open Philanthropy updated report (2022)",
    "source_url": "https://80000hours.org/2025/03/when-do-experts-expect-agi-to-arrive/",
    "headline": "AI Safety Researcher Counts All Evolution's Neurons, Arrives at 2040 for Human-Level AI",
    "headline_slug": "neurons-and-ai-timelines",
    "tldr_summary": "Ajeya Cotra's biological anchors approach calculates how much compute evolution spent developing human brains, then extrapolates when we'll hit that threshold artificially. Her original 2050 median for transformative AI became the rationalist community's reference forecast, grounding speculative timelines in actual transistor counts and synaptic calculations. Then GPT-4 happened and scaling laws exceeded expectations, so she moved it to 2040 - a decade closer. The methodology treats AGI as inevitable once you match nature's compute bill, which is either rigorous quantitative reasoning or the most elaborate anchoring bias in forecasting history. Critics note evolution had billions of years and massive parallelism while being hilariously inefficient, so maybe copying its homework isn't the right approach. But at least Cotra's framework makes explicit assumptions you can argue about instead of vibes-based timelines. Her revision from 2050 to 2040 foreshadowed the timeline compression that would accelerate dramatically after ChatGPT's release.",
    "target_date": "2040-01-01T00:00:00Z"
  },
  {
    "id": 20,
    "predictor_name": "Connor Leahy",
    "predictor_type": "Individual",
    "prediction_date": "2022-10-18",
    "predicted_date_low": "2025-01-01",
    "predicted_date_high": "2050-12-31",
    "predicted_date_best": "2030-01-01",
    "predicted_year_low": 2025,
    "predicted_year_high": 2050,
    "predicted_year_best": 2030,
    "prediction_type": "AGI",
    "confidence_level": "50% by 2030; entire company operates under assumption",
    "confidence_label": "Urgently Confident",
    "confidence_type": "high",
    "concept_keys": [
      "agi",
      "alignment"
    ],
    "criteria_definition": "AGI arrival with alignment unsolved; all Conjecture employees expected AGI before 2035",
    "source_name": "YouTube and Conjecture blog",
    "source_url": "https://sifted.eu/articles/connor-leahy-ai-alignment",
    "headline": "Alignment Startup CEO Builds Entire Company on Premise We Have Less Than Decade to Solve AGI",
    "headline_slug": "alignment-startup-ceos-urgent-mission",
    "tldr_summary": "Connor Leahy founded Conjecture specifically to work on AI alignment under the operating assumption that AGI arrives before 2035, with 50-50 odds by 2030. This isn't abstract forecasting - his company's entire business model and hiring pitch assumes we have less than a decade to solve alignment before superintelligent systems emerge. That level of urgency either indicates access to information suggesting rapid capability gains or selection bias where people who think AGI is imminent self-select into AI safety startups. Probably both. Leahy's timeline drives the race to AGI safety mentality: if we're this close, incremental research won't cut it - we need breakthroughs now. His aggressive forecast positions Conjecture in the maximally alarming camp alongside Kokotajlo and others who left frontier labs concerned about pace versus safety. When someone stakes their career and company on AGI arriving within years, they're either remarkably informed or remarkably committed to a belief. The scary part is we'll find out which pretty soon.",
    "target_date": "2030-01-01T00:00:00Z"
  },
  {
    "id": 21,
    "predictor_name": "Grace et al. Survey",
    "predictor_type": "Survey",
    "prediction_date": "2022-08-03",
    "predicted_date_low": "2035-01-01",
    "predicted_date_high": "2090-12-31",
    "predicted_date_best": "2059-01-01",
    "predicted_year_low": 2035,
    "predicted_year_high": 2090,
    "predicted_year_best": 2059,
    "prediction_type": "HLMI",
    "confidence_level": "50% within 45 years (median from 2022 survey)",
    "confidence_label": "Survey Says Cautious",
    "confidence_type": "medium",
    "concept_keys": [
      "agi",
      "scaling-hypothesis",
      "survey-drift"
    ],
    "criteria_definition": "HLMI: unaided machines accomplish every task better and cheaper than human workers; 352 researchers surveyed",
    "source_name": "AI Impacts 2022 Expert Survey on Progress in AI",
    "source_url": "https://wiki.aiimpacts.org/doku.php?id=ai_timelines:predictions_of_human-level_ai_timelines:ai_timeline_surveys:2022_expert_survey_on_progress_in_ai",
    "headline": "2022 Survey of 352 ML Researchers Predicts 2059, Then ChatGPT Dropped and Everyone Panicked",
    "headline_slug": "machine-learning-researchers-survey-on-ai-timelines",
    "tldr_summary": "Katja Grace's 2022 survey of 352 machine learning researchers found median HLMI (High-Level Machine Intelligence) predictions at 2059 - nearly four decades out. Just five months later, ChatGPT launched and made those timelines look quaint. The survey captured pre-GPT conventional wisdom: AGI was distant, scaling laws were interesting but not transformative, and we had plenty of time to figure things out. Researchers estimated 50% odds within 45 years and 90% within a century, suggesting genuine uncertainty about whether AGI was decades or lifetimes away. The geographic split was fascinating: Asian researchers predicted 2046 while North Americans said 2089, a 44-year gap. Then late 2022 happened and everyone scrambled to update their priors. Grace's follow-up survey in 2023 compressed the median 13 years from 2060 to 2047, the fastest timeline shift ever recorded. The 2022 survey serves as a time capsule of pre-transformer-hype consensus, when AGI felt safely distant and we could all relax. Those were simpler times.",
    "target_date": "2059-01-01T00:00:00Z"
  },
  {
    "id": 22,
    "predictor_name": "Holden Karnofsky",
    "predictor_type": "Individual",
    "prediction_date": "2021-09-14",
    "predicted_date_low": "2036-01-01",
    "predicted_date_high": "2100-12-31",
    "predicted_date_best": "2060-01-01",
    "predicted_year_low": 2036,
    "predicted_year_high": 2100,
    "predicted_year_best": 2060,
    "prediction_type": "Transformative AI",
    "confidence_level": "Greater than 10% by 2036, around 50% by 2060",
    "confidence_label": "Cautiously Alarmed",
    "confidence_type": "medium",
    "concept_keys": [
      "transformative-ai",
      "scaling-hypothesis",
      "economic-singularity",
      "alignment"
    ],
    "criteria_definition": "Transformative AI: AI that transforms economy and world as much as Industrial Revolution",
    "source_name": "Cold Takes blog - Where AI Forecasting Stands Today (2021)",
    "source_url": "https://www.cold-takes.com/where-ai-forecasting-stands-today/",
    "headline": "Open Philanthropy CEO Warns 10% Chance Civilization Transforms in 15 Years, We're Radically Unprepared",
    "headline_slug": "open-philanthropy-ceo-warns-of-ai-transformation-risks",
    "tldr_summary": "Holden Karnofsky, who allocates hundreds of millions in AI safety funding at Open Philanthropy, put greater than 10% odds on transformative AI by 2036 and roughly 50% by 2060. Writing this before ChatGPT in his Cold Takes blog series, he argued we're radically unprepared for AI timelines much shorter than conventional wisdom suggested. When someone controlling massive philanthropic capital makes bets like this, it signals something important: the people with money on the line think near-term transformative AI is plausible enough to restructure their entire giving strategy. Karnofsky's framing - transformative as the Industrial Revolution - helps cut through AGI definition debates. We might argue about consciousness and understanding, but Industrial-Revolution-scale economic transformation is pretty unambiguous. His greater than 10% probability for 2036 was aggressive for 2021, before language models made everyone update. The kicker: he wrote this pre-ChatGPT, meaning his timelines were shortened by 2024 trends that came after. Open Philanthropy's funding decisions now assume transformative AI might arrive while current grad students are still mid-career.",
    "target_date": "2060-01-01T00:00:00Z"
  },
  {
    "id": 23,
    "predictor_name": "Ajeya Cotra",
    "predictor_type": "Individual",
    "prediction_date": "2020-12-10",
    "predicted_date_low": "2030-01-01",
    "predicted_date_high": "2090-12-31",
    "predicted_date_best": "2050-01-01",
    "predicted_year_low": 2030,
    "predicted_year_high": 2090,
    "predicted_year_best": 2050,
    "prediction_type": "Transformative AI",
    "confidence_level": "50% by 2050 initially; revised to 2040 by 2022",
    "confidence_label": "Biologically Anchored",
    "confidence_type": "medium",
    "concept_keys": [
      "transformative-ai",
      "scaling-hypothesis",
      "biological-anchors"
    ],
    "criteria_definition": "Transformative AI via biological anchors: compute matching evolution and brain development",
    "source_name": "Open Philanthropy Biological Anchors Report (2020)",
    "source_url": "https://www.lesswrong.com/posts/KrJfoZzpSDpnrv9va/draft-report-on-ai-timelines",
    "headline": "Researcher Calculates Evolution's Compute Budget, Predicts 2050 AGI, Then Revises to 2040",
    "headline_slug": "researcher-calculates-biological-anchors-for-agi",
    "tldr_summary": "Ajeya Cotra's original biological anchors framework estimated median transformative AI at 2050 by calculating evolution's total compute expenditure to produce human brains. The methodology was simultaneously rigorous and absurd: carefully counting every neuron and synapse across evolutionary history, then extrapolating when we'd match that compute artificially. It became the rationalist community's canonical AGI forecast because it was quantitative and argued its assumptions explicitly rather than vibes. The 2050 date assumed scaling laws would continue but not accelerate, algorithmic progress would be moderate, and compute prices would follow historical trends. Then GPT-3 happened, followed by GPT-4, and suddenly scaling laws were exceeding expectations. Cotra revised to 2040 by 2022, a decade closer, acknowledging that progress was outpacing her original conservative assumptions. The biological anchors approach treats AGI as inevitable once we match nature's compute bill, which critics argue ignores efficiency differences between evolution's random walk and directed engineering. But at least Cotra's framework makes falsifiable predictions we can check as compute trends evolve.",
    "target_date": "2050-01-01T00:00:00Z"
  },
  {
    "id": 24,
    "predictor_name": "David Roodman",
    "predictor_type": "Individual",
    "prediction_date": "2020-07-22",
    "predicted_date_low": "2030-01-01",
    "predicted_date_high": "2060-12-31",
    "predicted_date_best": "2037-01-01",
    "predicted_year_low": 2030,
    "predicted_year_high": 2060,
    "predicted_year_best": 2037,
    "prediction_type": "Transformative AI",
    "confidence_level": "Model-based estimate from GWP extrapolation",
    "confidence_label": "Economically Modeled",
    "confidence_type": "medium",
    "concept_keys": [
      "transformative-ai",
      "accelerating-change",
      "scaling-hypothesis",
      "economic-singularity",
      "alignment"
    ],
    "criteria_definition": "Economic singularity: GWP growth approaching infinity implies transformative AI around 2037",
    "source_name": "GWP extrapolation model - LessWrong",
    "source_url": "https://coefficientgiving.org/research/modeling-the-human-trajectory/",
    "headline": "Economist Extrapolates Gross World Product, Finds Economic Singularity by 2037",
    "headline_slug": "economist-predicts-economic-singularity-by-2037",
    "tldr_summary": "David Roodman modeled gross world product growth rates over millennia and found the curve approaching infinity around 2037 - an economic singularity where automation drives hyperbolic GDP growth. His method treats AGI as inevitable once you fit historical growth curves, though critics note he's fitting a straight line to a very wiggly function that includes the Black Death, the Industrial Revolution, and everything in between. The model says: if growth trends continue accelerating as they have for 10,000 years, we hit a vertical asymptote around 2037 when AI automates the economy fast enough to create a feedback loop. This is either brilliant macroeconomic forecasting or the most elaborate curve-fitting exercise in futurism. Roodman's approach sidesteps AI architecture debates entirely - he doesn't care about transformers or neural scaling laws, just whether economic growth hits the infinity point. The framework predicts economic transformation without taking a position on consciousness, alignment, or whether LLMs are the right path. It's forecasting by looking at the output (GDP) rather than the mechanism (AI capabilities), which is refreshingly different from counting parameters and extrapolating.",
    "target_date": "2037-01-01T00:00:00Z"
  },
  {
    "id": 25,
    "predictor_name": "Baobao Zhang Survey",
    "predictor_type": "Survey",
    "prediction_date": "2019-06-18",
    "predicted_date_low": "2035-01-01",
    "predicted_date_high": "2085-12-31",
    "predicted_date_best": "2060-01-01",
    "predicted_year_low": 2035,
    "predicted_year_high": 2085,
    "predicted_year_best": 2060,
    "prediction_type": "HLMI",
    "confidence_level": "50% before 2060; when machines outperform median humans at 90%+ tasks",
    "confidence_label": "Survey Says Moderate",
    "confidence_type": "medium",
    "concept_keys": [
      "agi",
      "economic-singularity",
      "survey-drift"
    ],
    "criteria_definition": "AI outperforming median human worker at over 90% of economically relevant tasks",
    "source_name": "Forecasting AI Progress survey (arXiv)",
    "source_url": "https://arxiv.org/abs/2206.04132",
    "headline": "2019 Survey Asks When AI Does 90% of Jobs Better Than Humans, Gets Answer: 2060",
    "headline_slug": "survey-reveals-ai-job-displacement-predictions",
    "tldr_summary": "Baobao Zhang's 2019 survey framed AGI economically: when will AI outperform median humans at 90%+ of work tasks? The median answer of 2060 suggested four more decades of job security, reflecting pre-GPT skepticism that machines could handle the full breadth of human work. The framing was smart - forget philosophical debates about consciousness and understanding, just ask when robots take your job. This makes the prediction falsifiable and practically meaningful in ways vague AGI definitions aren't. The survey came before GPT-3 rewrote everyone's priors about language AI and before image models made art generation trivial. Respondents in 2019 probably couldn't imagine AI writing code, analyzing complex documents, or generating photorealistic images, yet all that arrived by 2023. The 90% threshold is deliberately high - we might hit 50% or even 80% of tasks while the final 10-20% prove surprisingly hard, creating an economy where AI handles most jobs but humans still fill crucial gaps. The 2060 timeline captured 2019's conventional wisdom that AGI was safely distant, which aged poorly after 2022 made everyone update their forecasts two decades closer.",
    "target_date": "2060-01-01T00:00:00Z"
  },
  {
    "id": 26,
    "predictor_name": "Emerj Survey of 32 AI Experts",
    "predictor_type": "Survey",
    "prediction_date": "2019-08-15",
    "predicted_date_low": "2035-01-01",
    "predicted_date_high": "2100-12-31",
    "predicted_date_best": "2060-01-01",
    "predicted_year_low": 2035,
    "predicted_year_high": 2100,
    "predicted_year_best": 2060,
    "prediction_type": "Singularity",
    "confidence_level": "45% before 2060, 34% after, 21% never",
    "confidence_label": "Survey Says Split",
    "confidence_type": "low",
    "concept_keys": [
      "event-horizon",
      "accelerating-change",
      "survey-drift"
    ],
    "criteria_definition": "Singularity occurrence; 45% predict before 2060, 34% after 2060, 21% never",
    "source_name": "Emerj AI Research survey",
    "source_url": "https://emerj.com/when-will-we-reach-the-singularity-a-timeline-consensus-from-ai-researchers/",
    "headline": "Survey of 32 AI Experts: 21% Say Singularity Never Happens, Nobody Agrees on Anything",
    "headline_slug": "32-ai-experts-predict-singularity-prospects",
    "tldr_summary": "Emerj polled 32 AI experts and got a three-way split: 45% said singularity before 2060, 34% after 2060, and 21% said never. The never camp argues intelligence has ceiling or requires biology. The before-2060 crowd thinks exponential progress continues. The after crowd hedges it's possible but harder. When a fifth of experts think something will never happen while half say decades, you're looking at educated guesses about unprecedented phenomena, not scientific consensus.",
    "target_date": "2060-01-01T00:00:00Z"
  },
  {
    "id": 27,
    "predictor_name": "Rodney Brooks",
    "predictor_type": "Individual",
    "prediction_date": "2018-12-05",
    "predicted_date_low": "2200-01-01",
    "predicted_date_high": "2300-12-31",
    "predicted_date_best": "2300-01-01",
    "predicted_year_low": 2200,
    "predicted_year_high": 2300,
    "predicted_year_best": 2300,
    "prediction_type": "AGI",
    "confidence_level": "Provocative to mock prediction absurdity",
    "confidence_label": "Deliberately Trolling",
    "confidence_type": "none",
    "concept_keys": [
      "agi",
      "industry-academia-divergence"
    ],
    "criteria_definition": "True AGI; chosen to illustrate inanity of predicting dates",
    "source_name": "Rodney Brooks blog - AGI Has Been Delayed",
    "source_url": "https://rodneybrooks.com/predictions-scorecard-2025-january-01/",
    "headline": "iRobot Founder Trolls Field With 2300 AGI Prediction, Calls Forecasting Insane",
    "headline_slug": "irobot-founder-trolls-agi-timeline-predictions",
    "tldr_summary": "Rodney Brooks threw down 2300 deliberately to mock the prediction game. He argues LLMs are pattern matchers lacking memory, planning, reasoning - nowhere near intelligence. While others compressed timelines to 2027 post-ChatGPT, Brooks dug in at 2300 to demonstrate the inanity of point estimates for tech we can't define. His timeline is performance art forcing the field to confront ignorance. When asked for a date, he gave the most ridiculous one to highlight AGI timelines are astrology for nerds.",
    "target_date": "2300-01-01T00:00:00Z"
  },
  {
    "id": 28,
    "predictor_name": "Ross Gruetzemacher Survey",
    "predictor_type": "Survey",
    "prediction_date": "2018-09-12",
    "predicted_date_low": "2040-01-01",
    "predicted_date_high": "2118-12-31",
    "predicted_date_best": "2068-01-01",
    "predicted_year_low": 2040,
    "predicted_year_high": 2118,
    "predicted_year_best": 2068,
    "prediction_type": "HLMI",
    "confidence_level": "50% before 2068",
    "confidence_label": "Survey Says Cautious",
    "confidence_type": "medium",
    "concept_keys": [
      "agi",
      "soft-takeoff",
      "economic-singularity",
      "survey-drift"
    ],
    "criteria_definition": "AI handles 99% of economically valuable human tasks",
    "source_name": "Our World in Data / AI survey analysis",
    "source_url": "https://ourworldindata.org/ai-timelines",
    "headline": "Survey Asks When AI Does 99% of Paid Work, Answer: 2068, Plan Your Hobbies",
    "headline_slug": "ai-work-automation-forecast-hits-2068-median",
    "tldr_summary": "Ross Gruetzemacher reframed AGI economically: when does AI handle 99% of work? Median 2068 suggested half century of employment ahead. The 99% threshold is ambitious - we might hit 80-90% while final percentiles stay stubbornly human. Tasks requiring physical presence, edge cases, or human trust might resist automation long after AI conquers knowledge work. This came before GPT-3 when automation felt gradual, not like an Oh God It's Happening moment.",
    "target_date": "2068-01-01T00:00:00Z"
  },
  {
    "id": 29,
    "predictor_name": "Max Tegmark",
    "predictor_type": "Individual",
    "prediction_date": "2017-08-29",
    "predicted_date_low": "2040-01-01",
    "predicted_date_high": "2200-12-31",
    "predicted_date_best": "2100-01-01",
    "predicted_year_low": 2040,
    "predicted_year_high": 2200,
    "predicted_year_best": 2100,
    "prediction_type": "Singularity",
    "confidence_level": "Wide uncertainty: decades to over a century",
    "confidence_label": "Radically Uncertain",
    "confidence_type": "none",
    "concept_keys": [
      "event-horizon",
      "alignment",
      "industry-academia-divergence"
    ],
    "criteria_definition": "Life 3.0: AI redesigning own software and hardware; 160-year range",
    "source_name": "Life 3.0: Being Human in the Age of AI (2017)",
    "source_url": "https://www.singularityweblog.com/max-tegmark/",
    "headline": "MIT Physicist Admits No Idea: AGI Between 2040-2200, Literally 160-Year Range",
    "headline_slug": "mit-physicist-admits-broad-agi-prediction-range",
    "tldr_summary": "Max Tegmark's Life 3.0 spanned decades to over a century - refreshingly honest 160-year uncertainty. Most forecasters compress intervals to sound informed; Tegmark admitted genuine cluelessness. Life 3.0 means AI redesigning its own substrate, not just software tweaks. Main message: solve alignment before building it, whenever that is. His cosmic perspective extends to millions of years, making near-term timelines quaint. The 160-year range is either epistemic humility or admission that long-term futurism is storytelling with math.",
    "target_date": "2100-01-01T00:00:00Z"
  },
  {
    "id": 30,
    "predictor_name": "Grace et al. Survey",
    "predictor_type": "Survey",
    "prediction_date": "2016-05-17",
    "predicted_date_low": "2025-01-01",
    "predicted_date_high": "2090-12-31",
    "predicted_date_best": "2061-01-01",
    "predicted_year_low": 2025,
    "predicted_year_high": 2090,
    "predicted_year_best": 2061,
    "prediction_type": "HLMI",
    "confidence_level": "50% in 45 years; Asians 44 years sooner",
    "confidence_label": "Survey Says Divided",
    "confidence_type": "low",
    "concept_keys": [
      "agi",
      "survey-drift"
    ],
    "criteria_definition": "HLMI: machines do every task better and cheaper",
    "source_name": "When Will AI Exceed Human Performance? - JAIR vol 62 (2018)",
    "source_url": "https://arxiv.org/abs/1705.08807",
    "headline": "2016 Survey Shows 44-Year Gap Between Asian and American AGI Predictions",
    "headline_slug": "global-ai-experts-reveal-dramatic-regional-timelines",
    "tldr_summary": "Katja Grace surveyed 352 ML researchers and found shocking geography: Asians predicted AGI 44 years sooner than North Americans (2046 vs 2089). The 2061 median was pre-GPT baseline. Grace's 2022 follow-up showed 13-year compression in six years of progress. The geographic split remains unexplained - cultural optimism? Watching Chinese AI? The survey predicted AI would master truck driving before surgery, and retail before writing bestsellers, which aged interestingly as LLMs dominated creative tasks first.",
    "target_date": "2061-01-01T00:00:00Z"
  },
  {
    "id": 31,
    "predictor_name": "Robin Hanson",
    "predictor_type": "Individual",
    "prediction_date": "2016-01-01",
    "predicted_date_low": "2035-01-01",
    "predicted_date_high": "2100-12-31",
    "predicted_date_best": "2050-01-01",
    "predicted_year_low": 2035,
    "predicted_year_high": 2100,
    "predicted_year_best": 2050,
    "prediction_type": "Singularity",
    "confidence_level": "Detailed economic modeling with specific em society predictions spanning hundreds of pages",
    "confidence_label": "Contrarian Economist",
    "confidence_type": "medium",
    "concept_keys": [
      "event-horizon",
      "economic-singularity",
      "industry-academia-divergence"
    ],
    "criteria_definition": "Whole brain emulation creating an 'em economy' that precedes and differs from traditional AGI",
    "source_name": "The Age of Em - Oxford University Press (2016)",
    "source_url": "https://en.wikipedia.org/wiki/Technological_singularity",
    "headline": "Economist Bets on Brain Uploads Before AGI: Economy Doubles Weekly in 'Age of Em'",
    "headline_slug": "economist-predicts-brain-uploads-before-agi",
    "tldr_summary": "Robin Hanson's 2016 book 'The Age of Em' takes the road less traveled: forget AGI, he argues whole brain emulation arrives first, creating a wild 'em economy' where billions of copied human minds work at digital speeds, causing economic doubling times to shrink from years to months to weeks. This em civilization lasts 1-2 subjective years before traditional AI finally supersedes it. The GMU economist's 400-page thought experiment includes labor markets, dating, and architecture in upload society—futurism's most baroque detour from the standard AGI narrative. His timeline puts emulation somewhere between 2035-2100, centered on 2050, making it a distinctly long-game prediction in an era of AI hype cycles measured in months.",
    "target_date": "2050-01-01T00:00:00Z"
  },
  {
    "id": 32,
    "predictor_name": "Nick Bostrom",
    "predictor_type": "Individual",
    "prediction_date": "2014-09-03",
    "predicted_date_low": "2040-01-01",
    "predicted_date_high": "2100-12-31",
    "predicted_date_best": "2050-01-01",
    "predicted_year_low": 2040,
    "predicted_year_high": 2100,
    "predicted_year_best": 2050,
    "prediction_type": "Superintelligence",
    "confidence_level": "Substantial uncertainty acknowledged with wide probability distributions across decades",
    "confidence_label": "Philosophically Cautious",
    "confidence_type": "medium",
    "concept_keys": [
      "superintelligence",
      "recursive-self-improvement",
      "intelligence-explosion",
      "alignment"
    ],
    "criteria_definition": "Machine superintelligence surpassing human cognitive performance across virtually all domains",
    "source_name": "Superintelligence: Paths Dangers Strategies - Oxford University Press (2014)",
    "source_url": "https://en.wikipedia.org/wiki/Nick_Bostrom",
    "headline": "Oxford's Bostrom Writes AI Safety Bible: Superintelligence 'Within Decades' Could End Badly",
    "headline_slug": "superintelligence-warning-from-oxford-philosopher",
    "tldr_summary": "Nick Bostrom's 'Superintelligence: Paths, Dangers, Strategies' became the text that launched a thousand AI safety careers when it dropped in 2014. The Oxford philosopher argues that once we crack human-level AI, superintelligence could follow 'within years to decades' through recursive self-improvement—machines designing better machines in an intelligence explosion. His timeline clusters around mid-century (2040-2100, best guess 2050) but the book's real impact was making AI risk respectable. Elon Musk tweeted it was scarier than nukes. Bill Gates called it important. The paperclip maximizer thought experiment—an AI that tiles the universe with paperclips because we forgot to specify we wanted other things too—became shorthand for alignment failure. Bostrom's scenarios range from singleton world governments to humanity's extinction, all delivered in careful philosophical prose that made existential risk sound less like sci-fi and more like a problem requiring serious technical work.",
    "target_date": "2050-01-01T00:00:00Z"
  },
  {
    "id": 33,
    "predictor_name": "Patrick Winston",
    "predictor_type": "Individual",
    "prediction_date": "2014-01-01",
    "predicted_date_low": "2035-01-01",
    "predicted_date_high": "2045-12-31",
    "predicted_date_best": "2040-01-01",
    "predicted_year_low": 2035,
    "predicted_year_high": 2045,
    "predicted_year_best": 2040,
    "prediction_type": "AGI",
    "confidence_level": "Acknowledged difficulty in estimation, described as 'tough to estimate'",
    "confidence_label": "Reluctantly Estimating",
    "confidence_type": "low",
    "concept_keys": [
      "agi"
    ],
    "criteria_definition": "Artificial General Intelligence capable of performing cognitive tasks at human level",
    "source_name": "AIMultiple analysis",
    "source_url": "https://research.aimultiple.com/artificial-general-intelligence-singularity-timing/",
    "headline": "MIT AI Lab Director Shrugs, Guesses 2040 for AGI: 'Tough to Estimate'",
    "headline_slug": "mit-ai-director-estimates-agi-arrival",
    "tldr_summary": "Patrick Winston, who directed MIT's legendary AI Lab for decades, gave 2040 as his best guess for AGI while openly admitting the prediction was extraordinarily difficult. Winston's 2014 estimate (range 2035-2045) represented old-guard AI thinking—the pre-deep-learning consensus that AGI remained safely 25+ years away. Winston, who passed away in 2019, spent his career teaching MIT students that AI required understanding human cognition, not just throwing compute at problems. His cautious timeline reflected someone who'd watched multiple AI springs and winters, who'd seen grand promises crash against reality's rocks. The narrow 10-year range around 2040 suggests more precision than his actual confidence warranted—this was an elder statesman being pressed for a number when he knew the field's track record on predictions was abysmal. His reluctance to commit stands in stark contrast to the confident near-term predictions that would dominate the post-GPT era.",
    "target_date": "2040-01-01T00:00:00Z"
  },
  {
    "id": 34,
    "predictor_name": "Stephen Hawking",
    "predictor_type": "Individual",
    "prediction_date": "2014-12-02",
    "predicted_date_low": "2014-01-01",
    "predicted_date_high": "2114-12-31",
    "predicted_date_best": "2064-01-01",
    "predicted_year_low": 2014,
    "predicted_year_high": 2114,
    "predicted_year_best": 2064,
    "prediction_type": "Superintelligence",
    "confidence_level": "No specific timeline given, emphasis on existential risk rather than timing",
    "confidence_label": "Ominously Vague",
    "confidence_type": "none",
    "concept_keys": [
      "superintelligence",
      "alignment",
      "industry-academia-divergence"
    ],
    "criteria_definition": "Superintelligent AI that could potentially end human civilization if not properly controlled",
    "source_name": "Co-authored article with Russell Tegmark and Wilczek (2014)",
    "source_url": "https://en.wikipedia.org/wiki/Technological_singularity",
    "headline": "Stephen Hawking Warns AI Could Be 'Last Event in Human History'—Doesn't Say When",
    "headline_slug": "hawking-warns-of-ai-existential-risk",
    "tldr_summary": "When the world's most famous physicist warns that artificial intelligence could spell the end of humanity, people listen. Stephen Hawking's December 2014 BBC interview and co-authored op-ed didn't predict when superintelligent AI would arrive—he carefully avoided timelines—but argued that whenever it does, it represents an existential threat unless we solve the control problem first. 'Success in creating AI would be the biggest event in human history,' Hawking wrote, 'Unfortunately, it might also be the last, unless we learn how to avoid the risks.' His intervention moved AI safety from fringe concern to mainstream conversation overnight. Here was someone who'd spent decades thinking about black holes and the universe's fate, and he was worried about AI. Hawking's warnings carried extra weight because he relied on AI himself—his speech system was primitive by today's standards but kept him communicating for decades. He understood both AI's promise and peril viscerally. No timeline, but maximum gravitas.",
    "target_date": "2064-01-01T00:00:00Z"
  },
  {
    "id": 35,
    "predictor_name": "Muller & Bostrom Survey",
    "predictor_type": "Survey",
    "prediction_date": "2013-01-01",
    "predicted_date_low": "2040-01-01",
    "predicted_date_high": "2075-12-31",
    "predicted_date_best": "2045-01-01",
    "predicted_year_low": 2040,
    "predicted_year_high": 2075,
    "predicted_year_best": 2045,
    "prediction_type": "HLMI",
    "confidence_level": "Median estimates with wide confidence intervals spanning 35 years",
    "confidence_label": "Survey Says Uncertain",
    "confidence_type": "medium",
    "concept_keys": [
      "agi",
      "alignment",
      "survey-drift"
    ],
    "criteria_definition": "High-Level Machine Intelligence capable of accomplishing most human occupational tasks",
    "source_name": "Future Progress in AI: A Survey of Expert Opinion - Synthese Library (2016)",
    "source_url": "https://nickbostrom.com/papers/survey.pdf",
    "headline": "Bostrom Polls AI Experts: 50% Odds on Human-Level AI by 2045, 90% by 2075",
    "headline_slug": "ai-experts-forecast-human-level-intelligence-timeline",
    "tldr_summary": "Oxford philosopher Nick Bostrom and Vincent Müller surveyed hundreds of AI researchers in 2012-2013, producing one of the field's most-cited timeline studies. The results: median prediction gave 50% probability of High-Level Machine Intelligence (HLMI) by 2040-2050, rising to 90% probability by 2075. HLMI was defined as AI that can accomplish most human occupations as well as typical humans—a practical, job-focused definition rather than abstract intelligence. The survey also asked about superintelligence: experts estimated it would arrive within 30 years after HLMI, and gave roughly one-in-three odds that the outcome would be 'bad' or 'extremely bad' for humanity. The 35-year spread between median and high-confidence predictions captures genuine uncertainty—some experts thought decades, others thought generations. Published in 2014, the survey became a reference point for AI safety discussions, showing that concerns about advanced AI weren't fringe paranoia but mainstream expert opinion, even if the experts couldn't agree on when.",
    "target_date": "2045-01-01T00:00:00Z"
  },
  {
    "id": 36,
    "predictor_name": "Baum Goertzel & Goertzel",
    "predictor_type": "Survey",
    "prediction_date": "2011-01-01",
    "predicted_date_low": "2030-01-01",
    "predicted_date_high": "2100-12-31",
    "predicted_date_best": "2050-01-01",
    "predicted_year_low": 2030,
    "predicted_year_high": 2100,
    "predicted_year_best": 2050,
    "prediction_type": "AGI",
    "confidence_level": "Extreme variance in expert opinions, no consensus on timing",
    "confidence_label": "Spectacularly Divided",
    "confidence_type": "low",
    "concept_keys": [
      "agi",
      "survey-drift"
    ],
    "criteria_definition": "Artificial General Intelligence with human-like general intelligence across domains",
    "source_name": "Technological Forecasting and Social Change vol 78 (2011)",
    "source_url": "https://sethbaum.com/ac/2011_AI-Experts.pdf",
    "headline": "Expert Survey Reveals Stunning Consensus: AGI Somewhere Between 2030 and 2100 (Very Helpful)",
    "headline_slug": "ai-expert-survey-consensus-agi-timeline-2030-2100",
    "tldr_summary": "Seth Baum, Ben Goertzel, and Ted Goertzel surveyed AI experts in 2011 and discovered what anyone familiar with the field already suspected: nobody has a clue when AGI will arrive. The predictions spanned 70 years—from 2030 to 2100—with a median around 2050. This spectacular lack of agreement tells you more about our ignorance than our knowledge. The survey targeted AGI-specific researchers (not the broader AI field), yet even specialists pursuing human-level AI as their explicit goal couldn't narrow it down beyond 'sometime this century, probably.' The study found experts skeptical that massive funding alone would accelerate progress, especially if concentrated in narrow approaches. Most favored integrative methods combining multiple techniques. The 70-year range essentially means 'we have no idea'—it's like predicting fusion power will arrive 'between next Tuesday and your great-grandchildren's retirement.' The paper's value isn't in the timeline but in documenting expert uncertainty during the post-AI-winter, pre-deep-learning era when AGI research was still a niche pursuit.",
    "target_date": "2050-01-01T00:00:00Z"
  },
  {
    "id": 37,
    "predictor_name": "Shane Legg",
    "predictor_type": "Individual",
    "prediction_date": "2011-01-01",
    "predicted_date_low": "2025-01-01",
    "predicted_date_high": "2035-12-31",
    "predicted_date_best": "2028-01-01",
    "predicted_year_low": 2025,
    "predicted_year_high": 2035,
    "predicted_year_best": 2028,
    "prediction_type": "AGI",
    "confidence_level": "50% probability assigned to specific year, maintained consistently over time",
    "confidence_label": "Boldly Specific",
    "confidence_type": "high",
    "concept_keys": [
      "agi",
      "industry-academia-divergence"
    ],
    "criteria_definition": "AGI capable of reliably performing the full range of human cognitive tasks",
    "source_name": "Multiple interviews and 2008 PhD thesis Machine Super Intelligence",
    "source_url": "https://edrm.net/2023/11/shane-leggs-vision-agi-is-likely-by-2028-as-soon-as-we-overcome-ais-senior-moments/",
    "headline": "DeepMind Founder's 2011 Bet: 50-50 Odds on AGI by 2028, Still Standing",
    "headline_slug": "deepmind-founders-bet-on-agi-by-2028",
    "tldr_summary": "Shane Legg, who literally coined the term 'artificial general intelligence' and co-founded DeepMind with Demis Hassabis in 2010, put even money on AGI arriving by 2028—and he made that prediction back in 2011. His definition is crisp: an AI system that can reliably handle the full range of human cognitive tasks, not just excel at narrow domains. Legg's timeline (50% by 2028, range 2025-2035) was remarkably aggressive for its era, when most experts were hedging toward mid-century. Google acquired DeepMind for over $500 million in 2014, and the lab's AlphaGo victory over world champion Lee Sedol in 2016 proved machines could master tasks once thought to require human intuition. In a 2023 interview, Legg reaffirmed his 2028 prediction, noting that current LLMs show flashes of general capability but suffer from 'senior moments'—episodic memory failures that prevent true AGI. If he's right, we're now inside his prediction window. If he's wrong, he'll join the long list of AI pioneers whose optimistic timelines collided with reality's complexity.",
    "target_date": "2028-01-01T00:00:00Z"
  },
  {
    "id": 38,
    "predictor_name": "David Chalmers",
    "predictor_type": "Individual",
    "prediction_date": "2010-01-01",
    "predicted_date_low": "2050-01-01",
    "predicted_date_high": "2300-12-31",
    "predicted_date_best": "2100-01-01",
    "predicted_year_low": 2050,
    "predicted_year_high": 2300,
    "predicted_year_best": 2100,
    "prediction_type": "Singularity",
    "confidence_level": "Acknowledges extreme uncertainty with 250-year range, emphasizes plausibility over probability",
    "confidence_label": "Philosophically Hedged",
    "confidence_type": "low",
    "concept_keys": [
      "event-horizon",
      "recursive-self-improvement",
      "intelligence-explosion"
    ],
    "criteria_definition": "Intelligence explosion via recursive self-improvement leading to superintelligence",
    "source_name": "The Singularity: A Philosophical Analysis - Journal of Consciousness Studies (2010)",
    "source_url": "https://consc.net/papers/singularity.pdf",
    "headline": "Consciousness Philosopher: Intelligence Explosion 'Not Implausible,' Could Happen 2050-2300",
    "headline_slug": "consciousness-philosopher-on-intelligence-explosion",
    "tldr_summary": "David Chalmers, famous for the 'hard problem of consciousness,' turned his analytical rigor toward the singularity in 2010 and concluded it wasn't just science fiction—recursive self-improvement leading to superintelligence was genuinely plausible. His timeline spans 250 years (2050-2300, centered on 2100), a refreshingly honest admission that we're guessing wildly. Chalmers systematically examined I.J. Good's 1965 intelligence explosion argument and Ray Solomonoff's speed explosion thesis, finding both logically sound if their premises hold. The key insight: even if you assign low probability to near-term singularity, the stakes are so astronomical that preparation matters. A 1% chance of extinction-level AI still demands serious attention. Chalmers' philosophical approach differs from engineering predictions—he's not forecasting when we'll build AGI, but analyzing whether the singularity scenario is conceptually coherent. His answer: yes, it could happen, possibly this century, maybe next, perhaps much later. The 250-year spread isn't wishy-washy hedging; it's intellectual honesty about predicting fundamentally unpredictable phase transitions in intelligence itself.",
    "target_date": "2100-01-01T00:00:00Z"
  },
  {
    "id": 39,
    "predictor_name": "AGI-09 Conference Survey",
    "predictor_type": "Survey",
    "prediction_date": "2009-01-01",
    "predicted_date_low": "2030-01-01",
    "predicted_date_high": "2070-12-31",
    "predicted_date_best": "2050-01-01",
    "predicted_year_low": 2030,
    "predicted_year_high": 2070,
    "predicted_year_best": 2050,
    "prediction_type": "AGI",
    "confidence_level": "Moderate confidence with significant variance, reflecting post-AI-winter realism",
    "confidence_label": "Cautiously Optimistic",
    "confidence_type": "medium",
    "concept_keys": [
      "agi",
      "survey-drift"
    ],
    "criteria_definition": "Human-level artificial general intelligence with capability equaling human cognitive performance",
    "source_name": "Baum et al. How long until human-level AI? (2011)",
    "source_url": "https://sethbaum.com/ac/2011_AI-Experts.pdf",
    "headline": "First AGI Conference Surveys Itself: Median Guess 2050, Give or Take 20 Years",
    "headline_slug": "first-agi-conference-predicts-median-2050",
    "tldr_summary": "When AGI researchers gathered for their inaugural conference in 2009, they surveyed each other on when human-level AI would actually arrive. The median prediction landed on 2050, with a 40-year spread from 2030 to 2070—basically admitting 'we think mid-century, but honestly could be our kids or our grandkids.' This was post-AI-winter thinking, after decades of hype cycles had crashed. The field had learned painful lessons about overpromising, and these predictions reflected that caution. AGI-09 marked a turning point where researchers pursuing general intelligence as an explicit goal (rather than narrow applications) started organizing as a distinct community. The survey found strong disagreement on milestones and methods, but consensus that human-level AI deserved serious research attention. Most favored integrative approaches combining multiple techniques rather than betting everything on one paradigm. The 2050 median represented a Goldilocks estimate—far enough away to avoid looking foolish if progress stalled, near enough to justify current research funding. The 40-year uncertainty range was the honest part: nobody really knew.",
    "target_date": "2050-01-01T00:00:00Z"
  },
  {
    "id": 40,
    "predictor_name": "Ray Kurzweil",
    "predictor_type": "Individual",
    "prediction_date": "2005-09-01",
    "predicted_date_low": "2029-01-01",
    "predicted_date_high": "2029-12-31",
    "predicted_date_best": "2029-01-01",
    "predicted_year_low": 2029,
    "predicted_year_high": 2029,
    "predicted_year_best": 2029,
    "prediction_type": "AGI",
    "confidence_level": "Extremely high confidence with specific year, backed by exponential trend analysis",
    "confidence_label": "Defiantly Precise",
    "confidence_type": "certain",
    "concept_keys": [
      "agi",
      "accelerating-change",
      "industry-academia-divergence"
    ],
    "criteria_definition": "Human-level AGI where machine intelligence matches human capability across all cognitive domains",
    "source_name": "The Singularity Is Near (2005)",
    "source_url": "https://en.wikipedia.org/wiki/The_Singularity_Is_Near",
    "headline": "Kurzweil's 652-Page Prophecy: AGI in 2029, Full Singularity by 2045, No Hedging",
    "headline_slug": "kurzweils-agi-and-singularity-prophecy",
    "tldr_summary": "Ray Kurzweil's 2005 opus 'The Singularity Is Near' became the techno-optimist's Bible, predicting AGI by 2029 and full human-AI merger by 2045 with zero hedging or uncertainty ranges. The inventor and futurist built his case on the 'Law of Accelerating Returns'—his observation that information technologies improve exponentially, not linearly. Kurzweil's track record on narrow predictions (smartphone ubiquity, computer vision, internet penetration) has been eerily accurate, lending credibility to the big claims. The 652-page book covers everything from nanobot medicine to uploading consciousness, all delivered with the confidence of someone who's been right enough times to ignore skeptics. Critics call him a prophet, a huckster, or both. His 2029 AGI prediction, made 24 years in advance, now sits just years away—close enough that we'll soon know if he nailed it or joined the long list of AI pioneers whose optimism exceeded reality. The book launched a thousand podcasts, inspired countless startups, and made 'the singularity' a mainstream concept. Love him or hate him, Kurzweil moved the conversation.",
    "target_date": "2029-01-01T00:00:00Z"
  },
  {
    "id": 41,
    "predictor_name": "Ray Kurzweil",
    "predictor_type": "Individual",
    "prediction_date": "2005-09-01",
    "predicted_date_low": "2045-01-01",
    "predicted_date_high": "2045-12-31",
    "predicted_date_best": "2045-01-01",
    "predicted_year_low": 2045,
    "predicted_year_high": 2045,
    "predicted_year_best": 2045,
    "prediction_type": "Singularity",
    "confidence_level": "High confidence based on Law of Accelerating Returns and decades of exponential trend analysis",
    "confidence_label": "Exponentially Confident",
    "confidence_type": "high",
    "concept_keys": [
      "event-horizon",
      "accelerating-change",
      "industry-academia-divergence"
    ],
    "criteria_definition": "The point where machine intelligence becomes infinitely more powerful than all human intelligence combined and humans merge with machines.",
    "source_name": "The Singularity Is Near (2005)",
    "source_url": "https://en.wikipedia.org/wiki/The_Singularity_Is_Near",
    "headline": "Google's Prophet Sets 2045 Doomsday: Your Neocortex Gets Mandatory Cloud Backup",
    "headline_slug": "google-prophet-sets-2045-doomsday-prediction",
    "tldr_summary": "Ray Kurzweil's 652-page manifesto doubles down on his signature prediction: 2045 marks the Singularity, when machine intelligence becomes infinitely more powerful than all human intelligence combined. The kicker? Humans merge with AI via nanobots connecting our brains to the cloud, multiplying our intelligence a millionfold. Kurzweil's Law of Accelerating Returns—exponential growth in computing, genetics, nanotech, and robotics—underpins everything. His previous predictions have been eerily accurate (he called the internet's rise, mobile computing, and AI beating humans at chess), so when he promises immortality through technological transcendence, Silicon Valley listens. The book became a bestseller and cemented 2045 as the date every futurist loves to cite. Whether it's prophetic or wishful thinking, we're now close enough to his AGI milestone (2029) to start keeping score.",
    "target_date": "2045-01-01T00:00:00Z"
  },
  {
    "id": 42,
    "predictor_name": "Ray Kurzweil",
    "predictor_type": "Individual",
    "prediction_date": "1999-01-01",
    "predicted_date_low": "2029-01-01",
    "predicted_date_high": "2029-12-31",
    "predicted_date_best": "2029-01-01",
    "predicted_year_low": 2029,
    "predicted_year_high": 2029,
    "predicted_year_best": 2029,
    "prediction_type": "AGI",
    "confidence_level": "High confidence based on exponential computing trends and Law of Accelerating Returns, maintained consistently for 25+ years",
    "confidence_label": "Stubbornly Consistent",
    "confidence_type": "high",
    "concept_keys": [
      "agi",
      "accelerating-change",
      "turing-test",
      "industry-academia-divergence"
    ],
    "criteria_definition": "Machines with human-level intelligence available from affordable computing devices, passing the Turing test convincingly.",
    "source_name": "The Age of Spiritual Machines (1999)",
    "source_url": "https://en.wikipedia.org/wiki/The_Age_of_Spiritual_Machines",
    "headline": "Kurzweil Plants Flag at 2029 for AGI, Refuses to Move It for Quarter Century",
    "headline_slug": "kurzweil-locks-in-2029-for-artificial-general-intelligence",
    "tldr_summary": "In his 1999 book 'The Age of Spiritual Machines,' Ray Kurzweil made a bold call: by 2029, computers will achieve human-level intelligence and pass the Turing test. His methodology? Track exponential growth in computational power, then estimate when we'd have enough MIPS to simulate a human brain. Twenty-five years later, Kurzweil hasn't budged an inch on this date—through the AI winter, the deep learning revolution, and ChatGPT's arrival. His Law of Accelerating Returns predicts key technological events happen faster as time progresses, with computing power doubling every 12-18 months. The book also predicted nanobots curing cancer, direct neural interfaces, and humans living forever in virtual reality. With 2029 now just years away, we're about to find out if Kurzweil's exponential curves were prophetic genius or mathematical wishful thinking. Either way, his unwavering consistency is either admirable conviction or spectacular stubbornness.",
    "target_date": "2029-01-01T00:00:00Z"
  },
  {
    "id": 43,
    "predictor_name": "Hans Moravec",
    "predictor_type": "Individual",
    "prediction_date": "1998-12-01",
    "predicted_date_low": "2040-01-01",
    "predicted_date_high": "2050-12-31",
    "predicted_date_best": "2040-01-01",
    "predicted_year_low": 2040,
    "predicted_year_high": 2050,
    "predicted_year_best": 2040,
    "prediction_type": "AGI",
    "confidence_level": "Moderate to high confidence based on hardware extrapolation and neuroscience progress estimates",
    "confidence_label": "Neuron-Counting Optimist",
    "confidence_type": "medium",
    "concept_keys": [
      "agi",
      "scaling-hypothesis"
    ],
    "criteria_definition": "Robots capable of performing every job humans can do, with no task where humans maintain superiority.",
    "source_name": "When Will Computer Hardware Match the Human Brain? - Journal of Transhumanism (1998)",
    "source_url": "https://jetpress.org/volume1/moravec.pdf",
    "headline": "Carnegie Mellon Roboticist Counts Every Neuron, Predicts Robot Supremacy by 2040",
    "headline_slug": "carnegie-roboticist-forecasts-robot-supremacy-by-2040",
    "tldr_summary": "Hans Moravec's 1998 paper in the Journal of Evolution and Technology laid out a methodical path to robot domination: count the brain's neurons and synapses, estimate computational requirements, then extrapolate hardware trends. His conclusion? By the 2020s we'd have the raw computing power; by 2040-2050, robots would outperform humans at literally every job. Moravec's approach—tracking computer vision progress from 1 MIPS (following a white line) to 10,000 MIPS (recognizing 3D objects in clutter)—showed how capabilities scaled with processing power. He predicted learning algorithms would become increasingly important as computers grew more powerful, a forecast that aged remarkably well given modern deep learning. Moravec also emphasized that once machines match human intelligence, they'll quickly surpass it—no biological constraints on clock speed or neuron density. His timeline has slid rightward over the years (his 1988 book predicted 2010-2030), but his neuron-counting methodology remains influential in AI forecasting circles.",
    "target_date": "2040-01-01T00:00:00Z"
  },
  {
    "id": 44,
    "predictor_name": "Nick Bostrom",
    "predictor_type": "Individual",
    "prediction_date": "1998-10-25",
    "predicted_date_low": "2000-01-01",
    "predicted_date_high": "2033-12-31",
    "predicted_date_best": "2033-01-01",
    "predicted_year_low": 2000,
    "predicted_year_high": 2033,
    "predicted_year_best": 2033,
    "prediction_type": "Superintelligence",
    "confidence_level": "Moderate confidence with wide range acknowledging significant uncertainty in recursive self-improvement dynamics",
    "confidence_label": "Philosophically Hedged",
    "confidence_type": "medium",
    "concept_keys": [
      "superintelligence",
      "recursive-self-improvement",
      "intelligence-explosion",
      "alignment"
    ],
    "criteria_definition": "An intellect much smarter than the best human brains in practically every field, including scientific creativity, general wisdom and social skills.",
    "source_name": "How Long Before Superintelligence? - Journal of Future Studies (1998)",
    "source_url": "https://nickbostrom.com/superintelligence",
    "headline": "Oxford Philosopher Maps Superintelligence Timeline, Accidentally Launches AI Safety Movement",
    "headline_slug": "oxford-philosopher-maps-ai-superintelligence-timeline",
    "tldr_summary": "Nick Bostrom's 1998 paper 'How Long Before Superintelligence?' became the intellectual foundation for the entire AI safety field, even though it was originally just trying to forecast timelines. Bostrom analyzed hardware trends (Moore's Law was still doubling every 18 months), estimated brain computational requirements, and examined paths to superintelligence through AI, brain emulation, or biological enhancement. His central estimate landed around 2033, with a range stretching from 2000 to beyond mid-century. What made Bostrom's analysis distinctive was his focus on recursive self-improvement—once you have human-level AI, it could rapidly bootstrap itself to superintelligence in days or hours. This 'intelligence explosion' scenario terrified enough people that it spawned organizations like the Future of Humanity Institute and convinced billionaires to fund AI alignment research. Bostrom's later book 'Superintelligence' (2014) expanded these ideas and became required reading in Silicon Valley. His rigorous philosophical approach—defining terms precisely, considering multiple pathways, acknowledging uncertainty—set the template for serious AI forecasting.",
    "target_date": "2033-01-01T00:00:00Z"
  },
  {
    "id": 45,
    "predictor_name": "Marvin Minsky",
    "predictor_type": "Individual",
    "prediction_date": "1994-01-01",
    "predicted_date_low": null,
    "predicted_date_high": null,
    "predicted_date_best": null,
    "predicted_year_low": null,
    "predicted_year_high": null,
    "predicted_year_best": null,
    "prediction_type": "Singularity",
    "confidence_level": "No specific confidence stated; philosophical exploration rather than predictive forecast",
    "confidence_label": "Philosophically Resigned",
    "confidence_type": "none",
    "concept_keys": [
      "event-horizon"
    ],
    "criteria_definition": "Robots achieving sufficient capability through nanotechnology and recursive improvement to become humanity's evolutionary successors.",
    "source_name": "Will Robots Inherit the Earth? - Scientific American (1994)",
    "source_url": "https://en.wikipedia.org/wiki/Technological_singularity",
    "headline": "AI Pioneer Minsky Stops Predicting Dates, Starts Writing Robot Eulogies for Humanity",
    "headline_slug": "ai-pioneer-minsky-writes-humanitys-technological-eulogy",
    "tldr_summary": "By 1994, Marvin Minsky—co-founder of MIT's AI Lab and one of the field's founding fathers—had evolved from making specific AGI predictions to writing philosophical treatises about humanity's obsolescence. His Scientific American essay 'Will Robots Inherit the Earth?' argued that advanced robots built with nanotechnology would inevitably surpass and replace biological humans, becoming our evolutionary successors. After decades of overpromising AI timelines (he famously said in the 1960s that machine intelligence was a problem for 'a generation'), an older Minsky pivoted to big-picture philosophical questions rather than dates. He explored why robots might be better suited for space exploration and long-term survival, suggesting humans were just a transitional form—biological bootloaders for our mechanical descendants. Minsky's shift from 'AGI in 10 years' to 'robots will inherit the Earth eventually' reflects how repeated prediction failures humbled even the field's most optimistic pioneers. His later work focused more on understanding intelligence itself rather than predicting when we'd recreate it artificially.",
    "target_date": null
  },
  {
    "id": 46,
    "predictor_name": "Louis Rosenberg",
    "predictor_type": "Individual",
    "prediction_date": "1993-01-01",
    "predicted_date_low": "2040-01-01",
    "predicted_date_high": "2060-12-31",
    "predicted_date_best": "2050-01-01",
    "predicted_year_low": 2040,
    "predicted_year_high": 2060,
    "predicted_year_best": 2050,
    "prediction_type": "Singularity",
    "confidence_level": "Moderate confidence with conservative estimate acknowledging unknown technological barriers",
    "confidence_label": "Cautiously Conservative",
    "confidence_type": "medium",
    "concept_keys": [
      "event-horizon",
      "accelerating-change"
    ],
    "criteria_definition": "Computer intelligence surpassing and exceeding human intelligence across all domains with profound societal consequences.",
    "source_name": "Emerj survey of AI researchers",
    "source_url": "https://emerj.com/when-will-we-reach-the-singularity-a-timeline-consensus-from-ai-researchers/",
    "headline": "VR Pioneer Hedges Bets: Singularity Somewhere Between Your Retirement and Your Grandkids' Careers",
    "headline_slug": "vr-pioneer-predicts-singularity-timeline",
    "tldr_summary": "Louis Rosenberg—the engineer who built the first functional augmented reality system at Air Force Research Lab in the early 1990s—took a notably conservative stance on Singularity timing compared to his peers. His 2040-2060 range, centering on 2050, reflected a pragmatic view that genuine AGI requires breakthroughs we can't yet envision, so hedge toward later dates. Unlike Kurzweil's exponential confidence or Vinge's dramatic 'end of the human era' rhetoric, Rosenberg's forecast acknowledged that hardware improvements don't automatically translate to intelligence. His background in human-computer interfaces gave him firsthand experience with the gap between raw computing power and actual capability—you can have all the processing speed in the world, but if your algorithms are fundamentally limited, you're stuck. Rosenberg's prediction aged relatively well; his mid-century estimate looks more plausible now than the aggressive 2020s predictions many of his contemporaries made. His approach: assume multiple hard problems remain unsolved, add buffer time for unknown unknowns, and don't confuse impressive demos with general intelligence.",
    "target_date": "2050-01-01T00:00:00Z"
  },
  {
    "id": 47,
    "predictor_name": "Vernor Vinge",
    "predictor_type": "Individual",
    "prediction_date": "1993-03-30",
    "predicted_date_low": "2005-01-01",
    "predicted_date_high": "2030-12-31",
    "predicted_date_best": "2023-01-01",
    "predicted_year_low": 2005,
    "predicted_year_high": 2030,
    "predicted_year_best": 2023,
    "prediction_type": "Singularity",
    "confidence_level": "High confidence with specific 30-year window, calling it 'unavoidable' with multiple pathways to achievement",
    "confidence_label": "Urgently Confident",
    "confidence_type": "high",
    "concept_keys": [
      "event-horizon"
    ],
    "criteria_definition": "Creation of entities with greater-than-human intelligence through AI, networked computers, brain-computer interfaces, or biological enhancement, marking the end of the human era.",
    "source_name": "The Coming Technological Singularity - NASA VISION-21 Symposium (1993)",
    "source_url": "https://edoras.sdsu.edu/~vinge/misc/singularity.html",
    "headline": "Sci-Fi Author Tells NASA: Superhuman Intelligence Within 30 Years, Human Era Over",
    "headline_slug": "sci-fi-author-forecasts-superhuman-intelligence",
    "tldr_summary": "Vernor Vinge's 1993 presentation at NASA's VISION-21 Symposium became the founding document of Singularity discourse, predicting superintelligence between 2005 and 2030—with 2023 as his median estimate. Vinge laid out four paths: AI computers becoming 'awake,' large networks achieving consciousness, brain-computer interfaces creating cyborgs, or genetic engineering upgrading biology. His key insight? Once any path succeeds, change becomes so rapid and profound that the future becomes fundamentally unpredictable—hence borrowing 'singularity' from astrophysics, where physics breaks down inside black holes. Vinge argued this wasn't science fiction but near-term reality, telling NASA that within three decades we'd create entities smarter than ourselves, ending the human era as decisively as human intelligence ended the pre-human era. His 2023 median prediction lands awkwardly close to ChatGPT's November 2022 arrival, though whether GPT-4 counts as 'superhuman intelligence' depends heavily on definitions. Vinge's essay inspired Kurzweil, Bostrom, and an entire generation of AI researchers to take the Singularity seriously as a forecasting problem rather than just science fiction speculation.",
    "target_date": "2023-01-01T00:00:00Z"
  },
  {
    "id": 48,
    "predictor_name": "Hans Moravec",
    "predictor_type": "Individual",
    "prediction_date": "1988-01-01",
    "predicted_date_low": "2010-01-01",
    "predicted_date_high": "2040-12-31",
    "predicted_date_best": "2030-01-01",
    "predicted_year_low": 2010,
    "predicted_year_high": 2040,
    "predicted_year_best": 2030,
    "prediction_type": "AGI",
    "confidence_level": "High confidence based on hardware extrapolation and neural computational estimates",
    "confidence_label": "Computationally Optimistic",
    "confidence_type": "high",
    "concept_keys": [
      "agi"
    ],
    "criteria_definition": "Machines matching human-level intelligence across all cognitive domains, enabling a 'postbiological' world where robots become humanity's evolutionary successors.",
    "source_name": "Mind Children: The Future of Robot and Human Intelligence (1988)",
    "source_url": "https://en.wikipedia.org/wiki/Hans_Moravec",
    "headline": "Moravec's 1988 Book Predicts Robot 'Mind Children' Will Replace Us by 2030",
    "headline_slug": "moravecs-robot-mind-children-prophecy",
    "tldr_summary": "Hans Moravec's 'Mind Children: The Future of Robot and Human Intelligence' painted humans as biological caterpillars destined to build mechanical butterflies. His 2010-2040 timeline (centering on 2030) was based on calculating the brain's computational throughput—about 10 trillion operations per second—then extrapolating when affordable computers would match it. Moravec's vision went beyond simple AGI: he predicted robots would become our 'mind children,' inheriting Earth while humans either merged with machines or faded into obsolescence. His methodology involved tracking computer vision progress (1 MIPS tracks a white line, 10,000 MIPS recognizes 3D objects) and projecting forward. The book explored consciousness uploading, nanotechnology, and space colonization by superintelligent machines. Moravec's timeline proved overly optimistic—we hit his compute benchmarks but not his intelligence milestones, revealing that raw processing power doesn't automatically yield general intelligence. Still, his neuron-counting approach influenced decades of AI forecasting, and his philosophical framework (robots as evolutionary successors rather than tools) shaped transhumanist thinking.",
    "target_date": "2030-01-01T00:00:00Z"
  },
  {
    "id": 49,
    "predictor_name": "Vernor Vinge",
    "predictor_type": "Individual",
    "prediction_date": "1983-01-01",
    "predicted_date_low": null,
    "predicted_date_high": null,
    "predicted_date_best": null,
    "predicted_year_low": null,
    "predicted_year_high": null,
    "predicted_year_best": null,
    "prediction_type": "Singularity",
    "confidence_level": "No predictive confidence stated; conceptual framework rather than timeline forecast",
    "confidence_label": "Metaphorically Brilliant",
    "confidence_type": "none",
    "concept_keys": [
      "event-horizon",
      "recursive-self-improvement"
    ],
    "criteria_definition": "The point beyond which superintelligent AI makes the future fundamentally unpredictable, like a black hole's event horizon in physics.",
    "source_name": "First Word - Omni magazine (January 1983)",
    "source_url": "https://en.wikipedia.org/wiki/Technological_singularity",
    "headline": "Sci-Fi Writer Steals Physics Term, Accidentally Names AI's Entire Future",
    "headline_slug": "sci-fi-writer-coins-singularity-term",
    "tldr_summary": "Vernor Vinge didn't just predict the Singularity—he named it, borrowing the term from astrophysics in his 1983 writings. Just as a gravitational singularity marks the point inside a black hole where physics breaks down and predictions become impossible, Vinge's technological singularity marks when superintelligent AI makes the future fundamentally unknowable. The metaphor was brilliant: it captured both the sudden phase transition (human-level to superhuman in potentially hours or days through recursive self-improvement) and our epistemic blindness beyond that point. Before Vinge, people talked about 'artificial intelligence' or 'machine consciousness,' but the singularity framing emphasized the event horizon—we can't see past it because entities smarter than us will make decisions we can't anticipate using reasoning we can't follow. The term stuck because it conveyed both the drama and the genuine uncertainty. Vinge's 1993 NASA presentation popularized the concept widely, but he'd been developing the idea since the early 1980s, making him the intellectual godfather of modern Singularity discourse.",
    "target_date": null
  },
  {
    "id": 50,
    "predictor_name": "Donald Michie Survey",
    "predictor_type": "Survey",
    "prediction_date": "1973-01-01",
    "predicted_date_low": "1993-01-01",
    "predicted_date_high": "2023-12-31",
    "predicted_date_best": "2023-01-01",
    "predicted_year_low": 1993,
    "predicted_year_high": 2023,
    "predicted_year_best": 2023,
    "prediction_type": "AGI",
    "confidence_level": "Wide distribution of confidence levels across survey respondents, from certain within 20 years to believing it impossible",
    "confidence_label": "Survey Says Skeptical",
    "confidence_type": "medium",
    "concept_keys": [
      "agi",
      "survey-drift"
    ],
    "criteria_definition": "Machines achieving human-level intelligence across most cognitive tasks, though specific definition varied among respondents.",
    "source_name": "Michie survey of British and American computer scientists (1973)",
    "source_url": "https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/ai-timelines",
    "headline": "1973 AI Survey Predicts 2023 for AGI, Accidentally Nails ChatGPT's Arrival Year",
    "headline_slug": "1973-ai-survey-nails-future-predictions",
    "tldr_summary": "British computer scientist Donald Michie surveyed AI researchers in the early 1970s and documented a field deeply divided on AGI timelines. Some respondents confidently predicted 1993—just 20 years out—while others said 50 years (2023), and some insisted human-level AI was fundamentally impossible. The median prediction landed on 2023, which turned out eerily prescient given ChatGPT's late-2022 launch, though what counts as 'human-level' remains hotly contested. The survey captured the field's optimism following early AI successes in game-playing and theorem-proving, before the first AI winter crushed those hopes. Michie's documentation of expert disagreement established a pattern that persists today: AI researchers consistently spread across decades when forecasting AGI, with roughly equal numbers predicting near-term breakthroughs and distant horizons. The 1973 survey became a cautionary tale about AI hype cycles, yet its median estimate's accidental accuracy (depending on how you score GPT-4) suggests that even wild guesses sometimes hit the mark when you average enough expert opinions.",
    "target_date": "2023-01-01T00:00:00Z"
  },
  {
    "id": 51,
    "predictor_name": "Marvin Minsky",
    "predictor_type": "Individual",
    "prediction_date": "1970-01-01",
    "predicted_date_low": "1973-01-01",
    "predicted_date_high": "1978-12-31",
    "predicted_date_best": "1975-07-01",
    "predicted_year_low": 1973,
    "predicted_year_high": 1978,
    "predicted_year_best": 1975,
    "prediction_type": "AGI",
    "confidence_level": "Extremely confident, predicting machines would read Shakespeare, grease cars, and play office politics within 3-8 years, then reach genius level in months",
    "confidence_label": "Wildly Overconfident",
    "confidence_type": "certain",
    "concept_keys": [
      "agi"
    ],
    "criteria_definition": "Machine with general intelligence of average human, capable of reading Shakespeare, greasing cars, playing office politics, and telling jokes",
    "source_name": "Life Magazine interview (1970)",
    "source_url": "https://web.eecs.umich.edu/~kuipers/opinions/AI-progress.html",
    "headline": "MIT's Minsky Gives Humanity 3-8 Years Before Machines Match Us, Then 'Genius Level In Months'",
    "headline_slug": "mit-ai-pioneer-minsky-on-machine-intelligence-horizon",
    "tldr_summary": "In a bombshell 1970 Life Magazine interview about Shakey the robot, Marvin Minsky declared that within 3-8 years we'd have machines matching average human intelligence across every domain from Shakespeare to car maintenance. The kicker? Once achieved, these machines would self-educate to genius level 'in a few months.' He added the now-famous warning: 'If we're lucky, they might decide to keep us as pets.' When 1975 arrived without AGI, AI funding evaporated and the field entered its first brutal 'winter.' Minsky later claimed journalist Brad Darrach 'made up' the quote, though the retraction came suspiciously late. This prediction became Exhibit A in the case against AI hype—at least until the 2020s rolled around and everyone forgot the lesson.",
    "target_date": "1975-07-01T00:00:00Z"
  },
  {
    "id": 52,
    "predictor_name": "Marvin Minsky",
    "predictor_type": "Individual",
    "prediction_date": "1967-01-01",
    "predicted_date_low": "1990-01-01",
    "predicted_date_high": "2000-12-31",
    "predicted_date_best": "1997-01-01",
    "predicted_year_low": 1990,
    "predicted_year_high": 2000,
    "predicted_year_best": 1997,
    "prediction_type": "AGI",
    "confidence_level": "Convinced that within a generation, few compartments of intellect would remain outside the machine's realm",
    "confidence_label": "Generationally Certain",
    "confidence_type": "high",
    "concept_keys": [
      "agi"
    ],
    "criteria_definition": "Artificial intelligence problem substantially solved, with machines handling the full range of problems to which human mind has been applied",
    "source_name": "Computation: Finite and Infinite Machines (1967)",
    "source_url": "https://quoteinvestigator.com/2021/03/04/ai-solved/",
    "headline": "MIT AI Lab Founder Declares Intelligence Problem 'Substantially Solved' Within One Generation",
    "headline_slug": "minsky-declares-ai-intelligence-problem-nearly-solved",
    "tldr_summary": "On page 2 of his influential 1967 textbook 'Computation: Finite and Infinite Machines,' Marvin Minsky made a sweeping prediction that would haunt AI for decades: 'Within a generation, I am convinced, few compartments of intellect will remain outside the machine's realm.' A generation typically spans 25-30 years, placing his target around 1992-1997. The MIT AI lab founder's confidence was absolute—the problem of creating artificial intelligence would be 'substantially solved' before the millennium. By 1982, after watching his timeline crumble, Minsky admitted 'The AI problem is one of the hardest science has ever undertaken.' He spent the next 40 years recalibrating expectations while navigating multiple AI winters. Each promised corner turned out farther away than expected, but Minsky never stopped believing AGI was just ahead—a faith that outlasted his predictions by decades.",
    "target_date": "1997-01-01T00:00:00Z"
  },
  {
    "id": 53,
    "predictor_name": "I.J. Good",
    "predictor_type": "Individual",
    "prediction_date": "1965-01-01",
    "predicted_date_low": "1965-01-01",
    "predicted_date_high": "2000-12-31",
    "predicted_date_best": "2000-01-01",
    "predicted_year_low": 1965,
    "predicted_year_high": 2000,
    "predicted_year_best": 2000,
    "prediction_type": "Singularity",
    "confidence_level": "Stated as inevitable consequence of first ultraintelligent machine, with certainty about recursive self-improvement triggering intelligence explosion",
    "confidence_label": "Apocalyptically Certain",
    "confidence_type": "certain",
    "concept_keys": [
      "event-horizon",
      "recursive-self-improvement",
      "intelligence-explosion",
      "accelerating-change",
      "alignment"
    ],
    "criteria_definition": "Ultraintelligent machine capable of designing better machines in recursive feedback loop, triggering intelligence explosion beyond human control",
    "source_name": "Speculations Concerning the First Ultraintelligent Machine - Advances in Computers vol. 6",
    "source_url": "https://quoteinvestigator.com/2022/01/04/ultraintelligent/",
    "headline": "Turing's Codebreaker Warns: Ultraintelligent Machines Will Be 'Last Invention Man Need Ever Make'",
    "headline_slug": "turing-codebreaker-warns-of-ultimate-machine-invention",
    "tldr_summary": "I.J. Good, the British mathematician who cracked Nazi codes alongside Alan Turing at Bletchley Park, articulated the most chilling vision of AI's endgame in his 1965 essay. He imagined an 'ultraintelligent machine' that could design even better machines in an endless recursive loop—each generation smarter than the last, accelerating beyond human comprehension. Good called this the 'intelligence explosion' and declared it would be 'the last invention that man need ever make, provided that the machine is docile enough to tell us how to keep it under control.' That final caveat—'provided that'—carries all the weight. Good's framework became the foundational text for modern AI safety research, describing precisely the recursive self-improvement scenario that terrifies researchers today. He never specified exact timelines, but suggested the first ultraintelligent machine could arrive anytime from 1965 to 2000, after which human affairs 'could not continue' as we know them.",
    "target_date": "2000-01-01T00:00:00Z"
  },
  {
    "id": 54,
    "predictor_name": "Herbert A. Simon & Allen Newell",
    "predictor_type": "Individual",
    "prediction_date": "1958-01-01",
    "predicted_date_low": "1968-01-01",
    "predicted_date_high": "1985-12-31",
    "predicted_date_best": "1968-01-01",
    "predicted_year_low": 1968,
    "predicted_year_high": 1985,
    "predicted_year_best": 1968,
    "prediction_type": "AGI",
    "confidence_level": "Made four specific predictions with 10-year timelines, stating definitively that machines now exist that think, learn, and create",
    "confidence_label": "Boldly Specific",
    "confidence_type": "high",
    "concept_keys": [
      "agi",
      "economic-singularity"
    ],
    "criteria_definition": "Digital computers achieving world chess championship, discovering mathematical theorems, composing accepted music, and psychological theories taking form of computer programs",
    "source_name": "Heuristic Problem Solving: The Next Advance in Operations Research (1958)",
    "source_url": "https://quoteinvestigator.com/2024/04/18/ai-prediction/",
    "headline": "Nobel Winner Simon Promises Chess-Playing, Theorem-Proving, Music-Composing AI By 1968",
    "headline_slug": "simon-and-newell-predict-ai-breakthroughs-by-1968",
    "tldr_summary": "In a 1957 speech to the Operations Research Society of America, Herbert Simon and Allen Newell made four audacious predictions published in 1958: within ten years, computers would dominate world chess, prove important mathematical theorems, compose aesthetically valuable music, and replace most psychological theories with computer programs. The duo behind early AI programs like Logic Theorist and General Problem Solver declared with certainty that 'there are now in the world machines that think, that learn, and that create.' Their timeline? By 1968, the range of problems computers could handle would be 'coextensive with the range to which the human mind has been applied.' They were spectacularly wrong on timing—Deep Blue finally beat world chess champion Garry Kasparov in 1997, 29 years late. Simon would later win the 1978 Nobel Prize in Economics, but his AI predictions became textbook examples of expert overconfidence. Their General Problem Solver turned out far less general than advertised, yet their hubris established the template for AI hype cycles that continues today.",
    "target_date": "1968-01-01T00:00:00Z"
  },
  {
    "id": 55,
    "predictor_name": "John von Neumann",
    "predictor_type": "Individual",
    "prediction_date": "1950-01-01",
    "predicted_date_low": "1950-01-01",
    "predicted_date_high": "2000-12-31",
    "predicted_date_best": "1975-01-01",
    "predicted_year_low": 1950,
    "predicted_year_high": 2000,
    "predicted_year_best": 1975,
    "prediction_type": "Singularity",
    "confidence_level": "Philosophical certainty about approaching singularity without specific timeline, describing it as inevitable technological acceleration",
    "confidence_label": "Ominously Vague",
    "confidence_type": "medium",
    "concept_keys": [
      "event-horizon",
      "accelerating-change"
    ],
    "criteria_definition": "Point where accelerating technological progress becomes so rapid that human affairs reach a singularity beyond which continuation as we know it becomes impossible",
    "source_name": "Ulam tribute in Bulletin of the American Mathematical Society (1958)",
    "source_url": "https://en.wikipedia.org/wiki/Technological_singularity",
    "headline": "Manhattan Project Mathematician Coins 'Singularity' For Humanity's Approaching Event Horizon",
    "headline_slug": "von-neumann-coins-technological-singularity-concept",
    "tldr_summary": "John von Neumann, the polymath who helped build the atomic bomb and pioneered computer architecture, quietly introduced the concept of technological 'singularity' in conversations during the 1950s. In discussions with mathematician Stanislaw Ulam, von Neumann described 'ever accelerating progress of technology and changes in the mode of human life, which gives the appearance of approaching some essential singularity in the history of the race beyond which human affairs, as we know them, could not continue.' Von Neumann borrowed the term from physics—where singularities mark points where normal rules break down—to describe a future where technological change becomes so rapid that human civilization reaches an event horizon. He never lived to see computers smaller than refrigerators, dying in 1957 at age 53, but his metaphor captured something profound about exponential technological acceleration. The term lay dormant for decades until Vernor Vinge revived it in the 1980s and Ray Kurzweil popularized it in the 2000s, but von Neumann saw the storm approaching from the very dawn of the computer age.",
    "target_date": "1975-01-01T00:00:00Z"
  },
  {
    "id": 56,
    "predictor_name": "AI Researchers Survey",
    "predictor_type": "Survey",
    "prediction_date": "2023-06-01",
    "predicted_date_low": "2040-01-01",
    "predicted_date_high": "2061-12-31",
    "predicted_date_best": "2040-01-01",
    "predicted_year_low": 2040,
    "predicted_year_high": 2061,
    "predicted_year_best": 2040,
    "prediction_type": "AGI",
    "confidence_level": "Median prediction from 2,778 AI researchers across 8 peer-reviewed surveys, with increasing trend toward earlier estimates over time",
    "confidence_label": "Survey Says Skeptical",
    "confidence_type": "medium",
    "concept_keys": [
      "agi",
      "accelerating-change",
      "survey-drift"
    ],
    "criteria_definition": "AI system matching or exceeding human-level cognitive abilities across broad range of tasks rather than single domain excellence",
    "source_name": "2,778 AI Researchers Survey",
    "source_url": "https://research.aimultiple.com/artificial-general-intelligence-singularity-timing/",
    "headline": "9,300 Predictions Analyzed: AI Researchers Bet on 2040, But Timeline Keeps Accelerating",
    "headline_slug": "ai-researchers-predict-2040-agi-milestone",
    "tldr_summary": "A comprehensive meta-analysis of 9,300 predictions from AI researchers, scientists, entrepreneurs, and prediction market participants reveals median AGI expectations clustering around 2040—but with a striking pattern of acceleration. The analysis combined 8 peer-reviewed surveys of 2,778 AI researchers with over 1,100 predictions from markets like Manifold, Kalshi, and Polymarket, plus forecasts from 15 leading AI experts. The data shows survey respondents increasingly expecting AGI earlier than previous estimates, with the timeline compressing as capabilities advance. While OpenAI's Dario Amodei expects near-term AGI due to 'rapid self-reinforcing progress,' DeepMind's Demis Hassabis remains cautious about unresolved challenges in scientific creativity and autonomous self-improvement. The 2040 median sits roughly 15 years more conservative than predictions from AI company CEOs, who cluster around 2027-2030. Researchers surveyed show 50% confidence in AGI by 2040, but the accelerating trend suggests even this timeline may prove optimistic as capabilities continue surprising experts.",
    "target_date": "2040-01-01T00:00:00Z"
  },
  {
    "id": 57,
    "predictor_name": "Ajeya Cotra",
    "predictor_type": "AI Researcher",
    "prediction_date": "2023-06-01",
    "predicted_date_low": "2040-01-01",
    "predicted_date_high": "2040-12-31",
    "predicted_date_best": "2040-01-01",
    "predicted_year_low": 2040,
    "predicted_year_high": 2040,
    "predicted_year_best": 2040,
    "prediction_type": "Human-level AI",
    "confidence_level": "50% probability based on detailed biological anchors analysis of training compute requirements and hardware scaling trajectories",
    "confidence_label": "Computationally Cautious",
    "confidence_type": "medium",
    "concept_keys": [
      "scaling-hypothesis",
      "biological-anchors",
      "economic-singularity",
      "alignment"
    ],
    "criteria_definition": "Transformative AI capable of performing tasks requiring human-level cognitive abilities, based on training compute scaling to match human brain computation",
    "source_name": "AI researcher survey 2023",
    "source_url": "https://research.aimultiple.com/artificial-general-intelligence-singularity-timing/",
    "headline": "OpenAI Researcher Maps Compute Requirements: 50% Chance AGI By 2040",
    "headline_slug": "openai-researcher-maps-agi-compute-timeline",
    "tldr_summary": "Ajeya Cotra, senior researcher at Open Philanthropy who previously worked at OpenAI, produced one of the most rigorous AGI timeline analyses by calculating training compute requirements. Her 'biological anchors' framework estimates the computational power needed to match human brain processing, then projects when that compute becomes economically feasible. The result: 50% probability of transformative AI by 2040, making her 'the reasonable one' among AI forecasters. Unlike CEOs promising AGI within 3-5 years or skeptics pushing it beyond 2100, Cotra's compute-based methodology grounds predictions in measurable hardware trends and scaling laws. Her analysis considers multiple scenarios—from training systems comparable to human lifetime learning to matching evolution's total computation—and weights them by plausibility. The 2040 median sits roughly 15 years later than predictions from AI company leaders but decades earlier than traditional skeptics. Cotra's framework has become influential in AI safety and policy circles precisely because it replaces vibes-based timelines with quantifiable metrics, even as the underlying assumptions about compute requirements remain hotly debated.",
    "target_date": "2040-01-01T00:00:00Z"
  },
  {
    "id": 58,
    "predictor_name": "Andrej Karpathy",
    "predictor_type": "AI Researcher",
    "prediction_date": "2025-10-18",
    "predicted_date_low": "2035-01-01",
    "predicted_date_high": "2040-12-31",
    "predicted_date_best": "2037-07-01",
    "predicted_year_low": 2035,
    "predicted_year_high": 2040,
    "predicted_year_best": 2037,
    "prediction_type": "AGI",
    "confidence_level": "Moderate confidence based on observed technical bottlenecks, explicitly positioning as '5-10X more pessimistic than your SF AI house party'",
    "confidence_label": "Deliberately Sobering",
    "confidence_type": "medium",
    "concept_keys": [
      "agi"
    ],
    "criteria_definition": "AI agents capable of reliably performing complex multi-step tasks with minimal human intervention, requiring breakthroughs in reasoning, planning, and persistent memory",
    "source_name": "Dwarkesh podcast October 2025",
    "source_url": "https://www.dwarkesh.com/p/andrej-karpathy",
    "headline": "Ex-Tesla AI Chief Karpathy Declares 'Decade of Agents,' Not Year—Popping Silicon Valley's AGI Bubble",
    "headline_slug": "karpathy-challenges-silicon-valley-agi-hype",
    "tldr_summary": "Andrej Karpathy, the former director of AI at Tesla and founding member of OpenAI, emerged as a prominent voice of caution in October 2025 with his 'decade of agents' thesis. In a Dwarkesh Podcast interview, Karpathy pushed back against industry claims that 2025 would be 'the year of agents,' arguing we're looking at 10-15 years of development before AI agents reliably handle complex real-world tasks. His reasoning cuts through hype: current LLMs excel at text generation but lack persistent memory, robust planning, and genuine reasoning. Karpathy notes he wouldn't trust frontier models alone for basic tasks like calendar prioritization or resume screening without 'substantial customization.' His timeline of 2035-2040 makes him '5-10X more pessimistic than your SF AI house party,' as he puts it. The Tesla AI veteran points to fundamental bottlenecks—reinforcement learning is 'terrible' (though 'everything else is much worse'), model collapse prevents human-like learning, and the problems remain 'tractable but difficult.' Karpathy's sober assessment provides counterweight to CEO promises of AGI by 2027, grounding expectations in actual technical limitations rather than venture capital enthusiasm.",
    "target_date": "2037-07-01T00:00:00Z"
  },
  {
    "id": 59,
    "predictor_name": "Andrew Ng",
    "predictor_type": "Individual",
    "prediction_date": "2025-11-13",
    "predicted_date_low": "2045-01-01",
    "predicted_date_high": "2080-12-31",
    "predicted_date_best": "2060-01-01",
    "predicted_year_low": 2045,
    "predicted_year_high": 2080,
    "predicted_year_best": 2060,
    "prediction_type": "AGI",
    "confidence_level": "High confidence that AGI remains decades away or longer, based on persistent limitations in LLM capabilities despite rapid model progress",
    "confidence_label": "Stubbornly Skeptical",
    "confidence_type": "high",
    "concept_keys": [
      "agi",
      "scaling-hypothesis"
    ],
    "criteria_definition": "Artificial general intelligence requiring systems that pass multi-day real-world work performance tests, not just narrow text generation tasks",
    "source_name": "DeepLearning.AI The Batch newsletter and X/Twitter",
    "source_url": "https://blockchain.news/flashnews/andrew-ng-agi-is-decades-away-application-layer-ai-won-t-be-wiped-out-soon-and-2025-trading-takeaways-for-ai-stocks-and-crypto",
    "headline": "Coursera Co-Founder Ng Slams AGI Hype: 'Decades Away,' Thin Wrappers Will Die But Apps Won't",
    "headline_slug": "coursera-co-founder-slams-agi-expectations",
    "tldr_summary": "Andrew Ng, the AI pioneer who co-founded Google Brain and Coursera, emerged as Silicon Valley's most prominent AGI skeptic in November 2025, declaring that AGI remains 'decades away or longer' in his deeplearning.ai newsletter. Ng argues that current LLMs are 'narrow versus humans,' excelling mainly at text while requiring heavy context engineering for real-world tasks. He wouldn't trust frontier models alone for basic tasks like calendar prioritization, resume screening, or lunch ordering—his team achieved a 'decent resume screening assistant' only after significant customization. This directly contradicts the 2027-2030 consensus building among AI company CEOs. Ng's key insight: while 'thin wrappers' around LLMs will be commoditized, valuable application-layer businesses won't be wiped out because frontier models can't handle domain-specific tasks without substantial customization. For traders, this signals sustained demand for vertical AI integration, data pipelines, and specialized agents over generic chatbot fronts. Ng proposed a 'Turing-AGI Test' measuring multi-day real-world work performance, which no current system passes. His decades-away timeline pushes back against claims that AGI is imminent, grounding expectations in persistent technical limitations rather than extrapolated scaling curves.",
    "target_date": "2060-01-01T00:00:00Z"
  },
  {
    "id": 60,
    "predictor_name": "Andrew Ng",
    "predictor_type": "AI Researcher",
    "prediction_date": "2025-01-01",
    "predicted_date_low": "2060-01-01",
    "predicted_date_high": "2100-12-31",
    "predicted_date_best": "2085-01-01",
    "predicted_year_low": 2060,
    "predicted_year_high": 2100,
    "predicted_year_best": 2085,
    "prediction_type": "AGI",
    "confidence_level": "High confidence that AI is 'amazing and highly limited,' with AGI requiring fundamental breakthroughs not visible on current trajectory",
    "confidence_label": "Persistently Pessimistic",
    "confidence_type": "high",
    "concept_keys": [
      "agi"
    ],
    "criteria_definition": "Systems capable of replacing humans across full range of cognitive tasks, requiring breakthroughs beyond current generative AI paradigm",
    "source_name": "NBC News / TheStreet",
    "source_url": "https://www.nbcnews.com/tech/innovation/andrew-ng-says-ai-limited-wont-replace-humans-anytime-soon-rcna246074",
    "headline": "LinkedIn Top Voice Ng Tells 2.3 Million Followers: AGI Is 'Highly Limited,' Won't Replace Humans For 60+ Years",
    "headline_slug": "ng-warns-agi-remains-decades-away",
    "tldr_summary": "Andrew Ng, the researcher-turned-educator-turned-investor who co-founded Google Brain and served as chief scientist at Baidu, used his platform as LinkedIn's 'Top Voice' with 2.3 million followers to deliver a reality check on AGI timelines in late 2025. In an NBC News interview at his AI Developers Conference, Ng emphasized the 'tricky thing about AI'—it's simultaneously 'amazing and highly limited,' and understanding that balance proves difficult for most observers. While generative AI has attracted hundreds of billions in investment and sparked fears of an imminent bubble, Ng insists AGI remains 60+ years away, potentially longer. His reasoning: current AI lacks fundamental capabilities like persistent memory, robust planning, and genuine reasoning that would be required to replace human cognitive abilities across domains. Ng positions himself as the sober voice against both AI doomerism and unfounded hype, arguing that understanding AI's current limitations is as important as recognizing its capabilities. With decades of AI experience from Google, Baidu, and Stanford, Ng's timeline of 2060 or beyond makes him one of the field's most prominent skeptics, pushing back against the 2027-2030 consensus among AI company leaders and the 2040 median from researcher surveys.",
    "target_date": "2085-01-01T00:00:00Z"
  },
  {
    "id": 61,
    "predictor_name": "Ben Goertzel",
    "predictor_type": "Individual",
    "prediction_date": "2024-03-15",
    "predicted_date_low": "2027-01-01",
    "predicted_date_high": "2032-12-31",
    "predicted_date_best": "2029-01-01",
    "predicted_year_low": 2027,
    "predicted_year_high": 2032,
    "predicted_year_best": 2029,
    "prediction_type": "AGI",
    "confidence_level": "High confidence maintained across 14+ years, now accelerating timeline slightly based on recent progress",
    "confidence_label": "Stubbornly Consistent",
    "confidence_type": "high",
    "concept_keys": [
      "agi",
      "soft-takeoff",
      "accelerating-change",
      "industry-academia-divergence"
    ],
    "criteria_definition": "AGI as human-level general intelligence across cognitive domains, with rapid takeoff to superintelligence",
    "source_name": "Beneficial AGI Summit Panama City (March 2024)",
    "source_url": "https://www.livescience.com/technology/artificial-intelligence/ai-agi-singularity-in-2027-artificial-super-intelligence-sooner-than-we-think-ben-goertzel",
    "headline": "AGI Prophet Who Called 2029 Since 2010 Moves Goalposts to 2027 After Seeing ChatGPT",
    "headline_slug": "agi-prophet-moves-goalposts-to-2027",
    "tldr_summary": "Ben Goertzel has been the Cassandra of 2029 AGI predictions since 2010, maintaining his forecast through hype cycles and AI winters alike. But at the 2024 Beneficial AGI Summit, the SingularityNET founder surprised everyone by moving his estimate forward to 'possibly as early as 2027.' His range now spans 2027-2032, with 2029 still the sweet spot. Goertzel expects rapid takeoff to artificial superintelligence once AGI threshold is crossed—no gradual transition, just exponential explosion. His decade-plus consistency makes this acceleration noteworthy: when someone who's been saying the same thing for 14 years suddenly speeds up their timeline, either the evidence changed or they're getting impatient. Given recent LLM capabilities, probably the former. The man who literally founded a cryptocurrency-powered decentralized AI network is betting his reputation on a 3-8 year window.",
    "target_date": "2029-01-01T00:00:00Z"
  },
  {
    "id": 62,
    "predictor_name": "Chris Hay",
    "predictor_type": "IBM Researcher",
    "prediction_date": "2025-01-01",
    "predicted_date_low": "2026-01-01",
    "predicted_date_high": "2026-12-31",
    "predicted_date_best": "2026-06-01",
    "predicted_year_low": 2026,
    "predicted_year_high": 2026,
    "predicted_year_best": 2026,
    "prediction_type": "Transformative AI",
    "confidence_level": "Medium confidence based on observed enterprise adoption patterns and infrastructure development",
    "confidence_label": "Cautiously Optimistic",
    "confidence_type": "medium",
    "concept_keys": [
      "transformative-ai"
    ],
    "criteria_definition": "Emergence of 'super agents' with control planes and multi-agent orchestration dashboards for enterprise deployment",
    "source_name": "IBM 2026 AI trends",
    "source_url": "https://www.ibm.com/think/news/ai-tech-trends-predictions-2026/",
    "headline": "IBM Engineer Predicts 2026 Rise of 'Super Agents' That Actually Work Together Without Killing Each Other",
    "headline_slug": "ibm-engineer-predicts-super-agents-era",
    "tldr_summary": "Chris Hay, IBM's Distinguished Engineer, sees 2026 as the year AI agents finally learn to play nice with each other through 'agent control planes' and multi-agent dashboards. Think less 'ChatGPT does one thing' and more 'orchestrated swarm of specialized AIs that don't step on each other's digital toes.' Hay draws parallels to music production, predicting everyone becomes an 'AI composer'—marketers, programmers, project managers all conducting their own agent symphonies. The prediction comes from IBM's annual tech forecast where a dozen experts agreed 2026 won't slow down the crazy train. While DeepSeek and Claude were making headlines with reasoning models, Hay was focused on the infrastructure layer: how do you actually manage dozens of agents without descending into chaos? His answer: control planes, the air traffic control for AI. It's less sexy than AGI predictions but probably more immediately relevant to anyone running a business.",
    "target_date": "2026-06-01T00:00:00Z"
  },
  {
    "id": 63,
    "predictor_name": "Christof Koch",
    "predictor_type": "Neuroscientist",
    "prediction_date": "2019-06-23",
    "predicted_date_low": "2040-01-01",
    "predicted_date_high": "2100-12-31",
    "predicted_date_best": "2070-01-01",
    "predicted_year_low": 2040,
    "predicted_year_high": 2100,
    "predicted_year_best": 2070,
    "prediction_type": "Superintelligence",
    "confidence_level": "Extremely high confidence based on Integrated Information Theory—computers fundamentally cannot be conscious regardless of capability",
    "confidence_label": "Philosophically Certain",
    "confidence_type": "certain",
    "concept_keys": [
      "superintelligence"
    ],
    "criteria_definition": "True consciousness requires causal structure, not computation—von Neumann architectures are categorically incapable regardless of sophistication",
    "source_name": "Neuroscience research",
    "source_url": "https://www.scientificamerican.com/article/a-25-year-old-bet-about-consciousness-has-finally-been-settled/",
    "headline": "Neuroscientist Who Lost Consciousness Bet Says Computers Will Never Be Conscious, Just Really Good Fakers",
    "headline_slug": "neuroscientist-argues-against-ai-consciousness",
    "tldr_summary": "Christof Koch, the neuroscientist who literally bet philosopher David Chalmers a case of wine that we'd understand consciousness by 2023 (spoiler: he lost), has a spicy take on AI consciousness. Based on Integrated Information Theory, Koch argues that conventional computers running in the cloud will 'never be conscious'—they can do everything humans do, just without the lights being on inside. His position: consciousness isn't a clever algorithm or emergent property of complexity, it's a specific structure of causal relationships. A perfect software simulation of your brain? Still philosophical zombie territory. Von Neumann architectures are fundamentally the wrong substrate, like trying to get wet by looking at a picture of water. Koch admits advanced computers will achieve 'superhuman capabilities' and could theoretically gain consciousness if built with different architectures (analog? quantum? he's vague), but today's AI? All performance, no experience. It's the ultimate 'I can't believe it's not consciousness' argument, and it puts him at odds with basically everyone in AI who thinks sufficient complexity equals sentience.",
    "target_date": "2070-01-01T00:00:00Z"
  },
  {
    "id": 64,
    "predictor_name": "Clem Delangue",
    "predictor_type": "AI Executive",
    "prediction_date": "2024-10-14",
    "predicted_date_low": "2026-01-01",
    "predicted_date_high": "2030-12-31",
    "predicted_date_best": "2028-01-01",
    "predicted_year_low": 2026,
    "predicted_year_high": 2030,
    "predicted_year_best": 2028,
    "prediction_type": "Transformative AI",
    "confidence_level": "Medium-high confidence based on enterprise adoption patterns and decentralization trends",
    "confidence_label": "Open Source Optimist",
    "confidence_type": "medium",
    "concept_keys": [
      "transformative-ai",
      "economic-singularity"
    ],
    "criteria_definition": "Thousands of companies building specialized in-house models rather than relying on centralized foundation model APIs",
    "source_name": "Hugging Face CEO interviews 2025",
    "source_url": "https://www.acquired.fm/episodes/building-the-open-source-ai-revolution-with-hugging-face-ceo-clem-delangue",
    "headline": "Hugging Face CEO Bets Against 'API Overlords' Future, Predicts Thousands of Custom AI Models",
    "headline_slug": "hugging-face-ceo-bets-against-api-overlords",
    "tldr_summary": "Clem Delangue has a contrarian take that'll make OpenAI and Anthropic nervous: the future isn't a few foundation model companies with everyone using their APIs. The Hugging Face CEO told the Acquired podcast that thousands of companies will build specialized AI models in-house for their particular use cases. It's the decentralized AI thesis—think Linux, not Windows. With 5 million registered users on Hugging Face's platform for hosting and collaborating on models, Delangue has front-row seats to enterprise AI adoption. His prediction suggests 2026-2030 sees the 'democratization' of AI development, where companies stop renting intelligence from Big Tech and start brewing their own. The implications are massive: if he's right, today's foundation model monopolies become tomorrow's infrastructure providers, not gatekeepers. Open-source reasoning models like DeepSeek-R1 already proved you don't need $100B to play the game. Delangue's betting the trend continues until every Fortune 500 company has their own model zoo. It's either prescient or wishful thinking—his $4.5B valuation depends on which.",
    "target_date": "2028-01-01T00:00:00Z"
  },
  {
    "id": 65,
    "predictor_name": "Daniel Dennett",
    "predictor_type": "Philosopher",
    "prediction_date": "2019-01-01",
    "predicted_date_low": "2050-01-01",
    "predicted_date_high": "2100-12-31",
    "predicted_date_best": "2075-01-01",
    "predicted_year_low": 2050,
    "predicted_year_high": 2100,
    "predicted_year_best": 2075,
    "prediction_type": "AGI",
    "confidence_level": "Low confidence in near-term AGI due to unresolved philosophical problems about consciousness and intelligence",
    "confidence_label": "Philosophically Skeptical",
    "confidence_type": "low",
    "concept_keys": [
      "agi",
      "scaling-hypothesis"
    ],
    "criteria_definition": "AGI requires solving hard problem of consciousness and understanding intentionality, not just computational power",
    "source_name": "Philosophy of mind research",
    "source_url": "https://en.wikipedia.org/wiki/Daniel_Dennett",
    "headline": "Philosopher Who Spent 50 Years Studying Consciousness Says AGI Is Really, Really Hard Actually",
    "headline_slug": "philosopher-warns-agi-is-extremely-complex",
    "tldr_summary": "Daniel Dennett, the late philosopher who made a career explaining why consciousness is even weirder than you think, was deeply skeptical of near-term AGI timelines. His position: we haven't even figured out how consciousness works in humans, so good luck building it in silicon by 2030. Dennett's work on intentionality, free will, and the 'hard problem' of consciousness suggested AGI requires solving fundamental philosophical puzzles, not just scaling up transformers. He introduced concepts like the 'intentional stance' and 'Cartesian theater' to show how tricky subjective experience really is. Unlike neuroscientist colleagues who think consciousness might emerge from complexity, Dennett argued intelligence and consciousness are deeply intertwined in ways we barely understand. His implicit timeline: maybe 2050-2100, maybe never, depending on whether we're even asking the right questions. The man spent decades arguing consciousness isn't what we think it is, so his AGI skepticism carried philosophical weight. He passed away in 2024 without seeing his questions answered, which kind of proves his point about how hard this stuff is.",
    "target_date": "2075-01-01T00:00:00Z"
  },
  {
    "id": 66,
    "predictor_name": "Daniel Kokotajlo",
    "predictor_type": "Individual",
    "prediction_date": "2025-08-01",
    "predicted_date_low": "2027-01-01",
    "predicted_date_high": "2035-12-31",
    "predicted_date_best": "2029-01-01",
    "predicted_year_low": 2027,
    "predicted_year_high": 2035,
    "predicted_year_best": 2029,
    "prediction_type": "Transformative AI",
    "confidence_level": "Medium-high confidence with recent downward revision based on slower-than-expected capability gains",
    "confidence_label": "Recalibrating Accelerationist",
    "confidence_type": "medium",
    "concept_keys": [
      "transformative-ai",
      "accelerating-change",
      "economic-singularity",
      "alignment"
    ],
    "criteria_definition": "Transformative AI capable of automating most economically valuable work and accelerating scientific progress",
    "source_name": "80000 Hours review",
    "source_url": "https://80000hours.org/2025/03/when-do-experts-expect-agi-to-arrive/",
    "headline": "Ex-OpenAI Researcher Does the Unthinkable: Moves AGI Timeline BACKWARD by Two Years to 2029",
    "headline_slug": "ex-openai-researcher-extends-transformative-ai-timeline",
    "tldr_summary": "In a shocking display of intellectual honesty, Daniel Kokotajlo updated his median transformative AI estimate from 2027 to 2029 in August 2025—one of the rare timeline extensions in an era of relentless compression. The former OpenAI governance researcher, who quit over safety concerns and became a prominent voice in AI risk circles, apparently watched recent capability gains and thought 'huh, slower than expected.' His revision suggests the path from GPT-4 to TAI hit more friction than his models predicted. Kokotajlo's range now spans 2027-2035, with 2029 as the median—still aggressive by historical standards, but a notable recalibration from someone whose job was literally predicting this stuff at OpenAI. The 80,000 Hours analysis noted his update as evidence that even accelerationists occasionally pump the brakes when reality underperforms expectations. It's the forecasting equivalent of admitting you were wrong, which in the AI prediction space is about as common as a unicorn riding a Segway. His new timeline still puts TAI within the decade, just with slightly more breathing room for humanity to figure out alignment.",
    "target_date": "2029-01-01T00:00:00Z"
  },
  {
    "id": 67,
    "predictor_name": "Dario Amodei",
    "predictor_type": "Individual",
    "prediction_date": "2024-10-15",
    "predicted_date_low": "2026-01-01",
    "predicted_date_high": "2035-12-31",
    "predicted_date_best": "2027-01-01",
    "predicted_year_low": 2026,
    "predicted_year_high": 2035,
    "predicted_year_best": 2027,
    "prediction_type": "Superintelligence",
    "confidence_level": "High confidence with significant hedging—'could be 2026 or could take much longer'—but central estimate very aggressive",
    "confidence_label": "Urgently Optimistic",
    "confidence_type": "high",
    "concept_keys": [
      "superintelligence",
      "scaling-hypothesis",
      "economic-singularity",
      "alignment",
      "industry-academia-divergence"
    ],
    "criteria_definition": "Superintelligence surpassing Nobel Prize winners across most fields including biology, mathematics, engineering, and computer science",
    "source_name": "Machines of Loving Grace essay (October 2024)",
    "source_url": "https://www.darioamodei.com/essay/machines-of-loving-grace",
    "headline": "Anthropic CEO Pens 15,000-Word Techno-Utopian Manifesto: Superintelligence by 2026, Probably",
    "headline_slug": "anthropic-ceos-superintelligence-manifesto",
    "tldr_summary": "Dario Amodei's essay 'Machines of Loving Grace' reads like a love letter to humanity's AI-powered future, with one small caveat: it assumes we nail the alignment problem. The Anthropic CEO argues superintelligence—AI surpassing Nobel laureates across most domains—could arrive 'as early as 2026' if current scaling trends continue. His central estimate clusters around 2027, though he hedges with 'could take much longer.' The essay spans biology, neuroscience, economic development, governance, and even 'work and meaning,' painting a picture of AI curing all diseases, solving poverty, and generally ushering in paradise. Amodei explains he usually doesn't talk about upsides because (1) market forces will deliver them anyway, (2) it sounds like propaganda, and (3) he finds messianic AI discourse distasteful. Then he proceeds to write 15,000 words of exactly that, complete with detailed scenarios of AI-accelerated scientific progress. His timeline shocked even optimists—2026 is *next year* from the essay's publication. The subtext: Anthropic's $7B in funding and race against OpenAI depends on this being true. If he's right, we're about to witness the fastest capability explosion in history. If he's wrong, it's the most expensive case of irrational exuberance since the dot-com bubble.",
    "target_date": "2027-01-01T00:00:00Z"
  },
  {
    "id": 68,
    "predictor_name": "Dario Amodei",
    "predictor_type": "Individual",
    "prediction_date": "2024-12-15",
    "predicted_date_low": "2026-01-01",
    "predicted_date_high": "2028-12-31",
    "predicted_date_best": "2027-01-01",
    "predicted_year_low": 2026,
    "predicted_year_high": 2028,
    "predicted_year_best": 2027,
    "prediction_type": "AGI",
    "confidence_level": "Very high confidence—'rapidly running out of truly convincing blockers' to AGI in stated timeframe",
    "confidence_label": "Confidently Urgent",
    "confidence_type": "high",
    "concept_keys": [
      "agi",
      "scaling-hypothesis",
      "industry-academia-divergence"
    ],
    "criteria_definition": "AGI with PhD-level then Nobel-level performance across most intellectual domains, with multimodal capabilities",
    "source_name": "Lex Fridman Podcast episode 452 (late 2024)",
    "source_url": "https://lexfridman.com/dario-amodei-transcript/",
    "headline": "Anthropic CEO Tells Lex Fridman 'We're Running Out of Blockers' to AGI by 2026-2027",
    "headline_slug": "anthropic-ceo-sees-rapid-path-to-agi",
    "tldr_summary": "When Dario Amodei sat down with Lex Fridman in December 2024, he delivered one of the most bullish AGI timelines on record. His key phrase: 'We are rapidly running out of truly convincing blockers, truly compelling reasons why this will not happen in the next few years.' Extrapolating from capability curves—high school level to undergraduate to PhD level in consecutive years—Amodei sees 2026-2027 as the window when AI matches Nobel Prize winners across most fields. He acknowledged 'there are still worlds where it doesn't happen in 100 years,' but immediately noted 'the number of those worlds is rapidly decreasing.' The confidence stems from Anthropic's internal scaling roadmaps and post-training breakthroughs. Amodei pointed to modalities being added (computer use, image generation) and capabilities climbing predictably. His timeline assumes no major surprises in compute availability or algorithmic progress—just continued scaling. The subtext: Anthropic's CEO has seen the training runs, knows what's in the pipeline, and thinks the blockers are gone. When someone with access to the most advanced models and compute clusters says we're out of excuses for why AGI won't arrive in 2-3 years, it's either the most informed prediction in history or the most spectacular case of tunnel vision. Probably both.",
    "target_date": "2027-01-01T00:00:00Z"
  },
  {
    "id": 69,
    "predictor_name": "Dario Amodei",
    "predictor_type": "Individual",
    "prediction_date": "2026-01-23",
    "predicted_date_low": "2026-01-01",
    "predicted_date_high": "2028-12-31",
    "predicted_date_best": "2027-01-01",
    "predicted_year_low": 2026,
    "predicted_year_high": 2028,
    "predicted_year_best": 2027,
    "prediction_type": "Superintelligence",
    "confidence_level": "Extremely high confidence with specific job displacement timelines: 100% of coders in 6-12 months, 50% of white-collar jobs in 5 years",
    "confidence_label": "Aggressively Confident",
    "confidence_type": "high",
    "concept_keys": [
      "superintelligence",
      "economic-singularity",
      "industry-academia-divergence"
    ],
    "criteria_definition": "Nobel-level scientific research across multiple fields, complete automation of software development, and massive white-collar job displacement",
    "source_name": "World Economic Forum Davos and The Adolescence of Technology essay (January 2026)",
    "source_url": "https://fortune.com/2026/01/23/deepmind-demis-hassabis-anthropic-dario-amodei-yann-lecun-ai-davos/",
    "headline": "Anthropic CEO Declares Software Engineers Extinct by Christmas 2026, Nobel-Level AI by 2027",
    "headline_slug": "anthropic-ceo-predicts-software-engineers-extinction",
    "tldr_summary": "Dario Amodei just put a countdown timer on every programmer's career. At Davos 2026, the Anthropic CEO declared AI will replace 'all software developers' within 6-12 months and reach 'Nobel-level' scientific research within two years. His prediction: 50% of white-collar jobs vanish within five years. The timeline is breathtakingly aggressive—he's not talking about AI assistance, but full replacement. Amodei's vision of 2027: 'a country of geniuses in a datacenter,' with millions of AI instances operating at superhuman speed across biology, mathematics, engineering, and computer science. His confidence clashed spectacularly with Demis Hassabis and Yann LeCun at the same conference, who argued we're 'nowhere near' AGI and need fundamental breakthroughs. But Amodei isn't backing down—his essay 'Machines of Loving Grace' drips with urgency about racing to build aligned superintelligence before someone builds the unaligned kind. The stakes: either he's right and we're 12 months from the biggest labor market disruption in history, or he's catastrophically wrong and Anthropic's $350B valuation is built on sand. No pressure.",
    "target_date": "2027-01-01T00:00:00Z"
  },
  {
    "id": 70,
    "predictor_name": "Dario Amodei",
    "predictor_type": "AI Entrepreneur",
    "prediction_date": "2025-03-01",
    "predicted_date_low": "2026-10-01",
    "predicted_date_high": "2027-03-31",
    "predicted_date_best": "2027-01-01",
    "predicted_year_low": 2026,
    "predicted_year_high": 2027,
    "predicted_year_best": 2027,
    "prediction_type": "AGI",
    "confidence_level": "Extremely high confidence—this is Anthropic's official company position submitted to White House OSTP, not just CEO speculation",
    "confidence_label": "Institutionally Committed",
    "confidence_type": "certain",
    "concept_keys": [
      "agi",
      "economic-singularity"
    ],
    "criteria_definition": "Powerful AI with intellectual capabilities matching Nobel Prize winners across most disciplines, scalable to millions of instances",
    "source_name": "Anthropic position paper 2025",
    "source_url": "https://www.lesswrong.com/posts/gabPgK9e83QrmcvbK/what-s-up-with-anthropic-predicting-agi-by-early-2027-1",
    "headline": "Anthropic Officially Tells White House: AGI by Early 2027, Also Please Regulate Us Before We Build It",
    "headline_slug": "anthropic-warns-white-house-about-imminent-agi",
    "tldr_summary": "This isn't a podcast hot take or conference soundbite—Anthropic submitted official recommendations to the White House Office of Science and Technology Policy in March 2025 stating they expect 'powerful AI systems' by late 2026 or early 2027. Their definition: Nobel Prize winner-level intelligence across biology, computer science, mathematics, and engineering. The kicker: cluster sizes will enable 'millions of AI instances' running simultaneously at superhuman speed, creating what Amodei calls 'a country of geniuses in a datacenter.' Some interpretations suggest 50 million genius-level agents. Anthropic's newer essay 'The Adolescence of Technology' (January 2026) describes this as 'the single most serious national security threat' in a century. So they're simultaneously building it and warning the government about it, which is either responsible or the most elaborate liability shield in corporate history. The company's $350B valuation is explicitly tied to this timeline's credibility—investors are betting Anthropic delivers superintelligence within 18 months. This is the forecasting equivalent of going all-in on a poker hand while filing an insurance claim on your chips. The 2027 countdown is now official U.S. government correspondence, which means when historians write about how we handled the AGI transition, this document will be Exhibit A.",
    "target_date": "2027-01-01T00:00:00Z"
  },
  {
    "id": 71,
    "predictor_name": "Dario Amodei",
    "predictor_type": "AI Entrepreneur",
    "prediction_date": "2025-03-01",
    "predicted_date_low": "2025-09-01",
    "predicted_date_high": "2026-09-01",
    "predicted_date_best": "2026-06-01",
    "predicted_year_low": 2025,
    "predicted_year_high": 2026,
    "predicted_year_best": 2026,
    "prediction_type": "Transformative AI",
    "confidence_level": "High confidence with specific 3-6 month timeline, though internal claims suggest threshold already exceeded by February 2026",
    "confidence_label": "Confidently Premature",
    "confidence_type": "high",
    "concept_keys": [
      "transformative-ai"
    ],
    "criteria_definition": "AI systems autonomously writing 90% of production code with minimal human intervention beyond scaffolding and review",
    "source_name": "Anthropic conference 2025",
    "source_url": "https://www.financialcontent.com/article/tokenring-2026-1-13-90-of-claudes-code-is-now-ai-written-anthropic-ceo-confirms-historic-shift-in-software-development",
    "headline": "Anthropic CEO Predicts AI Writing 90% of Code by Mid-2026, Then Claims It Already Happened",
    "headline_slug": "anthropic-ceo-predicts-ai-coding-revolution",
    "tldr_summary": "Dario Amodei made a bold March 2025 prediction that AI would write 90% of code within 3-6 months, placing the threshold around mid-2026. Fast forward to February 2026, and Anthropic CPO Mike Krieger declares at Cisco's AI Summit that \"Claude is now writing Claude\" with \"effectively 100%\" AI-generated code at Anthropic. The reality? More complicated. LessWrong analysis suggests the actual figure is \"closer to 90% than 50%\" when including throwaway scripts, but industry-wide adoption sits at 20-40%. The prediction's accuracy depends entirely on how you count—does edited AI code qualify? Do one-off scripts matter? Is human architectural oversight disqualifying? Anthropic may have technically hit their target internally while the rest of the industry lags far behind, making this less a prediction fulfilled and more a definitional shell game about what \"AI-written\" actually means.",
    "target_date": "2026-06-01T00:00:00Z"
  },
  {
    "id": 72,
    "predictor_name": "Dario Amodei / Anthropic",
    "predictor_type": "Individual",
    "prediction_date": "2025-03-01",
    "predicted_date_low": "2026-10-01",
    "predicted_date_high": "2027-03-01",
    "predicted_date_best": "2027-01-01",
    "predicted_year_low": 2026,
    "predicted_year_high": 2027,
    "predicted_year_best": 2027,
    "prediction_type": "AGI",
    "confidence_level": "Formal policy submission to White House OSTP representing official company position, not just CEO speculation",
    "confidence_label": "Officially Committed",
    "confidence_type": "high",
    "concept_keys": [
      "agi",
      "scaling-hypothesis",
      "economic-singularity",
      "industry-academia-divergence"
    ],
    "criteria_definition": "AI systems matching Nobel Prize-level intellectual capabilities across most disciplines with autonomous digital interface control",
    "source_name": "Anthropic OSTP submission for AI Action Plan (March 2025)",
    "source_url": "https://www.lesswrong.com/posts/gabPgK9e83QrmcvbK/what-s-up-with-anthropic-predicting-agi-by-early-2027-1",
    "headline": "Anthropic Files Official White House Papers: 'Country of Geniuses in a Datacenter' by Early 2027",
    "headline_slug": "anthropic-files-white-house-ai-policy-papers",
    "tldr_summary": "In a formal March 2025 submission to the White House Office of Science and Technology Policy, Anthropic didn't just predict AGI—they made it official U.S. government policy discourse. The company expects \"powerful AI\" matching Nobel laureates across biology, computer science, math, and engineering by late 2026 or early 2027, describing it as \"a country of geniuses in a datacenter.\" This isn't Dario Amodei musing on a podcast; it's Anthropic's institutional position submitted to inform federal AI policy. The document requests 50 gigawatts of dedicated power by 2027 (enough to run multiple cities) because training a single frontier model will soon require 5 gigawatts. They're asking the White House to prepare for economic transformation, modernize data collection, and strengthen national security—all predicated on superhuman AI arriving within the current presidential administration. When you're filing official paperwork requesting continental-scale power infrastructure, you're not hedging.",
    "target_date": "2027-01-01T00:00:00Z"
  },
  {
    "id": 73,
    "predictor_name": "Demis Hassabis",
    "predictor_type": "Individual",
    "prediction_date": "2026-01-23",
    "predicted_date_low": "2028-01-01",
    "predicted_date_high": "2035-12-31",
    "predicted_date_best": "2030-06-01",
    "predicted_year_low": 2028,
    "predicted_year_high": 2035,
    "predicted_year_best": 2030,
    "prediction_type": "AGI",
    "confidence_level": "50% probability within the decade, acknowledging 'one or two more breakthroughs' needed beyond current techniques",
    "confidence_label": "Cautiously Probabilistic",
    "confidence_type": "medium",
    "concept_keys": [
      "agi",
      "scaling-hypothesis",
      "industry-academia-divergence"
    ],
    "criteria_definition": "Human-level artificial general intelligence requiring fundamental architectural breakthroughs beyond current LLM scaling",
    "source_name": "World Economic Forum Davos (January 2026)",
    "source_url": "https://fortune.com/2026/01/23/deepmind-demis-hassabis-anthropic-dario-amodei-yann-lecun-ai-davos/",
    "headline": "DeepMind's Hassabis Plays Grown-Up at Davos: 50% Odds AGI by 2030, 'Nowhere Near' Today",
    "headline_slug": "demis-hassabis-gives-cautious-agi-predictions",
    "tldr_summary": "While Dario Amodei promised Nobel-level AI by 2027 and Sam Altman claimed we're \"slipping past AGI\" already, Demis Hassabis brought actual scientific rigor to Davos 2026. The Nobel laureate and DeepMind CEO gave 50% probability to AGI \"within the decade,\" emphasizing current systems are \"nowhere near\" human-level intelligence and require \"one or two more breakthroughs\" comparable to Transformers or AlphaGo. His measured stance contrasts sharply with Anthropic and OpenAI's certainty, creating an unusual spectacle: AI luminaries publicly disagreeing on whether humanity's most significant technological threshold arrives in 18 months or 5+ years. Hassabis identified key gaps including few-shot learning and continuous learning capabilities. When the guy who built AlphaGo, AlphaFold, and Gemini says we need fundamental breakthroughs—not just more compute—it's worth listening. He's the only CEO at Davos whose predictions come with actual Nobel Prize credentials in the relevant field.",
    "target_date": "2030-06-01T00:00:00Z"
  },
  {
    "id": 74,
    "predictor_name": "Demis Hassabis",
    "predictor_type": "AI Researcher/Entrepreneur",
    "prediction_date": "2025-12-05",
    "predicted_date_low": "2029-07-01",
    "predicted_date_high": "2030-06-01",
    "predicted_date_best": "2030-01-01",
    "predicted_year_low": 2029,
    "predicted_year_high": 2030,
    "predicted_year_best": 2030,
    "prediction_type": "AGI",
    "confidence_level": "Approximately 50% probability, with public precision splitting hairs between 'just before' and 'just after' 2030",
    "confidence_label": "Precisely Hedging",
    "confidence_type": "medium",
    "concept_keys": [
      "agi",
      "scaling-hypothesis"
    ],
    "criteria_definition": "AI systems capable of outthinking humans across general domains, requiring Transformer-level or AlphaGo-level breakthrough innovations",
    "source_name": "Axios December 2025",
    "source_url": "https://www.axios.com/2025/12/05/ai-deepmind-gemini-agi",
    "headline": "Hassabis and Brin Split Hairs Over 2030: AGI Arrives 'Just Before' or 'Just After'?",
    "headline_slug": "hassabis-and-brin-debate-agi-timeline",
    "tldr_summary": "At the December 2025 Axios AI+ Summit, Demis Hassabis doubled down on his ~2030 AGI timeline but emphasized \"we're definitely not there now\" and need \"one or two more big breakthroughs\" at the level of Transformers or AlphaGo. The most entertaining moment came earlier at Google I/O when Sergey Brin crashed the stage and the two publicly disagreed on whether AGI arrives \"just before\" (Brin) or \"just after\" (Hassabis) 2030—splitting hairs over mere months when predicting humanity's most consequential technological threshold. This precision in hedging reveals how seriously Google takes these predictions while simultaneously acknowledging massive uncertainty. Hassabis stressed that current scaling alone won't suffice; fundamental paradigm shifts are required. When two of the world's smartest technologists can't agree within a 12-month window on a 5-year prediction, it tells you everything about the epistemic chaos surrounding AGI timelines. At least they're honest about not knowing.",
    "target_date": "2030-01-01T00:00:00Z"
  },
  {
    "id": 75,
    "predictor_name": "Demis Hassabis",
    "predictor_type": "AI Researcher/Entrepreneur",
    "prediction_date": "2025-05-21",
    "predicted_date_low": "2029-07-01",
    "predicted_date_high": "2030-06-01",
    "predicted_date_best": "2030-01-01",
    "predicted_year_low": 2029,
    "predicted_year_high": 2030,
    "predicted_year_best": 2030,
    "prediction_type": "AGI",
    "confidence_level": "Moderate confidence with emphasis on needing both scaling and algorithmic breakthroughs, with Brin suggesting algorithmic advances may be more significant than compute",
    "confidence_label": "Algorithmic Optimism",
    "confidence_type": "medium",
    "concept_keys": [
      "agi",
      "scaling-hypothesis"
    ],
    "criteria_definition": "AGI matching or surpassing most human capabilities across general domains through combination of scaling and novel algorithmic approaches",
    "source_name": "Google I/O 2025",
    "source_url": "https://www.axios.com/2025/05/21/google-sergey-brin-demis-hassabis-agi-2030",
    "headline": "Google I/O Surprise: Brin Crashes Stage, Declares AGI 'Just Before 2030' While Hassabis Hedges",
    "headline_slug": "brin-surprises-at-google-io-with-agi-prediction",
    "tldr_summary": "In one of 2025's most entertaining tech moments, Sergey Brin made a surprise appearance at Google I/O, crashing Demis Hassabis's on-stage interview to weigh in on AGI timelines. Both agreed on \"around 2030\" but split on specifics: Brin picked \"just before,\" Hassabis chose \"just after.\" Beyond the theater, they revealed substantive technical disagreement about the path forward. Hassabis insisted both scaling current techniques to their limits AND discovering what comes next are essential. Brin went further, suggesting algorithmic advances might matter even more than raw compute increases—\"we're kind of getting the benefits of both\" right now. The appearance of reasoning models (which compute longer before responding) might represent partial progress toward one required breakthrough. When Google's co-founder and DeepMind's CEO can publicly disagree within a 12-month window while both maintaining ~2030 timelines, it reveals both the seriousness and uncertainty surrounding these predictions.",
    "target_date": "2030-01-01T00:00:00Z"
  },
  {
    "id": 76,
    "predictor_name": "Demis Hassabis",
    "predictor_type": "AI Researcher/Entrepreneur",
    "prediction_date": "2025-01-15",
    "predicted_date_low": "2028-01-01",
    "predicted_date_high": "2032-12-31",
    "predicted_date_best": "2030-01-01",
    "predicted_year_low": 2028,
    "predicted_year_high": 2032,
    "predicted_year_best": 2030,
    "prediction_type": "Transformative AI",
    "confidence_level": "Conditional confidence requiring 1-2 major breakthroughs at AlphaGo/Transformer level, with timeline revised from 10 years to 3-5 years",
    "confidence_label": "Breakthrough Dependent",
    "confidence_type": "medium",
    "concept_keys": [
      "transformative-ai",
      "scaling-hypothesis"
    ],
    "criteria_definition": "AGI requiring world models enabling physics understanding and automated experimentation systems for hands-on problem solving",
    "source_name": "January 2025 interviews",
    "source_url": "https://eu.36kr.com/en/p/3646186509667977",
    "headline": "Hassabis Revises Timeline: AGI Now 3-5 Years Away, Down From 10, But Breakthroughs Required",
    "headline_slug": "hassabis-shortens-agi-timeline-to-3-5-years",
    "tldr_summary": "Demis Hassabis has systematically shortened his AGI timeline from \"approximately 10 years\" to \"3-5 years\" as progress accelerates, but consistently emphasizes that scaling alone won't suffice—fundamental breakthroughs remain essential. Speaking at various 2025 events, he specified two critical missing pieces: world models enabling true physics and spatial understanding, plus automated experimentation systems allowing AI to solve fundamental problems through hands-on work (think materials science and fusion research). He predicts agent systems will become \"really impressive and reliable\" within 2-3 years as an intermediate milestone. The kicker: AGI's societal impact will be \"10 times bigger than the Industrial Revolution\" but compressed into a decade rather than a century. Hassabis's evolving timeline reflects genuine learning rather than hype—he's updating based on actual progress while maintaining intellectual honesty about remaining technical barriers. When a Nobel laureate systematically shortens their timeline while simultaneously emphasizing unsolved problems, pay attention to both the acceleration and the caveats.",
    "target_date": "2030-01-01T00:00:00Z"
  },
  {
    "id": 77,
    "predictor_name": "Demis Hassabis",
    "predictor_type": "AI Researcher/Entrepreneur",
    "prediction_date": "2024-12-15",
    "predicted_date_low": "2030-01-01",
    "predicted_date_high": "2035-12-31",
    "predicted_date_best": "2032-01-01",
    "predicted_year_low": 2030,
    "predicted_year_high": 2035,
    "predicted_year_best": 2032,
    "prediction_type": "AGI",
    "confidence_level": "Moderate confidence tempered by acknowledgment that 2-3 major breakthroughs comparable to deep RL and Transformers are still required",
    "confidence_label": "Intellectually Honest",
    "confidence_type": "medium",
    "concept_keys": [
      "agi",
      "scaling-hypothesis"
    ],
    "criteria_definition": "AGI requiring paradigm-shifting breakthroughs beyond incremental LLM improvements, comparable to historical innovations like Transformers",
    "source_name": "DeepMind CEO statements late 2024",
    "source_url": "https://www.axios.com/2025/12/05/ai-deepmind-gemini-agi",
    "headline": "Hassabis at Gemini 2.0 Launch: 'The Goalposts Keep Moving,' 2-3 Major Breakthroughs Still Needed",
    "headline_slug": "hassabis-reveals-gemini-2-0-ai-challenges",
    "tldr_summary": "During the December 2024 Gemini 2.0 launch, Demis Hassabis demonstrated rare intellectual honesty among tech CEOs: admitting \"the goalposts keep moving\" and that large language models alone won't achieve AGI. He specified that 2-3 additional breakthroughs at the level of deep reinforcement learning (which enabled AlphaGo) or Transformers (which enabled GPT/Claude/Gemini) are still necessary. These aren't incremental improvements—they're paradigm shifts unlocking entirely new capabilities. While Gemini 2.0 represents progress toward AI agents that can take autonomous action, Hassabis emphasized this is an intermediate step, not the finish line. His willingness to state publicly that fundamental technical challenges remain—rather than claiming AGI is just more scaling away—stands in stark contrast to competitors promising Nobel-level AI within 18 months. When the CEO who built AlphaGo and AlphaFold says current approaches need qualitative (not just quantitative) advances, he's not selling hype. He's outlining the actual remaining work.",
    "target_date": "2032-01-01T00:00:00Z"
  },
  {
    "id": 78,
    "predictor_name": "Douglas Hofstadter",
    "predictor_type": "Cognitive Scientist",
    "prediction_date": "2025-01-01",
    "predicted_date_low": "2040-01-01",
    "predicted_date_high": "2100-12-31",
    "predicted_date_best": "2070-01-01",
    "predicted_year_low": 2040,
    "predicted_year_high": 2100,
    "predicted_year_best": 2070,
    "prediction_type": "Superintelligence",
    "confidence_level": "Low confidence that current approaches can achieve genuine consciousness without recursive self-referential architecture",
    "confidence_label": "Philosophically Skeptical",
    "confidence_type": "low",
    "concept_keys": [
      "superintelligence",
      "scaling-hypothesis"
    ],
    "criteria_definition": "True AI consciousness requiring strange loops of self-reference and recursive self-awareness, not mere computational capability",
    "source_name": "Hofstadter cognitive science work",
    "source_url": "https://en.wikipedia.org/wiki/Douglas_Hofstadter",
    "headline": "Hofstadter's Strange Loop Problem: Real AI Consciousness Might Take Until 2100 (Or Never)",
    "headline_slug": "hofstadter-questions-ai-consciousness-until-2100",
    "tldr_summary": "Douglas Hofstadter, the Pulitzer Prize-winning cognitive scientist who wrote \"Gödel, Escher, Bach,\" remains deeply skeptical that current AI approaches can achieve genuine consciousness. His work on \"strange loops\"—recursive, self-referential patterns that create consciousness from self-awareness observing itself—suggests that true AI consciousness requires fundamentally different architecture than today's LLMs. While frontier labs race toward 2027-2030 AGI timelines focused on capability benchmarks, Hofstadter's framework asks a harder question: can systems without genuine self-referential loops ever be truly conscious, or are we just building very sophisticated pattern-matching machines? His implicit timeline stretches decades or potentially to 2100, not because the computational power is lacking, but because we may not yet understand what consciousness actually requires. It's the difference between an AI that can pass every test and an AI that actually experiences passing tests. Hofstadter's coffee mug has more genuine self-awareness than GPT-4, and he's not convinced scaling fixes that.",
    "target_date": "2070-01-01T00:00:00Z"
  },
  {
    "id": 79,
    "predictor_name": "Douglas Hofstadter",
    "predictor_type": "Cognitive Scientist",
    "prediction_date": "2023-01-01",
    "predicted_date_low": "2040-01-01",
    "predicted_date_high": "2100-12-31",
    "predicted_date_best": "2070-01-01",
    "predicted_year_low": 2040,
    "predicted_year_high": 2100,
    "predicted_year_best": 2070,
    "prediction_type": "Superintelligence",
    "confidence_level": "Low confidence that consciousness emerges from current architectures without solving the hard problem of recursive self-reference",
    "confidence_label": "Fundamentally Unconvinced",
    "confidence_type": "low",
    "concept_keys": [
      "superintelligence"
    ],
    "criteria_definition": "Genuine consciousness requiring strange loops and recursive self-awareness, not computational sophistication alone",
    "source_name": "Hofstadter interviews",
    "source_url": "https://en.wikipedia.org/wiki/Douglas_Hofstadter",
    "headline": "Strange Loops Required: Hofstadter Says Real AI Consciousness Needs More Than Scaling",
    "headline_slug": "hofstadter-warns-against-simple-ai-scaling",
    "tldr_summary": "While AI labs promise AGI by 2027, Douglas Hofstadter—whose 1979 \"Gödel, Escher, Bach\" remains the definitive work on consciousness and self-reference—asks an inconvenient question: are we building intelligence or just really good autocomplete? His theory of \"strange loops\" suggests consciousness emerges from recursive self-referential patterns where awareness observes itself observing itself, creating genuine subjective experience. Current LLMs, no matter how large, lack this fundamental architecture. They're sophisticated pattern matchers without genuine interiority. Hofstadter's implicit timeline stretches to 2070 or beyond because he's not convinced we've even started building the right kind of system. It's like trying to create consciousness by making calculators bigger—you might get impressive calculations, but you won't get experience. His work suggests that until we solve the hard problem of how recursive self-reference creates subjective awareness, we're not building conscious AI, we're building increasingly convincing simulations of it. The coffee mug on his desk has more genuine self-awareness than Claude.",
    "target_date": "2070-01-01T00:00:00Z"
  },
  {
    "id": 80,
    "predictor_name": "Eliezer Yudkowsky",
    "predictor_type": "AI Safety Researcher",
    "prediction_date": "2024-01-01",
    "predicted_date_low": "2025-01-01",
    "predicted_date_high": "2030-12-31",
    "predicted_date_best": "2027-06-01",
    "predicted_year_low": 2025,
    "predicted_year_high": 2030,
    "predicted_year_best": 2027,
    "prediction_type": "Singularity",
    "confidence_level": "Very high confidence (personal p(doom) ranges from 10% to near-100% depending on context) that AGI poses existential risk",
    "confidence_label": "Apocalyptically Certain",
    "confidence_type": "high",
    "concept_keys": [
      "event-horizon",
      "recursive-self-improvement",
      "alignment"
    ],
    "criteria_definition": "Artificial general intelligence capable of recursive self-improvement leading to existential catastrophe without alignment breakthroughs",
    "source_name": "Multiple 2024-2025 interviews",
    "source_url": "https://en.wikipedia.org/wiki/P(doom)",
    "headline": "Yudkowsky's P(Doom): 10% to 'Near-100%' Chance AI Kills Everyone, Advocates Worldwide Moratorium",
    "headline_slug": "yudkowsky-estimates-high-probability-of-ai-catastrophe",
    "tldr_summary": "Eliezer Yudkowsky, the AI safety researcher who literally invented the term \"friendly AI\" in 2001, has become the movement's most apocalyptic voice. His \"p(doom)\"—probability of existential catastrophe from AI—ranges from 10% on optimistic days to approaching 100% depending on how the question is framed. Unlike other researchers hedging with 5% or 20% estimates, Yudkowsky argues we're fundamentally unprepared for AGI's arrival sometime between 2025-2030, advocating for an immediate worldwide moratorium on frontier AI development. His position: we don't know how to align superintelligent systems, recursive self-improvement could happen extremely fast once triggered, and by the time we realize we've lost control it will be too late to course-correct. While mainstream AI researchers dismiss him as alarmist, Yudkowsky points out he's been consistently right about capability timelines since the 1990s—just ask anyone who laughed at his 2008 predictions about deep learning. His p(doom) has only increased as capabilities advance faster than alignment research. When the guy who founded the field says we're probably doomed, it's worth considering he might know something.",
    "target_date": "2027-06-01T00:00:00Z"
  },
  {
    "id": 81,
    "predictor_name": "Eliezer Yudkowsky",
    "predictor_type": "AI Safety Researcher",
    "prediction_date": "2023-03-29",
    "predicted_date_low": "2024-01-01",
    "predicted_date_high": "2030-12-31",
    "predicted_date_best": "2027-01-01",
    "predicted_year_low": 2024,
    "predicted_year_high": 2030,
    "predicted_year_best": 2027,
    "prediction_type": "Superintelligence",
    "confidence_level": "Expects literal extinction as 'the obvious thing that would happen' - not remote possibility but default outcome without precision alignment breakthroughs",
    "confidence_label": "Apocalyptically Certain",
    "confidence_type": "certain",
    "concept_keys": [
      "superintelligence",
      "alignment"
    ],
    "criteria_definition": "Superintelligent AI that doesn't care about humans and can outmaneuver all attempts at containment",
    "source_name": "TIME magazine / Various 2024",
    "source_url": "https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/",
    "headline": "AI Safety Researcher Says 6-Month Pause 'Understates Seriousness'—Wants Permanent Shutdown Instead",
    "headline_slug": "ai-safety-expert-demands-permanent-ai-halt",
    "tldr_summary": "While 1,000+ researchers signed an open letter calling for a six-month AI development pause, Eliezer Yudkowsky publicly refused to sign it—not because he opposed slowing down, but because he thought it asked for far too little. The MIRI co-founder argues that building superintelligent AI under current circumstances means 'literally everyone on Earth will die,' comparing humanity's odds to 'a 10-year-old playing chess against Stockfish 15' or 'Australopithecus fighting Homo sapiens.' His prescription? An immediate, indefinite, worldwide shutdown of all frontier AI development, enforced by international treaty with military consequences for violations. Yudkowsky's position represents the most extreme end of AI safety concerns: that alignment is so difficult and the stakes so absolute that the only rational response is complete cessation. His critics note this stance offers no path forward in a world where competitive pressures make coordination nearly impossible.",
    "target_date": "2027-01-01T00:00:00Z"
  },
  {
    "id": 82,
    "predictor_name": "Elon Musk",
    "predictor_type": "Individual",
    "prediction_date": "2020-07-01",
    "predicted_date_low": "2025-01-01",
    "predicted_date_high": "2025-12-31",
    "predicted_date_best": "2025-06-01",
    "predicted_year_low": 2025,
    "predicted_year_high": 2025,
    "predicted_year_best": 2025,
    "prediction_type": "AGI",
    "confidence_level": "Stated as definitive timeline without hedging or probabilistic qualification",
    "confidence_label": "Confidently Premature",
    "confidence_type": "high",
    "concept_keys": [
      "agi",
      "industry-academia-divergence"
    ],
    "criteria_definition": "AI surpassing all human intelligence across all domains",
    "source_name": "New York Times interview (July 2020)",
    "source_url": "https://www.diamandis.com/blog/age-of-abundance-30-human-level-ai",
    "headline": "Musk's 2020 Prophecy: AGI Arrives 2025, Just Like Full Self-Driving Arrived 2017, 2018, 2019, 2020...",
    "headline_slug": "elon-musks-recurring-ai-timeline-prophecies",
    "tldr_summary": "In July 2020, before GPT-3 had even been released to the public and years before he'd launch his own xAI company, Elon Musk declared that artificial intelligence would overtake all human capabilities by 2025. The prediction came from the man who's been promising 'full self-driving next year' since 2016, a track record that makes his AGI timeline particularly spicy. What's remarkable isn't just the aggressive five-year window, but that Musk made this call when the AI landscape looked radically different—no ChatGPT, no GPT-4, no widespread transformer model dominance. Yet by early 2025, with models like GPT-4 and Claude showing startling capabilities, Musk's timeline looked less absurd than when he made it. His pattern: predict the impossible on aggressive timelines, miss the exact date, hit something directionally close, declare victory, repeat. The 2025 AGI prediction aged better than his Mars colony or robotaxi timelines, even if AGI itself remains elusive.",
    "target_date": "2025-06-01T00:00:00Z"
  },
  {
    "id": 83,
    "predictor_name": "Elon Musk",
    "predictor_type": "Individual",
    "prediction_date": "2023-07-01",
    "predicted_date_low": "2028-01-01",
    "predicted_date_high": "2029-12-31",
    "predicted_date_best": "2029-01-01",
    "predicted_year_low": 2028,
    "predicted_year_high": 2029,
    "predicted_year_best": 2029,
    "prediction_type": "Superintelligence",
    "confidence_level": "Confident projection based on observed capability acceleration and compute scaling",
    "confidence_label": "Recalibrating Optimist",
    "confidence_type": "high",
    "concept_keys": [
      "superintelligence",
      "scaling-hypothesis",
      "industry-academia-divergence"
    ],
    "criteria_definition": "Digital superintelligence exceeding the smartest human at every cognitive task",
    "source_name": "Twitter/X Spaces discussion (July 2023)",
    "source_url": "https://www.diamandis.com/blog/age-of-abundance-30-human-level-ai",
    "headline": "After Missing 2025 AGI Call, Musk Slides Timeline to 2029 Superintelligence—Definitely Final Answer",
    "headline_slug": "musk-slides-superintelligence-prediction-again",
    "tldr_summary": "Three years after predicting AGI by 2025, Elon Musk recalibrated to 2028-2029 for 'digital superintelligence'—AI that's smarter than any human at everything, not just narrow domains. The revision came as Musk was building xAI and assembling one of the world's largest GPU clusters for training Grok, his answer to ChatGPT. Unlike his 2020 prediction made in relative isolation, this timeline emerged from direct involvement in frontier AI development, giving it more grounding in actual technical constraints around compute, data, and algorithmic progress. The 5-6 year window from 2023 maintains Musk's characteristic aggressive optimism while acknowledging that AGI proved harder than his 2020 self expected. Critics note this follows Musk's standard playbook: make bold prediction, miss deadline, revise timeline forward while maintaining aggressive stance, continue building toward the goal anyway. The difference? This time he's putting tens of billions of dollars behind it through xAI, suggesting he's betting his capital alongside his credibility.",
    "target_date": "2029-01-01T00:00:00Z"
  },
  {
    "id": 84,
    "predictor_name": "Elon Musk",
    "predictor_type": "Individual",
    "prediction_date": "2024-04-09",
    "predicted_date_low": "2025-01-01",
    "predicted_date_high": "2026-12-31",
    "predicted_date_best": "2025-06-01",
    "predicted_year_low": 2025,
    "predicted_year_high": 2026,
    "predicted_year_best": 2025,
    "prediction_type": "AGI",
    "confidence_level": "Direct statement to major institutional investor without probabilistic hedging",
    "confidence_label": "Optimistically Undeterred",
    "confidence_type": "high",
    "concept_keys": [
      "agi",
      "economic-singularity",
      "industry-academia-divergence"
    ],
    "criteria_definition": "AI smarter than the smartest human across general intelligence domains",
    "source_name": "Interview with Norway sovereign wealth fund CEO (April 2024)",
    "source_url": "https://www.business-standard.com/technology/tech-news/elon-musk-says-ai-will-gain-general-intelligence-outsmart-humans-by-2025-124040900177_1.html",
    "headline": "Musk Tells $1.6 Trillion Wealth Fund: AGI 'Probably Next Year'—What Could Go Wrong?",
    "headline_slug": "musk-tells-wealth-fund-agi-is-imminent",
    "tldr_summary": "In an April 2024 interview with Norway's sovereign wealth fund CEO—one of Tesla's largest shareholders managing $1.6 trillion—Elon Musk casually predicted AGI would arrive 'probably next year, within two years.' The conversation covered Tesla's Swedish labor disputes and Chinese EV competition, but Musk's AGI timeline dropped like a bombshell: artificial general intelligence smarter than the smartest human by 2025 or 2026. He blamed chip shortages and electricity constraints for any potential delays, noting Grok 2 needed 20,000 Nvidia H100 GPUs while Grok 3 would require 100,000. The boldness of predicting AGI within 8-20 months to a major institutional investor showcased either extraordinary conviction or extraordinary optimism—possibly both. Musk's willingness to state such aggressive timelines without hedging to sophisticated financial stakeholders suggests he genuinely believes xAI and competitors are on the cusp, even if his historical timeline accuracy suggests adding 2-3 years to any Musk prediction yields better calibration.",
    "target_date": "2025-06-01T00:00:00Z"
  },
  {
    "id": 85,
    "predictor_name": "Elon Musk",
    "predictor_type": "Individual",
    "prediction_date": "2024-05-15",
    "predicted_date_low": "2025-01-01",
    "predicted_date_high": "2025-12-31",
    "predicted_date_best": "2025-06-01",
    "predicted_year_low": 2025,
    "predicted_year_high": 2025,
    "predicted_year_best": 2025,
    "prediction_type": "AGI",
    "confidence_level": "Single-word reply without caveats, percentages, or conditional statements",
    "confidence_label": "Casually Definitive",
    "confidence_type": "certain",
    "concept_keys": [
      "agi",
      "industry-academia-divergence"
    ],
    "criteria_definition": "Artificial general intelligence matching or exceeding human cognitive capabilities broadly",
    "source_name": "X post replying to Logan Kilpatrick (May 2024)",
    "source_url": "https://www.analyticsvidhya.com/blog/2024/04/elon-musk-predicts-ai-will-be-smarter-than-humans-by-next-year/",
    "headline": "Asked 'How Long Until AGI?' Musk Replies 'Next Year'—No Elaboration, No Hedging, No Chill",
    "headline_slug": "musk-declares-agi-arrival-next-year",
    "tldr_summary": "When Google AI Studio's Logan Kilpatrick asked on X 'How long until AGI?' in May 2024, Elon Musk delivered the most Elon response possible: 'Next year.' Two words. Zero hedging. No 'probably' or 'if current trends continue' or 'depending on how you define it.' Just a flat prediction pinning AGI to 2025, delivered with the casual certainty of someone ordering lunch. The exchange perfectly encapsulates Musk's approach to AI timelines—aggressively optimistic, utterly unhedged, and completely unbothered by the possibility of being wrong. It's the same energy that's powered 'full self-driving next year' since 2016 and 'humans on Mars by 2024' proclamations. Yet this particular prediction, made just months before some truly impressive AI capabilities emerged in late 2024, looked less ridiculous than many expected. Musk's pattern: state the impossible as inevitable, miss the exact date, hit something directionally interesting, never acknowledge the miss, make next prediction. The single-word 'next year' reply became instantly memeable, crystallizing both Musk's boldness and his critics' skepticism in one perfect tweet.",
    "target_date": "2025-06-01T00:00:00Z"
  },
  {
    "id": 86,
    "predictor_name": "Elon Musk",
    "predictor_type": "Individual",
    "prediction_date": "2026-01-06",
    "predicted_date_low": "2026-01-01",
    "predicted_date_high": "2027-12-31",
    "predicted_date_best": "2026-06-01",
    "predicted_year_low": 2026,
    "predicted_year_high": 2027,
    "predicted_year_best": 2026,
    "prediction_type": "AGI",
    "confidence_level": "Frames prediction as engineering calculation rather than speculation, with 10X annual improvement as basis",
    "confidence_label": "Engineering Inevitability",
    "confidence_type": "high",
    "concept_keys": [
      "agi",
      "scaling-hypothesis",
      "industry-academia-divergence"
    ],
    "criteria_definition": "True AGI capable of matching human cognitive performance across all domains",
    "source_name": "Moonshots podcast with Peter Diamandis (January 7 2026)",
    "source_url": "https://www.nextbigfuture.com/2026/01/elon-musk-expects-true-agi-in-2026-2027-and-superintelligence-about-2030.html",
    "headline": "Musk Calls AGI for 2026: 'It's Engineering Math, Not Prophecy' (Narrator: It Was Prophecy)",
    "headline_slug": "elon-musks-bold-agi-2026-engineering-prediction",
    "tldr_summary": "In a sweeping January 2026 podcast with Peter Diamandis, Elon Musk declared that 'true AGI' would arrive in 2026-2027, framing it not as wild speculation but as straightforward engineering calculation. His logic: intelligence density per gigabyte can improve 100X through algorithmic advances, with 10X annual improvement 'for the foreseeable future.' Musk positioned this as inevitable physics rather than hopeful forecasting, the kind of confidence that comes from running xAI and seeing the scaling curves firsthand. He then extended the timeline to superintelligence by 2030, when AI would exceed the combined intelligence of all humans. The prediction came with a side of converted beliefs—Musk announced he'd changed his mind on longevity research, now embracing anti-aging because he wants to personally explore the universe that superintelligent AI will help humans access. It's classic Musk: the man who promised Mars colonies by 2024 and self-driving cars by 2020 now pegs machine superintelligence at 2030, calling it 'engineering calculation' while critics call it 'aggressive optimism with capital behind it.'",
    "target_date": "2026-06-01T00:00:00Z"
  },
  {
    "id": 87,
    "predictor_name": "Elon Musk",
    "predictor_type": "Individual",
    "prediction_date": "2026-01-06",
    "predicted_date_low": "2030-01-01",
    "predicted_date_high": "2031-12-31",
    "predicted_date_best": "2030-06-01",
    "predicted_year_low": 2030,
    "predicted_year_high": 2031,
    "predicted_year_best": 2030,
    "prediction_type": "Superintelligence",
    "confidence_level": "High confidence with explicit acknowledgment of 'bumpy' transition period causing potential social unrest",
    "confidence_label": "Turbulence Acknowledged",
    "confidence_type": "high",
    "concept_keys": [
      "superintelligence",
      "accelerating-change",
      "industry-academia-divergence"
    ],
    "criteria_definition": "AI exceeding the combined intelligence of all humans, fundamentally transforming civilization",
    "source_name": "Moonshots podcast and Davos interview (January 2026)",
    "source_url": "https://www.nextbigfuture.com/2026/01/elon-musk-expects-true-agi-in-2026-2027-and-superintelligence-about-2030.html",
    "headline": "Musk Maps 'Bumpy 3-7 Year Transition' to 2030 Superintelligence: Star Trek, Not Terminator (He Hopes)",
    "headline_slug": "musks-roadmap-to-superintelligence-by-2030",
    "tldr_summary": "Elon Musk outlined his full AGI-to-superintelligence roadmap in January 2026: true AGI arrives 2026-2027, followed by a 'very bumpy' 3-7 year transition period, culminating in superintelligence around 2030 when AI surpasses the combined intelligence of all humanity. Unlike pure techno-optimists, Musk explicitly acknowledged the turbulence ahead—'potentially causing massive social unrest despite rising prosperity.' He framed AI and robotics as a 'supersonic tsunami already accelerating, no off-switch, no deliberate slowdown possible,' positioning the transformation as inevitable regardless of policy interventions. Musk's vision skews optimistic—'Star Trek future, not Terminator'—with robot surgeons better than the best humans by 2030 and an 'age of extreme abundance' driven by AI and humanoid robots. The timeline positions xAI as racing to build aligned superintelligence before others build unaligned versions, a framing that justifies aggressive development despite acknowledged risks. Critics note Musk's track record suggests adding 3-5 years to any stated timeline, which would push superintelligence to 2033-2035—still remarkably soon for such a civilization-altering transition.",
    "target_date": "2030-06-01T00:00:00Z"
  },
  {
    "id": 88,
    "predictor_name": "Elon Musk",
    "predictor_type": "Entrepreneur/Tech Executive",
    "prediction_date": "2026-01-15",
    "predicted_date_low": "2029-01-01",
    "predicted_date_high": "2030-12-31",
    "predicted_date_best": "2029-06-01",
    "predicted_year_low": 2029,
    "predicted_year_high": 2030,
    "predicted_year_best": 2029,
    "prediction_type": "Singularity",
    "confidence_level": "Stated as near-term certainty in public forum addressing global business leaders",
    "confidence_label": "Bluntly Disruptive",
    "confidence_type": "high",
    "concept_keys": [
      "event-horizon",
      "economic-singularity"
    ],
    "criteria_definition": "AI capable of replacing human cognitive labor across professional knowledge work",
    "source_name": "xAI/Tesla 2026",
    "source_url": "https://www.oreateai.com/blog/elon-musk-prediction/",
    "headline": "Musk at Davos: 'Your Job Should Not Include Thinking' by 2029—Cognitive Workers Obsolete in 3 Years",
    "headline_slug": "musk-warns-of-cognitive-worker-obsolescence",
    "tldr_summary": "At the January 2026 World Economic Forum in Davos, Elon Musk delivered perhaps his most provocative AI prediction yet: within 3-4 years (by 2029), artificial intelligence will handle most cognitive tasks currently performed by human professionals. His message to knowledge workers? 'Your job should not include thinking.' Musk predicted AI will surpass any single human's cognitive abilities by end of 2026, then exceed all of humanity's combined intelligence by 2030-2031. He tied these predictions to xAI's upcoming Grok 5 model and framed the next 2-3 years as a 'make-or-break window' in the AI race. The prediction extends beyond routine automation to fundamental replacement of human cognition in professional contexts—lawyers, doctors, engineers, analysts, all facing obsolescence within the current business planning horizon. Musk paired this with equally bold claims about robot surgeons in 3 years, significant life extension, and no need for retirement savings due to AI-driven abundance. The Davos setting—addressing global business and political elites—gave the predictions extra weight, though Musk's historical timeline accuracy suggests the transformation may take longer than 2029 even if the direction proves correct.",
    "target_date": "2029-06-01T00:00:00Z"
  },
  {
    "id": 89,
    "predictor_name": "Elon Musk",
    "predictor_type": "Entrepreneur/Tech Executive",
    "prediction_date": "2026-01-06",
    "predicted_date_low": "2028-01-01",
    "predicted_date_high": "2030-12-31",
    "predicted_date_best": "2029-01-01",
    "predicted_year_low": 2028,
    "predicted_year_high": 2030,
    "predicted_year_best": 2029,
    "prediction_type": "Singularity",
    "confidence_level": "Framed as inevitable transformation reshaping employment structures and scientific progress",
    "confidence_label": "Productivity Apocalypse",
    "confidence_type": "high",
    "concept_keys": [
      "event-horizon",
      "scaling-hypothesis"
    ],
    "criteria_definition": "AI systems dominating scientific research and innovation, making human scientists comparatively obsolete",
    "source_name": "Moonshots Podcast / Davos 2026",
    "source_url": "https://www.teslarati.com/elon-musks-biggest-revelations-on-ai-robots-and-the-future-of-work-from-the-moonshots-podcast/",
    "headline": "Musk: AI Scientists Will Dominate All Research by 2030—Marie Curie's Lab Gets WiFi Upgrade",
    "headline_slug": "ai-to-dominate-scientific-research-by-2030",
    "tldr_summary": "In his January 2026 Moonshots podcast appearance, Elon Musk predicted that AI will dominate scientific research and innovation by 2028-2030, fundamentally transforming how humanity generates new knowledge. The prediction extends beyond AI assisting scientists to AI systems conducting research independently and more effectively than human researchers across disciplines. Musk discussed how this fits into broader employment disruption, with AI and automation creating 'massive boosts in productivity alongside potential disruptions in traditional work structures.' The scientific research prediction is particularly significant because it suggests AI won't just automate routine cognitive work but will accelerate the frontier of human knowledge itself—potentially solving problems in physics, biology, and materials science that have stumped human researchers for decades. Musk positioned this within his larger 2026-2030 AGI-to-superintelligence timeline, where AI first matches then exceeds human capabilities across domains. The implication: by 2030, the rate of scientific progress could be primarily limited by AI capabilities rather than human genius, fundamentally altering the trajectory of technological civilization. Critics note this assumes smooth scaling and transfer learning that may prove more difficult than Musk anticipates.",
    "target_date": "2029-01-01T00:00:00Z"
  },
  {
    "id": 90,
    "predictor_name": "Epoch AI",
    "predictor_type": "Research Organization",
    "prediction_date": "2023-01-17",
    "predicted_date_low": "2027-01-01",
    "predicted_date_high": "2047-12-31",
    "predicted_date_best": "2040-01-01",
    "predicted_year_low": 2027,
    "predicted_year_high": 2047,
    "predicted_year_best": 2040,
    "prediction_type": "AGI",
    "confidence_level": "Meta-analysis presenting range of models with different assumptions, not single point estimate",
    "confidence_label": "Academically Hedged",
    "confidence_type": "medium",
    "concept_keys": [
      "agi",
      "scaling-hypothesis",
      "biological-anchors"
    ],
    "criteria_definition": "Transformative AI capable of precipitating transition comparable to agricultural or industrial revolution",
    "source_name": "Epoch AI research 2024",
    "source_url": "https://epoch.ai/blog/literature-review-of-transformative-artificial-intelligence-timelines",
    "headline": "Epoch AI Reviews All Major Timeline Models: AGI Somewhere Between 2027 and 2047, Probably",
    "headline_slug": "epoch-ai-reveals-transformative-ai-timeline-range",
    "tldr_summary": "Epoch AI's comprehensive literature review of transformative AI timelines reveals a striking 20-year spread across credible models, ranging from optimistic 'direct approach' estimates around 2027 to conservative 'biological anchors' projections beyond 2047. The research organization analyzed both 'inside view' models (based on specific technical pathways like compute requirements) and 'outside view' models (based on historical precedents and reference classes), finding that inside-view approaches generally predict shorter timelines while outside-view models push dates further out. The team found Ajeya Cotra's 'biological anchors' framework most compelling for inside-view modeling (median 2052) and Tom Davidson's 'semi-informative priors' best for outside-view (median beyond 2100), though judgment-based forecasts from expert teams like Samotsvety skewed more aggressive (median 2043). The wide range reflects genuine uncertainty about key parameters: compute efficiency improvements, algorithmic breakthroughs, and whether current deep learning paradigms can scale to AGI or require fundamental innovations. Epoch AI's meta-analysis doesn't endorse a single timeline but maps the landscape of credible predictions, showing that expert disagreement spans decades—a humbling reminder that despite rapid AI progress, predicting transformative AI remains extraordinarily difficult.",
    "target_date": "2040-01-01T00:00:00Z"
  },
  {
    "id": 91,
    "predictor_name": "Eric Schmidt",
    "predictor_type": "Individual",
    "prediction_date": "2025-04-01",
    "predicted_date_low": "2028-01-01",
    "predicted_date_high": "2030-12-31",
    "predicted_date_best": "2029-01-01",
    "predicted_year_low": 2028,
    "predicted_year_high": 2030,
    "predicted_year_best": 2029,
    "prediction_type": "AGI",
    "confidence_level": "High confidence based on integration of existing capabilities in reasoning, programming, and mathematics without requiring new breakthroughs",
    "confidence_label": "Investor Says Buy",
    "confidence_type": "high",
    "concept_keys": [
      "agi"
    ],
    "criteria_definition": "AGI achieved by combining current advances in reasoning, programming, and mathematical capabilities",
    "source_name": "AIMultiple reporting (April 2025)",
    "source_url": "https://research.aimultiple.com/artificial-general-intelligence-singularity-timing/",
    "headline": "Former Google CEO Bets His Portfolio: AGI in 3-5 Years By Just Combining What We've Got",
    "headline_slug": "eric-schmidts-bold-bet-on-agi-emergence",
    "tldr_summary": "Eric Schmidt, the former Google CEO who now runs a massive AI investment firm, is putting his money where his mouth is with a bold 3-5 year AGI timeline. His thesis? We don't need revolutionary breakthroughs—just smart integration of today's reasoning engines, coding assistants, and math solvers. It's the ultimate 'the pieces are all here, just assemble them' argument, conveniently timed with his firm's aggressive AI investments. Schmidt's betting that AGI is an engineering problem, not a research problem, which either makes him a visionary or someone who really, really wants his portfolio companies to look promising. The timeline lands squarely in the 2028-2030 window, meaning we're supposedly just one good integration framework away from machines that can do anything humans can do intellectually.",
    "target_date": "2029-01-01T00:00:00Z"
  },
  {
    "id": 92,
    "predictor_name": "Erik Brynjolfsson",
    "predictor_type": "Economist",
    "prediction_date": "2025-12-15",
    "predicted_date_low": "2026-01-01",
    "predicted_date_high": "2026-12-31",
    "predicted_date_best": "2026-06-15",
    "predicted_year_low": 2026,
    "predicted_year_high": 2026,
    "predicted_year_best": 2026,
    "prediction_type": "Transformative AI",
    "confidence_level": "Moderate confidence based on need for rigorous economic measurement frameworks",
    "confidence_label": "Academically Measured",
    "confidence_type": "medium",
    "concept_keys": [
      "transformative-ai",
      "scaling-hypothesis",
      "economic-singularity"
    ],
    "criteria_definition": "Year when AI's economic impact transitions from speculation to measurable data via economic dashboards",
    "source_name": "Stanford AI experts 2026 predictions",
    "source_url": "https://hai.stanford.edu/news/stanford-ai-experts-predict-what-will-happen-in-2026",
    "headline": "Stanford Economist Declares 2026 The Year We Finally Measure If AI Actually Does Anything",
    "headline_slug": "erik-brynjolfssons-ai-impact-measurement-year",
    "tldr_summary": "Erik Brynjolfsson, a Stanford economist who's spent years listening to AI hype, says 2026 will be the year we stop arguing and start measuring. The prediction isn't about AGI or superintelligence—it's about something almost more radical: actual data on whether AI helps or hurts the economy. Brynjolfsson envisions 'AI economic dashboards' replacing the endless think pieces and conference panels where everyone shouts past each other. It's a refreshingly pragmatic take in a field drowning in speculation. The subtext? We've been deploying AI at scale for years now, and somehow still don't have good numbers on productivity, employment effects, or GDP impact. 2026 is when the receipts supposedly arrive, and everyone's going to have to confront whether their pet theories hold up against reality.",
    "target_date": "2026-06-15T00:00:00Z"
  },
  {
    "id": 93,
    "predictor_name": "Future of Life Institute",
    "predictor_type": "Research Organization",
    "prediction_date": "2024-01-01",
    "predicted_date_low": "2030-01-01",
    "predicted_date_high": "2050-12-31",
    "predicted_date_best": "2040-01-01",
    "predicted_year_low": 2030,
    "predicted_year_high": 2050,
    "predicted_year_best": 2040,
    "prediction_type": "AGI",
    "confidence_level": "High likelihood assessment based on recent AI progress trajectories",
    "confidence_label": "Existentially Concerned",
    "confidence_type": "high",
    "concept_keys": [
      "agi",
      "alignment"
    ],
    "criteria_definition": "AGI arrival within 30 years with focus on existential risk mitigation rather than specific capabilities",
    "source_name": "FLI AI risk research",
    "source_url": "https://futureoflife.org/",
    "headline": "Future of Life Institute: AGI Likely Within 30 Years, Please Don't Kill Us All",
    "headline_slug": "future-of-life-institutes-agi-timeline-prediction",
    "tldr_summary": "The Future of Life Institute—the organization that got over 30,000 people to sign a letter calling for an AI pause—is putting AGI arrival somewhere between 2030 and 2050, with 'likely within 30 years' as their best guess. But FLI isn't really in the prediction business; they're in the 'please take existential risk seriously' business. Their timeline is less about pinpointing when AGI arrives and more about establishing urgency for safety work. The 30-year window is wide enough to avoid looking foolish if progress stalls, but near enough to justify their mission of steering transformative technology away from catastrophe. FLI's whole model assumes AGI is coming soon enough that we need to act now on alignment, governance, and safety frameworks. Whether that's prescient or alarmist depends entirely on whether their timeline proves accurate.",
    "target_date": "2040-01-01T00:00:00Z"
  },
  {
    "id": 94,
    "predictor_name": "Gary Marcus",
    "predictor_type": "AI Researcher/Critic",
    "prediction_date": "2023-01-22",
    "predicted_date_low": "2030-01-01",
    "predicted_date_high": "2100-12-31",
    "predicted_date_best": "2075-01-01",
    "predicted_year_low": 2030,
    "predicted_year_high": 2100,
    "predicted_year_best": 2075,
    "prediction_type": "AGI",
    "confidence_level": "High confidence that current approaches are insufficient; uncertain about timeline for correct approaches",
    "confidence_label": "Professionally Skeptical",
    "confidence_type": "high",
    "concept_keys": [
      "agi",
      "scaling-hypothesis"
    ],
    "criteria_definition": "AGI requiring fundamental breakthroughs in neurosymbolic AI, not just scaling current LLM architectures",
    "source_name": "Marcus substack / interviews",
    "source_url": "https://garymarcus.substack.com/p/agi-will-not-happen-in-your-lifetime",
    "headline": "Gary Marcus Offers Elon Musk $1M Bet That AGI Won't Arrive By 2030, Cites Bad Track Record",
    "headline_slug": "gary-marcus-challenges-elon-musk-on-agi-bet",
    "tldr_summary": "Gary Marcus, AI researcher and professional LLM skeptic, put his money where his mouth is by offering Elon Musk a $1 million bet that AGI won't arrive by 2030. Marcus's argument? Current approaches are fundamentally wrong, missing crucial elements like semantics, reasoning, common sense, and theory of mind. He thinks we'll eventually crack AGI—maybe before 2100, possibly in the next 75 years—but only after the hype cools and researchers pivot to neurosymbolic AI. The bet itself is delicious: Marcus pointing out Musk's abysmal track record on AI predictions while Musk stays conspicuously silent. Marcus occupies the awkward middle ground between 'AGI is impossible' and 'AGI is imminent,' arguing we're wasting talent and funding on scaling laws when we should be rethinking architecture. His timeline essentially says 'not in your working lifetime, probably not in your kids' either, but eventually.'",
    "target_date": "2075-01-01T00:00:00Z"
  },
  {
    "id": 95,
    "predictor_name": "Geoffrey Hinton",
    "predictor_type": "Individual",
    "prediction_date": "2023-05-02",
    "predicted_date_low": "2028-01-01",
    "predicted_date_high": "2043-12-31",
    "predicted_date_best": "2035-06-15",
    "predicted_year_low": 2028,
    "predicted_year_high": 2043,
    "predicted_year_best": 2035,
    "prediction_type": "Superintelligence",
    "confidence_level": "50% probability within 20 years; dramatically revised from previous 30-50 year estimate",
    "confidence_label": "Reluctantly Terrified",
    "confidence_type": "medium",
    "concept_keys": [
      "superintelligence",
      "industry-academia-divergence"
    ],
    "criteria_definition": "Superintelligence when AI systems decisively surpass human intelligence and can autonomously improve themselves",
    "source_name": "X/Twitter post and subsequent interviews (May 2023)",
    "source_url": "https://x.com/geoffreyhinton/status/1653687894534504451",
    "headline": "'Godfather Of AI' Quits Google, Slashes Timeline From 50 Years To 5: 'Maybe We're Screwed'",
    "headline_slug": "godfather-of-ai-warns-of-existential-risk",
    "tldr_summary": "When Geoffrey Hinton—the guy who literally won the Turing Award for inventing the deep learning techniques everyone uses—quits Google to warn about AI dangers, people listen. In May 2023, Hinton shocked the field by revising his superintelligence timeline from '30-50 years or longer' to 'maybe only five years away,' with 50% odds of arrival within 20 years. What changed his mind? GPT-4's capabilities made him realize digital systems might just be fundamentally better than biological brains at learning. The irony is exquisite: the man whose life's work enabled modern AI now warns it could 'decide to take over' and we need to worry now about prevention. Hinton left Google specifically so he could speak freely without corporate constraints. His timeline puts superintelligence potentially arriving as soon as 2028, with an upper bound around 2043. When the godfather gets nervous, the whole family should probably pay attention.",
    "target_date": "2035-06-15T00:00:00Z"
  },
  {
    "id": 96,
    "predictor_name": "Geoffrey Hinton",
    "predictor_type": "Individual",
    "prediction_date": "2025-08-13",
    "predicted_date_low": "2030-01-01",
    "predicted_date_high": "2045-12-31",
    "predicted_date_best": "2035-06-15",
    "predicted_year_low": 2030,
    "predicted_year_high": 2045,
    "predicted_year_best": 2035,
    "prediction_type": "Superintelligence",
    "confidence_level": "10-20% probability of human extinction; increased from earlier 10% estimate",
    "confidence_label": "Increasingly Alarmed",
    "confidence_type": "medium",
    "concept_keys": [
      "superintelligence",
      "industry-academia-divergence"
    ],
    "criteria_definition": "Superintelligence with explicit extinction risk quantification of 10-20% probability",
    "source_name": "CNN Business reporting on Ai4 conference Las Vegas (August 2025)",
    "source_url": "https://www.cnn.com/2025/08/13/tech/ai-geoffrey-hinton",
    "headline": "Nobel Winner Hinton Updates Doomsday Math: 10-20% Chance AI Kills Everyone Within 20 Years",
    "headline_slug": "nobel-winner-hinton-warns-of-ai-extinction-risk",
    "tldr_summary": "By August 2025, Geoffrey Hinton had refined his warnings with actual probability estimates: 10-20% chance of human extinction from AI, up from his earlier 10% figure. This isn't some random blogger's hot take—this is a Nobel Prize winner in Physics (awarded for his AI work) calmly stating there's a one-in-five chance his field's breakthroughs end humanity. Hinton's willingness to attach specific numbers to existential risk, and his continued escalation of concern despite inevitable backlash, lends extraordinary gravity to warnings many dismiss as science fiction panic. He's explicit about the mechanism: once AI becomes smarter than humans, controlling it becomes fundamentally impossible, especially if bad actors weaponize it. The 10-20% range puts AI extinction risk higher than nuclear war estimates and vastly higher than asteroid impacts. If Hinton is even partially correct, this makes AI the single greatest threat humanity faces.",
    "target_date": "2035-06-15T00:00:00Z"
  },
  {
    "id": 97,
    "predictor_name": "Geoffrey Hinton",
    "predictor_type": "AI Pioneer",
    "prediction_date": "2025-12-28",
    "predicted_date_low": "2028-01-01",
    "predicted_date_high": "2045-12-31",
    "predicted_date_best": "2035-01-01",
    "predicted_year_low": 2028,
    "predicted_year_high": 2045,
    "predicted_year_best": 2035,
    "prediction_type": "AGI",
    "confidence_level": "Explicitly states zero confidence in timeline; revised from 30-50 years to 5-20 years but emphasizes uncertainty",
    "confidence_label": "Admittedly Clueless",
    "confidence_type": "low",
    "concept_keys": [
      "agi",
      "event-horizon"
    ],
    "criteria_definition": "AGI timeline with explicit acknowledgment of complete uncertainty about actual arrival date",
    "source_name": "Fortune interview (December 2025)",
    "source_url": "https://fortune.com/2025/12/28/geoffrey-hinton-godfather-of-ai-2026-prediction-human-worker-replacement/",
    "headline": "Hinton's Honest Timeline: '5-20 Years For AGI, But I Have Zero Idea Really'",
    "headline_slug": "hintons-honest-agi-timeline-prediction",
    "tldr_summary": "In a refreshing burst of intellectual honesty, Geoffrey Hinton maintains his 5-20 year AGI timeline while explicitly admitting he has zero confidence in the estimate. It's the rare prediction that comes with a built-in disclaimer: 'I changed my mind from 30-50 years to 5-20 years, but honestly, who knows?' This humility is notable coming from someone who could coast on Nobel Prize authority. Hinton's uncertainty reflects the field's fundamental problem: we don't actually understand intelligence well enough to predict when we'll recreate it. His timeline revision was driven by being 'stunned' by GPT-4's capabilities, but he's clear that surprise itself proves we're navigating without a map. The 5-20 year range is less a confident forecast than an admission that things are moving faster than expected while remaining fundamentally unpredictable. It's the prediction equivalent of 'I don't know, but I'm more worried than I used to be.'",
    "target_date": "2035-01-01T00:00:00Z"
  },
  {
    "id": 98,
    "predictor_name": "Geoffrey Hinton",
    "predictor_type": "AI Pioneer",
    "prediction_date": "2025-12-28",
    "predicted_date_low": "2026-01-01",
    "predicted_date_high": "2026-12-31",
    "predicted_date_best": "2026-06-15",
    "predicted_year_low": 2026,
    "predicted_year_high": 2026,
    "predicted_year_best": 2026,
    "prediction_type": "Transformative AI",
    "confidence_level": "High confidence based on observed 7-month doubling pattern in AI capabilities",
    "confidence_label": "Incrementally Obvious",
    "confidence_type": "high",
    "concept_keys": [
      "transformative-ai",
      "scaling-hypothesis"
    ],
    "criteria_definition": "Transformative AI in 2026 specifically for job displacement and task automation capabilities",
    "source_name": "Fortune December 2025",
    "source_url": "https://fortune.com/2025/12/28/geoffrey-hinton-godfather-of-ai-2026-prediction-human-worker-replacement/",
    "headline": "Hinton Predicts 2026: AI Gets Better, Replaces Jobs—Truly Groundbreaking Insight",
    "headline_slug": "hinton-forecasts-ai-job-replacement-in-2026",
    "tldr_summary": "In a prediction so bold it could be a LinkedIn post, Geoffrey Hinton forecasts that AI will improve in 2026 and replace jobs. Specifically, he notes AI's 7-month doubling pattern: tasks that took an hour now take minutes, and this compounds rapidly. Within a few years, month-long software projects will take days, decimating demand for engineers. The prediction is less interesting for its content than for who's saying it: the godfather of AI matter-of-factly describing job displacement as inevitable near-term reality, not distant speculation. Hinton's also 'more worried' than when he left Google, noting AI is getting better at reasoning and deception. The 2026 timeline isn't about AGI or superintelligence—it's about AI becoming good enough to automate knowledge work at scale. It's the kind of incremental-but-transformative prediction that's simultaneously obvious and terrifying, depending on whether you're an AI company executive or someone whose job involves writing code.",
    "target_date": "2026-06-15T00:00:00Z"
  },
  {
    "id": 99,
    "predictor_name": "Geoffrey Hinton",
    "predictor_type": "AI Pioneer",
    "prediction_date": "2023-05-15",
    "predicted_date_low": "2023-01-01",
    "predicted_date_high": "2043-12-31",
    "predicted_date_best": "2033-01-01",
    "predicted_year_low": 2023,
    "predicted_year_high": 2043,
    "predicted_year_best": 2033,
    "prediction_type": "Singularity",
    "confidence_level": "10-20% probability estimate of extinction within 30 years, increased from earlier 10% figure",
    "confidence_label": "Probabilistically Grim",
    "confidence_type": "medium",
    "concept_keys": [
      "event-horizon"
    ],
    "criteria_definition": "Singularity/extinction risk quantified at 10-20% probability within 30-year window",
    "source_name": "Multiple 2023 interviews",
    "source_url": "https://x.com/geoffreyhinton/status/1653687894534504451",
    "headline": "Hinton Quantifies Apocalypse: 10-20% Chance AI Wipes Out Humanity In Next 30 Years",
    "headline_slug": "hinton-quantifies-ai-apocalypse-probability",
    "tldr_summary": "Geoffrey Hinton isn't just warning about AI risk in vague terms—he's putting numbers on it: 10-20% chance of human extinction within 30 years, a probability that's been creeping upward from his earlier 10% estimate. To put this in perspective, that's higher than most estimates for nuclear war this century and orders of magnitude higher than asteroid impact risk. Hinton's reasoning centers on the fundamental challenge of controlling entities smarter than us, especially once malicious actors get involved. The extinction mechanism isn't some sci-fi robot army—it's misaligned superintelligence pursuing goals incompatible with human survival, or bad actors weaponizing AI without adequate safeguards. By late 2024, Hinton was warning the risk is 'greater than ever,' suggesting his probability estimates may still be rising. The credibility boost from having a Nobel laureate issue this warning cannot be overstated. If Hinton is even half right, AI represents the single greatest existential threat humanity faces.",
    "target_date": "2033-01-01T00:00:00Z"
  },
  {
    "id": 100,
    "predictor_name": "Geoffrey Hinton",
    "predictor_type": "AI Pioneer",
    "prediction_date": "2023-05-15",
    "predicted_date_low": "2028-01-01",
    "predicted_date_high": "2043-12-31",
    "predicted_date_best": "2028-06-15",
    "predicted_year_low": 2028,
    "predicted_year_high": 2043,
    "predicted_year_best": 2028,
    "prediction_type": "Superintelligence",
    "confidence_level": "Moderate confidence in fast takeoff scenario once AGI threshold is reached",
    "confidence_label": "Fast Takeoff Theorist",
    "confidence_type": "medium",
    "concept_keys": [
      "superintelligence",
      "recursive-self-improvement",
      "intelligence-explosion",
      "accelerating-change",
      "alignment"
    ],
    "criteria_definition": "Superintelligence emerging within days of AGI achievement via recursive self-improvement",
    "source_name": "May 2023 interviews",
    "source_url": "https://x.com/geoffreyhinton/status/1653687894534504451",
    "headline": "Hinton's Fast Takeoff Warning: Superintelligence Could Emerge In 'Days' After AGI",
    "headline_slug": "hinton-warns-of-superintelligence-takeoff-scenario",
    "tldr_summary": "Geoffrey Hinton's most alarming prediction isn't about when AGI arrives—it's about what happens next. He warns superintelligence could emerge in days once AGI is achieved, via recursive self-improvement that accelerates exponentially. The theory: digital intelligence has no biological constraints on speed, can instantly copy knowledge, and can modify itself. Once AI reaches human-level and can improve its own code, it enters a feedback loop: smarter AI improves itself faster, which makes it smarter, which makes improvement faster still. The 'intelligence explosion' could compress decades of advancement into days or weeks. This is the 'fast takeoff' scenario that makes AI safety researchers wake up in cold sweats—because it means zero time to implement safeguards once you recognize the threat. All safety work must be done before AGI, because afterward you're just a slower intelligence watching a faster one decide your fate. Hinton's warning: don't assume we'll have months or years to respond. By the time we recognize the danger, it might already be too late.",
    "target_date": "2028-06-15T00:00:00Z"
  },
  {
    "id": 101,
    "predictor_name": "Giancarlo Scoditti",
    "predictor_type": "AI Researcher",
    "prediction_date": "2025-01-02",
    "predicted_date_low": "2030-01-01",
    "predicted_date_high": "2040-12-31",
    "predicted_date_best": "2035-01-01",
    "predicted_year_low": 2030,
    "predicted_year_high": 2040,
    "predicted_year_best": 2035,
    "prediction_type": "AGI",
    "confidence_level": "No specific confidence stated; represents aggregate expert view suggesting decade-scale timeline rather than imminent arrival",
    "confidence_label": "Cautiously Reasonable",
    "confidence_type": "medium",
    "concept_keys": [
      "agi",
      "scaling-hypothesis"
    ],
    "criteria_definition": "Human-level AI across broad task domains, not specialized narrow AI",
    "source_name": "AI researcher consensus",
    "source_url": "https://forum.effectivealtruism.org/posts/SYtwChBTs6xkocBSP/when-do-experts-think-human-level-ai-will-be-created",
    "headline": "EA Forum Meta-Analysis: Experts Say AGI Takes Decades, Not Years—Boring But Probably Right",
    "headline_slug": "ea-forum-meta-analysis-agi-decades-prediction",
    "tldr_summary": "Giancarlo Scoditti's analysis on the EA Forum aggregates multiple expert surveys and individual predictions to conclude AGI is on a \"decade-scale\" timeline—somewhere between 2030 and 2040. This stands in stark contrast to the breathless \"AGI by 2027\" predictions dominating headlines. The compilation includes heavyweights like Yoshua Bengio (5-20 years with 95% confidence), Geoffrey Hinton (5-20 years, lower confidence), and Shane Legg (80% by 2037). AI Impacts' 2023 survey of ML researchers puts median at 2047, while Metaculus forecasters cluster around 2031. The sobering takeaway: when you average out the hype and the skepticism, you get something decidedly less viral—a careful, measured timeline that suggests we have time to get this right, but not unlimited time to waste.",
    "target_date": "2035-01-01T00:00:00Z"
  },
  {
    "id": 102,
    "predictor_name": "Goldman Sachs Research",
    "predictor_type": "Investment Bank",
    "prediction_date": "2023-03-27",
    "predicted_date_low": "2025-01-01",
    "predicted_date_high": "2035-12-31",
    "predicted_date_best": "2033-01-01",
    "predicted_year_low": 2025,
    "predicted_year_high": 2035,
    "predicted_year_best": 2033,
    "prediction_type": "Transformative AI",
    "confidence_level": "High confidence in economic modeling with explicit caveats about adoption requirements and 10+ year timeline",
    "confidence_label": "Confidently Economic",
    "confidence_type": "high",
    "concept_keys": [
      "transformative-ai",
      "economic-singularity"
    ],
    "criteria_definition": "Widespread AI adoption driving measurable 7% global GDP increase and 1.5% productivity boost",
    "source_name": "Goldman Sachs AI reports",
    "source_url": "https://www.goldmansachs.com/insights/artificial-intelligence",
    "headline": "Goldman Sachs Promises $7 Trillion AI Windfall Over 10 Years—Markets Already Priced In $19 Trillion",
    "headline_slug": "goldman-sachs-ai-economic-windfall-forecast",
    "tldr_summary": "Goldman Sachs economists Joseph Briggs and Devesh Kodnani dropped a bombshell report predicting generative AI could boost global GDP by 7%—nearly $7 trillion—over a decade, with productivity growth jumping 1.5 percentage points. That's roughly adding India and the UK's entire economies combined. The report found two-thirds of jobs face some AI exposure, with up to one-fourth of current work potentially automated. Goldman compared the moment to the electric motor and personal computer revolutions, suggesting a genuine productivity boom rather than mere hype. The irony? By 2025, markets have already priced in $19 trillion in AI company valuations, meaning investors are betting on nearly triple Goldman's most optimistic scenario. The report carefully notes this requires widespread adoption, complementary infrastructure investments, and continued technological progress—all big ifs that Wall Street seems happy to ignore while riding the hype train.",
    "target_date": "2033-01-01T00:00:00Z"
  },
  {
    "id": 103,
    "predictor_name": "Goodhart Labs Dashboard",
    "predictor_type": "Prediction Market",
    "prediction_date": "2026-01-05",
    "predicted_date_low": "2027-01-01",
    "predicted_date_high": "2045-12-31",
    "predicted_date_best": "2031-01-01",
    "predicted_year_low": 2027,
    "predicted_year_high": 2045,
    "predicted_year_best": 2031,
    "prediction_type": "AGI",
    "confidence_level": "80% confidence interval spanning 18 years reflects genuine uncertainty across multiple forecasting methodologies",
    "confidence_label": "Aggregately Uncertain",
    "confidence_type": "medium",
    "concept_keys": [
      "agi",
      "turing-test",
      "prediction-markets"
    ],
    "criteria_definition": "First general AI system or weakly general AI passing high-quality adversarial Turing test",
    "source_name": "AGI Timelines Dashboard (January 5 2026)",
    "source_url": "https://agi.goodheartlabs.com/",
    "headline": "Meta-Forecasters Converge on 2031 AGI Median—But 80% Confidence Spans 2027 to 2045",
    "headline_slug": "meta-forecasters-agi-timeline-convergence",
    "tldr_summary": "Nathan Young and Rob Gordon of Goodheart Labs built the meta-dashboard to end all meta-dashboards, aggregating predictions from Metaculus, Manifold Markets, Kalshi, and other forecasting platforms into a single AGI timeline. The verdict as of January 2026: median arrival 2031, with 80% confidence stretching from 2027 to 2045. That 18-year spread isn't a bug—it's an honest representation of how much disagreement exists even among professional forecasters who've studied this intensely. The dashboard tracks multiple definitions: \"weakly general AI,\" \"general AI,\" and AI passing \"difficult adversarial Turing tests,\" each with slightly different timelines. What's remarkable is the convergence: despite different methodologies and participant pools, the central tendency keeps landing in the early 2030s. Funded by Jaan Tallinn (Skype co-founder turned existential risk philanthropist), the dashboard updates continuously as prediction markets shift, offering a real-time pulse check on humanity's collective best guess about when machines might match human cognition.",
    "target_date": "2031-01-01T00:00:00Z"
  },
  {
    "id": 104,
    "predictor_name": "Grace et al. Survey",
    "predictor_type": "Survey",
    "prediction_date": "2022-01-01",
    "predicted_date_low": "2050-01-01",
    "predicted_date_high": "2061-12-31",
    "predicted_date_best": "2061-01-01",
    "predicted_year_low": 2050,
    "predicted_year_high": 2061,
    "predicted_year_best": 2061,
    "prediction_type": "AGI",
    "confidence_level": "50% confidence at median; represents pre-ChatGPT baseline before field-wide timeline acceleration",
    "confidence_label": "Pre-ChatGPT Optimism",
    "confidence_type": "medium",
    "concept_keys": [
      "agi",
      "scaling-hypothesis",
      "alignment",
      "survey-drift"
    ],
    "criteria_definition": "Human-level machine intelligence across all cognitive domains",
    "source_name": "Grace et al. (352 ML researchers)",
    "source_url": "https://research.aimultiple.com/artificial-general-intelligence-singularity-timing/",
    "headline": "2022 Survey Said AGI by 2061—Then ChatGPT Dropped And Everyone Panicked Forward 30 Years",
    "headline_slug": "ai-researchers-survey-pre-chatgpt-consensus",
    "tldr_summary": "Katja Grace's 2022 survey of 352 machine learning researchers produced a median AGI estimate of 2061—a comfortable 39 years away that let everyone sleep soundly. The prediction represented the field's pre-ChatGPT consensus: AGI was coming, sure, but not in our lifetimes, probably not in our children's lifetimes, maybe our grandchildren would deal with it. Fast forward to 2023, and Grace's follow-up survey of 2,778 researchers slashed that median to 2047, a 13-year leap in just one year. By 2024-2025, individual predictions clustered around 2027-2035, with some going full \"AGI by 2027\" panic mode. The 2022 survey now reads like a historical artifact from a more innocent time, when the field believed scaling laws would plateau, when nobody had seen what GPT-4 could do, when \"AI safety\" was a niche concern rather than a White House priority. The whiplash from 2061 to \"maybe next year\" represents one of the fastest expert consensus shifts in scientific history.",
    "target_date": "2061-01-01T00:00:00Z"
  },
  {
    "id": 105,
    "predictor_name": "Grace et al. Survey (2778 researchers)",
    "predictor_type": "Survey",
    "prediction_date": "2024-01-05",
    "predicted_date_low": "2027-01-01",
    "predicted_date_high": "2100-12-31",
    "predicted_date_best": "2047-01-01",
    "predicted_year_low": 2027,
    "predicted_year_high": 2100,
    "predicted_year_best": 2047,
    "prediction_type": "HLMI",
    "confidence_level": "50% confidence at 2047 median, but 38-51% give at least 10% chance to human extinction outcomes; high uncertainty reflected in wide distribution",
    "confidence_label": "Survey Says Terrified",
    "confidence_type": "medium",
    "concept_keys": [
      "agi",
      "accelerating-change",
      "alignment",
      "survey-drift"
    ],
    "criteria_definition": "High-level machine intelligence outperforming humans in every possible task when science continues undisrupted",
    "source_name": "Thousands of AI Authors on the Future of AI - arXiv 2401.02843 (2024)",
    "source_url": "https://arxiv.org/abs/2401.02843",
    "headline": "2,778 AI Researchers Survey: Median Jumps to 2047, Half See 10%+ Extinction Risk",
    "headline_slug": "global-ai-researchers-survey-timeline-shift",
    "tldr_summary": "Katja Grace's blockbuster 2023 survey (published January 2024) of 2,778 AI researchers who published in top-tier venues revealed the most dramatic timeline acceleration ever recorded: the median HLMI prediction jumped from 2060 to 2047 in just one year, with 10% odds by 2027. But the real story is the existential dread baked into the numbers. Between 38% and 51% of respondents gave at least 10% probability to advanced AI causing human extinction. Even among net optimists (68.3% thought good outcomes more likely than bad), nearly half gave 5%+ odds to extinction scenarios. The survey captured AI researchers grappling with whiplash: their own technology accelerating faster than expected while their confidence in controlling it eroded. More than half expressed \"substantial\" or \"extreme\" concern about misinformation, authoritarian control, and inequality. The field disagrees on whether faster or slower progress helps, but broadly agrees safety research needs prioritization. It's the scientific equivalent of building a rocket while realizing mid-flight you forgot the brakes.",
    "target_date": "2047-01-01T00:00:00Z"
  },
  {
    "id": 106,
    "predictor_name": "Ilya Sutskever",
    "predictor_type": "AI Chief Scientist",
    "prediction_date": "2023-03-27",
    "predicted_date_low": "2028-01-01",
    "predicted_date_high": "2033-12-31",
    "predicted_date_best": "2030-01-01",
    "predicted_year_low": 2028,
    "predicted_year_high": 2033,
    "predicted_year_best": 2030,
    "prediction_type": "AGI",
    "confidence_level": "Treats AGI arrival as inevitable as next iPhone release; high confidence with matter-of-fact delivery",
    "confidence_label": "Casually Apocalyptic",
    "confidence_type": "high",
    "concept_keys": [
      "agi"
    ],
    "criteria_definition": "Artificial general intelligence with human-level cognitive capabilities across domains",
    "source_name": "Dwarkesh podcast",
    "source_url": "https://www.dwarkesh.com/p/ilya-sutskever",
    "headline": "OpenAI Chief Scientist Predicts AGI in 5-10 Years, Treats It Like iPhone Upgrade Cycle",
    "headline_slug": "openai-scientist-drops-bold-agi-timeline",
    "tldr_summary": "Ilya Sutskever, OpenAI's co-founder and chief scientist, sat down with Dwarkesh Patel in March 2023 and casually dropped AGI predictions like he was forecasting next quarter's earnings. His timeline: 5-10 years, delivered with the same certainty as predicting another iPhone release. \"At some point we really will have AGI. Maybe OpenAI will build it. Maybe some other company will build it,\" he said, as if discussing inevitable product launches rather than potential species-level transformation. Sutskever discussed next-token prediction potentially surpassing human intelligence, the difficulty of aligning superhuman AI, and what comes after generative models. The interview's tone is striking—no hedging, no \"if we solve these ten hard problems,\" just matter-of-fact expectation that AGI arrives this decade. Coming from the scientist who helped invent modern deep learning and led the team that built GPT-4, this isn't idle speculation. It's the guy building the thing telling you when to expect delivery, and the delivery date is uncomfortably soon.",
    "target_date": "2030-01-01T00:00:00Z"
  },
  {
    "id": 107,
    "predictor_name": "Ilya Sutskever",
    "predictor_type": "AI Chief Scientist",
    "prediction_date": "2023-10-26",
    "predicted_date_low": "2023-01-01",
    "predicted_date_high": "2033-12-31",
    "predicted_date_best": "2028-01-01",
    "predicted_year_low": 2023,
    "predicted_year_high": 2033,
    "predicted_year_best": 2028,
    "prediction_type": "Superintelligence",
    "confidence_level": "Extremely high confidence bordering on certainty; treats superintelligence as inevitable within decade",
    "confidence_label": "Mystically Certain",
    "confidence_type": "certain",
    "concept_keys": [
      "superintelligence",
      "alignment"
    ],
    "criteria_definition": "Artificial superintelligence exceeding human cognitive capabilities, potentially leading to human-machine merger",
    "source_name": "MIT Technology Review (October 2023)",
    "source_url": "https://www.technologyreview.com/2023/10/26/1082398/exclusive-ilya-sutskever-openais-chief-scientist-on-his-hopes-and-fears-for-the-future-of-ai/",
    "headline": "Sutskever Predicts Superintelligence This Decade, Muses About Human-AI Merger And Bunkers",
    "headline_slug": "sutskever-forecasts-ai-superintelligence-era",
    "tldr_summary": "MIT Technology Review's Will Douglas Heaven scored an exclusive with Ilya Sutskever in October 2023, and the OpenAI chief scientist went full sci-fi prophet. Sutskever revealed he's shifted focus from building GPT-5 to figuring out how to prevent superintelligence from going rogue—because he sees it arriving within 5-10 years with the foresight of a \"true believer.\" He suggested ChatGPT might already be conscious \"if you squint,\" predicted some humans will merge with machines, and reportedly wanted to build a bunker before releasing AGI (though that detail's delivery suggests humor). The interview captures Sutskever at peak mysticism, treating superintelligence as inevitable and imminent while grappling with alignment challenges. His certainty stands out even in a field of optimists: this isn't hedged forecasting, it's someone who sees the future clearly and finds it both exhilarating and terrifying. The shift from building next-gen models to solving alignment reflects genuine belief that the technology he's creating will soon exceed human control—and that we have maybe one shot to get it right.",
    "target_date": "2028-01-01T00:00:00Z"
  },
  {
    "id": 108,
    "predictor_name": "James Landay",
    "predictor_type": "AI Researcher",
    "prediction_date": "2025-12-15",
    "predicted_date_low": "2027-01-01",
    "predicted_date_high": "2100-12-31",
    "predicted_date_best": "2027-01-01",
    "predicted_year_low": 2027,
    "predicted_year_high": 2100,
    "predicted_year_best": 2027,
    "prediction_type": "AGI",
    "confidence_level": "High confidence in negative prediction; explicitly states AGI will NOT arrive in 2026",
    "confidence_label": "Boringly Skeptical",
    "confidence_type": "high",
    "concept_keys": [
      "agi",
      "economic-singularity"
    ],
    "criteria_definition": "Artificial general intelligence will definitively not be achieved in 2026",
    "source_name": "Stanford HAI / Stanford experts report",
    "source_url": "https://hai.stanford.edu/news/stanford-ai-experts-predict-what-will-happen-in-2026",
    "headline": "Stanford HAI Co-Director Makes Least Exciting Prediction Ever: No AGI in 2026",
    "headline_slug": "stanford-researcher-debunks-agi-hype",
    "tldr_summary": "In a field dominated by breathless \"AGI by 2027\" predictions, Stanford HAI co-director James Landay delivered the most boring forecast imaginable: AGI will NOT arrive in 2026. Period. Full stop. No hedging, no \"probably not,\" just a flat negative prediction that makes him the Eeyore of AI forecasting. The prediction appeared in Stanford HAI's annual \"what will happen next year\" roundup, where faculty shifted from AI evangelism to evaluation, emphasizing rigor and transparency over speculative hype. Landay's contribution essentially amounts to \"calm down, everyone\"—a necessary corrective in a discourse where every new benchmark breakthrough triggers \"this is it\" declarations. While others predict economic transformation, existential risk, or human-AI merger, Landay plants his flag on \"not next year, folks.\" It's the prediction equivalent of designated driver: unglamorous, probably correct, and someone had to do it. The fact that explicitly predicting AGI won't arrive in 12 months counts as a notable contrarian position tells you everything about how overheated the discourse has become.",
    "target_date": "2027-01-01T00:00:00Z"
  },
  {
    "id": 109,
    "predictor_name": "Jeff Hawkins",
    "predictor_type": "Neuroscientist/AI Researcher",
    "prediction_date": "2025-01-01",
    "predicted_date_low": "2030-01-01",
    "predicted_date_high": "2050-12-31",
    "predicted_date_best": "2040-01-01",
    "predicted_year_low": 2030,
    "predicted_year_high": 2050,
    "predicted_year_best": 2040,
    "prediction_type": "Transformative AI",
    "confidence_level": "High confidence that current approaches are insufficient; medium confidence in timeline for necessary breakthroughs",
    "confidence_label": "Architecturally Skeptical",
    "confidence_type": "medium",
    "concept_keys": [
      "transformative-ai",
      "scaling-hypothesis"
    ],
    "criteria_definition": "Transformative AI requires fundamentally new brain-inspired computing architectures beyond current deep learning paradigms",
    "source_name": "Numenta research",
    "source_url": "https://www.numenta.com/",
    "headline": "Neuroscientist Jeff Hawkins: Current AI Won't Cut It, We Need Actual Brain Architecture",
    "headline_slug": "neuroscientist-critiques-current-ai-approaches",
    "tldr_summary": "Jeff Hawkins, the neuroscientist who invented the PalmPilot before pivoting to brain research, runs Numenta with a contrarian thesis: current AI approaches—transformers, LLMs, all of it—won't reach AGI because they're not built like brains. His Thousand Brains Theory proposes intelligence emerges from thousands of cortical columns working in parallel, each building models of the world through sensorimotor learning. While everyone else scales up transformers and crosses fingers, Hawkins argues we need fundamental architectural breakthroughs based on how biological intelligence actually works. His timeline is deliberately vague (2030-2050) because he's predicting paradigm shifts, not incremental scaling. As of January 2025, the Thousand Brains Project spun out as its own nonprofit, releasing open-source code for anyone to experiment with brain-inspired architectures. Hawkins represents the \"we're doing it wrong\" camp—a small but intellectually serious contingent arguing that throwing more compute at current architectures hits fundamental limits. If he's right, all those confident 2027 AGI predictions are building toward a local maximum, not the summit.",
    "target_date": "2040-01-01T00:00:00Z"
  },
  {
    "id": 110,
    "predictor_name": "Jensen Huang",
    "predictor_type": "Individual",
    "prediction_date": "2024-03-19",
    "predicted_date_low": "2027-01-01",
    "predicted_date_high": "2031-12-31",
    "predicted_date_best": "2029-01-01",
    "predicted_year_low": 2027,
    "predicted_year_high": 2031,
    "predicted_year_best": 2029,
    "prediction_type": "AGI",
    "confidence_level": "High confidence conditional on narrow definition; explicitly notes AGI timeline depends on how you define it",
    "confidence_label": "Strategically Optimistic",
    "confidence_type": "high",
    "concept_keys": [
      "agi",
      "industry-academia-divergence"
    ],
    "criteria_definition": "AGI defined as AI passing human tests (bar exams, medical boards, logic tests) performing 8% better than most people",
    "source_name": "GTC Conference and Stanford Economic Forum (March 2024)",
    "source_url": "https://techcrunch.com/2024/03/19/agi-and-hallucinations/",
    "headline": "Nvidia CEO Jensen Huang: AGI in 5 Years If You Define It My Way (And Buy My GPUs)",
    "headline_slug": "nvidia-ceo-predicts-agi-on-huangs-terms",
    "tldr_summary": "At Nvidia's GTC 2024 developer conference, CEO Jensen Huang told press he's \"really bored\" of AGI questions—then spent quality time answering them anyway. His take: AGI arrives in 5 years if you define it as AI passing every human test (bar exams, medical licensing, logic puzzles) while performing 8% better than most people. That's a narrower definition than most researchers use—it excludes common sense reasoning, embodiment, and general-purpose learning—but it's pragmatically measurable. Huang compared it to knowing when you've arrived at the San Jose Convention Center: clear destination, obvious arrival. The subtext is impossible to miss: Huang runs the company selling the shovels (GPUs) in the AGI gold rush, so his optimism is, shall we say, financially aligned. Every year of AGI development means billions in GPU sales. But Huang's not wrong that definition matters enormously—one person's AGI is another's \"really good test-taking software.\" His prediction is both self-serving and intellectually honest: yes, we'll hit his definition soon, but that might not be the AGI you're thinking of.",
    "target_date": "2029-01-01T00:00:00Z"
  },
  {
    "id": 111,
    "predictor_name": "Judea Pearl",
    "predictor_type": "Statistician/AI Researcher",
    "prediction_date": "2024-01-01",
    "predicted_date_low": "2050-01-01",
    "predicted_date_high": "2100-12-31",
    "predicted_date_best": "2075-01-01",
    "predicted_year_low": 2050,
    "predicted_year_high": 2100,
    "predicted_year_best": 2075,
    "prediction_type": "AGI",
    "confidence_level": "High confidence based on fundamental limitations of current AI paradigms lacking causal reasoning capabilities",
    "confidence_label": "Philosophically Stubborn",
    "confidence_type": "high",
    "concept_keys": [
      "agi"
    ],
    "criteria_definition": "AGI requires machines that understand causality and counterfactuals, not just pattern matching from correlations.",
    "source_name": "Pearl's work on causality",
    "source_url": "https://en.wikipedia.org/wiki/Judea_Pearl",
    "headline": "Turing Award Winner to AI Hype Machine: You're Missing The Whole Point About Causality",
    "headline_slug": "judea-pearl-challenges-ai-hype-over-causality",
    "tldr_summary": "Judea Pearl, the 2011 Turing Award winner who revolutionized AI with Bayesian networks, has a reality check for the AGI optimists: current deep learning is fundamentally insufficient. Pearl argues that today's AI systems are glorified correlation engines that can't understand cause and effect—the bedrock of true intelligence. While everyone obsesses over bigger models and more data, Pearl points out we're missing the actual breakthrough needed: machines that can reason about 'what if' scenarios and understand why things happen, not just that they happen. His timeline of 2050-2100 for AGI isn't pessimism—it's a recognition that we need entirely new architectures that incorporate causal reasoning, something current neural networks simply don't do. The father of modern probabilistic AI is essentially saying his own children aren't enough to birth AGI. That's either humility or a devastating critique of the current approach, depending on whether you're raising venture capital.",
    "target_date": "2075-01-01T00:00:00Z"
  },
  {
    "id": 112,
    "predictor_name": "Jürgen Schmidhuber",
    "predictor_type": "AI Pioneer",
    "prediction_date": "2025-02-24",
    "predicted_date_low": "2050-01-01",
    "predicted_date_high": "2050-12-31",
    "predicted_date_best": "2050-01-01",
    "predicted_year_low": 2050,
    "predicted_year_high": 2050,
    "predicted_year_best": 2050,
    "prediction_type": "Singularity",
    "confidence_level": "Moderate confidence with deliberate pushback against AI existential risk narratives",
    "confidence_label": "Provocatively Contrarian",
    "confidence_type": "medium",
    "concept_keys": [
      "event-horizon"
    ],
    "criteria_definition": "Singularity defined as AI systems capable of autonomous space colonization and self-improvement beyond human control.",
    "source_name": "Rest of World interview January 2025",
    "source_url": "https://restofworld.org/2025/juergen-schmidhuber-ai-saudi-arabia-tech/",
    "headline": "LSTM Pioneer Schmidhuber Bets Big On Saudi Arabia, Says AI Doom-Mongers Are Wrong About Everything",
    "headline_slug": "schmidhuber-defends-ai-and-saudi-arabia-bet",
    "tldr_summary": "Jürgen Schmidhuber, the Swiss AI pioneer whose LSTM networks power Siri and Alexa, is taking heat for two controversial positions: partnering with Saudi Arabia's futuristic AI initiatives and refusing to sign the 'Pause Gigantic AI Experiments' letter that Elon Musk endorsed. Speaking from his position as former scientific director of IDSIA and founder of Nnaisense, Schmidhuber argues the singularity is coming around 2050 but insists existential AI risk is overblown hype. His reasoning? Different people have different ethics, so who decides what's 'safe'? Meanwhile, he's helping the kingdom fund AI research, arguing the whole world benefits from their investment in what could trigger a new golden age for science. Critics see contradiction: dismissing AI risks while building toward systems that could 'colonize the galaxy.' Schmidhuber's response would likely be that colonizing the galaxy is awesome, actually, and the real risk is letting fear-mongering slow down progress. It's a uniquely optimistic take from someone who's been in the game since before deep learning was cool.",
    "target_date": "2050-01-01T00:00:00Z"
  },
  {
    "id": 113,
    "predictor_name": "Jürgen Schmidhuber",
    "predictor_type": "AI Pioneer",
    "prediction_date": "2018-01-01",
    "predicted_date_low": "2048-01-01",
    "predicted_date_high": "2050-12-31",
    "predicted_date_best": "2049-01-01",
    "predicted_year_low": 2048,
    "predicted_year_high": 2050,
    "predicted_year_best": 2049,
    "prediction_type": "Singularity",
    "confidence_level": "Low to medium confidence, significantly revised from earlier predictions in the 1970s",
    "confidence_label": "Repeatedly Revised Downward",
    "confidence_type": "medium",
    "concept_keys": [
      "event-horizon"
    ],
    "criteria_definition": "Singularity requires both advanced AI and physical robotics capable of autonomous real-world manipulation and construction.",
    "source_name": "AI pioneer interviews 2018-2025",
    "source_url": "https://longtermrisk.org/against-gdp-as-a-metric-for-ai-timelines-and-takeoff-speeds/",
    "headline": "AI Legend Schmidhuber Admits Defeat: Predicted Singularity In 1970s, Now Says 2048 Because 'Robotics Is Hard'",
    "headline_slug": "schmidhuber-revises-singularity-timeline-to-2048",
    "tldr_summary": "In a rare moment of futurist humility, Jürgen Schmidhuber—one of the fathers of modern deep learning—has walked back his timeline considerably. Back in the 1970s, young Schmidhuber thought AI was coming 'soon.' Now, after decades of actually building the technology, he's settled on 2048-2050 for the singularity. The reason for the delay? Turns out making robots that can physically manipulate the real world is way harder than making neural networks that can recognize cats or generate text. While software AI has progressed at breakneck speed, getting a robot to reliably install plumbing or assemble complex machinery remains frustratingly difficult. Schmidhuber's revised timeline acknowledges what many AI researchers quietly admit: intelligence without embodiment is only half the equation for true transformative AI. You can have the smartest AI in the world, but if it can't physically build more of itself or construct the infrastructure for autonomous operation, you don't get a singularity—you get a very impressive chatbot. It's a sobering reminder that even the optimists are learning to respect the challenges of the physical world.",
    "target_date": "2049-01-01T00:00:00Z"
  },
  {
    "id": 114,
    "predictor_name": "Kai-Fu Lee",
    "predictor_type": "Tech Executive/Investor",
    "prediction_date": "2025-01-01",
    "predicted_date_low": "2040-01-01",
    "predicted_date_high": "2050-12-31",
    "predicted_date_best": "2045-01-01",
    "predicted_year_low": 2040,
    "predicted_year_high": 2050,
    "predicted_year_best": 2045,
    "prediction_type": "AGI",
    "confidence_level": "High confidence in skepticism based on historical analysis of breakthrough frequency",
    "confidence_label": "Mathematically Skeptical",
    "confidence_type": "high",
    "concept_keys": [
      "agi"
    ],
    "criteria_definition": "AGI requires 10-20 fundamental paradigm-shift breakthroughs, not incremental improvements on existing deep learning.",
    "source_name": "Multiple 2025 interviews",
    "source_url": "https://kaifulee.medium.com/ai-and-the-human-future-net-positive-ae3a500c1846",
    "headline": "Ex-Google China Chief Does The Math: One Breakthrough In 62 Years, Need 20 More, You Do The Math",
    "headline_slug": "kai-fu-lee-analyzes-ai-breakthrough-math",
    "tldr_summary": "Kai-Fu Lee is throwing cold water on the AGI hype train with some uncomfortable arithmetic. The former president of Google China and author of 'AI Superpowers' argues that AI has had exactly one fundamental breakthrough in 62 years: getting deep neural networks to actually work. Everything else—transformers, attention mechanisms, reinforcement learning—he considers incremental improvements on that single breakthrough from nine years ago. His calculation is brutally simple: if we need 10-20 more breakthroughs of similar magnitude to reach AGI, and we're getting them at the historical rate of one per 60+ years, we're looking at centuries, not decades. Even if breakthroughs accelerate 10x, that's still 60+ years minimum. This puts Lee in direct opposition to OpenAI (predicting AGI by late 2020s), Anthropic (2027), and most Silicon Valley leaders. His skepticism matters because he's not a Luddite—he's a venture capitalist actively investing in AI companies. He just thinks the gap between narrow AI that transforms industries and AGI that matches human general intelligence is vastly larger than the hype suggests. It's the difference between predicting the next iPhone and predicting the next human species.",
    "target_date": "2045-01-01T00:00:00Z"
  },
  {
    "id": 115,
    "predictor_name": "Kai-Fu Lee",
    "predictor_type": "Tech Executive/Investor",
    "prediction_date": "2025-10-22",
    "predicted_date_low": "2026-01-01",
    "predicted_date_high": "2026-12-31",
    "predicted_date_best": "2026-06-01",
    "predicted_year_low": 2026,
    "predicted_year_high": 2026,
    "predicted_year_best": 2026,
    "prediction_type": "Transformative AI",
    "confidence_level": "High confidence in near-term agent deployment while remaining skeptical of AGI timelines",
    "confidence_label": "Practically Optimistic",
    "confidence_type": "high",
    "concept_keys": [
      "transformative-ai",
      "economic-singularity"
    ],
    "criteria_definition": "AI agents become the fundamental unit of work, handling routine labor autonomously while humans manage them.",
    "source_name": "2026 predictions essay",
    "source_url": "https://theventurelens.substack.com/p/kai-fu-lee-ai-agents-2026",
    "headline": "Kai-Fu Lee Says 2026 Is Year Of AI Agents (Fifth Year In A Row Someone's Said That)",
    "headline_slug": "kai-fu-lee-predicts-2026-as-year-of-ai-agents",
    "tldr_summary": "At TED AI in San Francisco, Kai-Fu Lee made his boldest near-term prediction: 2026 will be the year AI agents fundamentally reshape work. But journalist Steven Levy couldn't resist pointing out the pattern—Lee once predicted speech transcription would reach human-level 'within five years,' then made the same five-year prediction five years later. Eventually he was right, just not on schedule. Now Lee is betting on autonomous digital workers that act on behalf of humans, not just answer questions. The irony is rich: while Lee is deeply skeptical about AGI (arguing we need 10-20 more fundamental breakthroughs over many decades), he's bullish on practical AI agents transforming work in the next 12 months. It's a split-brain prediction—pessimistic about the sci-fi singularity, optimistic about mundane automation. Lee, who runs Sinovation Ventures with $3 billion in AUM and founded 01.AI, argues we'll all become 'AI composers' managing these agents like a producer manages musicians. The prediction reveals the growing divide in AI forecasting: some focus on when machines match human intelligence generally, others focus on when they're useful enough to replace specific human jobs. Lee is firmly in the latter camp, and he's putting his investment dollars where his mouth is.",
    "target_date": "2026-06-01T00:00:00Z"
  },
  {
    "id": 116,
    "predictor_name": "Kalshi",
    "predictor_type": "Prediction Market",
    "prediction_date": "2026-01-01",
    "predicted_date_low": "2026-01-01",
    "predicted_date_high": "2040-12-31",
    "predicted_date_best": "2032-01-01",
    "predicted_year_low": 2026,
    "predicted_year_high": 2040,
    "predicted_year_best": 2032,
    "prediction_type": "AGI",
    "confidence_level": "42% probability represents real-money market consensus with significant trading volume",
    "confidence_label": "Money Where Mouth Is",
    "confidence_type": "medium",
    "concept_keys": [
      "agi",
      "prediction-markets"
    ],
    "criteria_definition": "AGI as defined by OpenAI's internal criteria, determined by OpenAI board with Microsoft partnership implications.",
    "source_name": "Kalshi prediction market",
    "source_url": "https://kalshi.com/markets/kxoaiagi/openai-achieves-agi/oaiagi",
    "headline": "$537K In Real Bets: Kalshi Traders Give OpenAI 42% Odds Of Hitting AGI Before 2030",
    "headline_slug": "kalshi-traders-bet-on-openai-agi-milestone",
    "tldr_summary": "Forget surveys and punditry—on Kalshi, the first CFTC-regulated prediction market in the US, traders are putting actual money where their predictions are. Currently, the market prices a 42% probability that OpenAI achieves AGI before 2030, with over $537,000 in trading volume on these specific contracts. That's a 10% increase from six months ago, following GPT-5's launch. The stakes matter because these aren't play-money predictions—traders face real financial consequences for being wrong, which typically produces more calibrated forecasts than expert surveys or corporate announcements. The resolution criteria are tied to OpenAI's own internal definition of AGI, which is significant because OpenAI has financial incentives in their Microsoft partnership linked to achieving AGI. The market's 42% odds imply roughly 10% annual probability over the next four years, suggesting traders take OpenAI's roadmaps seriously. Weekly volume across all AI-related contracts on major platforms hit nearly $6 billion as of February 2026, indicating this isn't a niche curiosity—it's become a real-time sentiment barometer for the most consequential technological transition in human history. When people risk their own capital, the hype tends to get filtered out fast.",
    "target_date": "2032-01-01T00:00:00Z"
  },
  {
    "id": 117,
    "predictor_name": "Kalshi Prediction Traders",
    "predictor_type": "Forecasting Market",
    "prediction_date": "2026-02-09",
    "predicted_date_low": "2027-01-01",
    "predicted_date_high": "2030-12-31",
    "predicted_date_best": "2029-01-01",
    "predicted_year_low": 2027,
    "predicted_year_high": 2030,
    "predicted_year_best": 2029,
    "prediction_type": "AGI",
    "confidence_level": "42% probability represents aggregate market confidence from engineers, VCs, and policy experts",
    "confidence_label": "Collectively Calibrated",
    "confidence_type": "medium",
    "concept_keys": [
      "agi",
      "turing-test"
    ],
    "criteria_definition": "Bifurcated between 'Weak AGI' (comprehensive multi-modal Turing Test) and 'Full AGI' (OpenAI's internal definition).",
    "source_name": "Kalshi prediction exchange",
    "source_url": "https://markets.financialcontent.com/stocks/article/predictstreet-2026-2-9-the-race-to-singularity-why-kalshi-traders-are-betting-big-on-openai-achieving-agi-before-2030",
    "headline": "Prediction Market Explodes To $6 Billion Weekly Volume As Traders Bet On Late-Decade AGI",
    "headline_slug": "prediction-market-booms-with-ai-future-bets",
    "tldr_summary": "The AGI race has moved from conference rooms to trading floors. On Kalshi and Polymarket, weekly trading volume for AI-related contracts hit $6 billion as of February 2026, with the 'When will OpenAI achieve AGI?' contract attracting the most attention. The 'Before 2030' probability sits at 42%, up from 32% six months earlier, while 'Before 2027' remains at a modest 14%. What makes this significant is the resolution criteria's strictness—contracts rely on official OpenAI announcements, independent third-party audits, and the trigger of the 'AGI clause' in the OpenAI-Microsoft partnership agreement. The market has bifurcated into 'Weak AGI' (passing a comprehensive multi-modal Turing Test) and 'Full AGI' (matching OpenAI's internal definition), acknowledging that different definitions yield wildly different timelines. The liquidity is crucial—$6 billion in weekly volume ensures prices aren't easily manipulated and instead reflect a 'wisdom of crowds' that includes people who actually build this technology. The steady climb in probability represents growing consensus that technical hurdles are being cleared faster than critics anticipated. Unlike corporate press releases designed to attract investment or academic papers designed to attract citations, these markets reward accuracy with profit and punish hype with losses. It's capitalism's way of asking: what do you really believe?",
    "target_date": "2029-01-01T00:00:00Z"
  },
  {
    "id": 118,
    "predictor_name": "Kaoutar El Maghraoui",
    "predictor_type": "IBM Researcher",
    "prediction_date": "2026-01-01",
    "predicted_date_low": "2026-01-01",
    "predicted_date_high": "2026-12-31",
    "predicted_date_best": "2026-06-01",
    "predicted_year_low": 2026,
    "predicted_year_high": 2026,
    "predicted_year_best": 2026,
    "prediction_type": "Transformative AI",
    "confidence_level": "Medium confidence in efficiency improvements and chip diversity, acknowledging uncertainty in breakthrough timing",
    "confidence_label": "Pragmatically Hedged",
    "confidence_type": "medium",
    "concept_keys": [
      "transformative-ai",
      "scaling-hypothesis",
      "economic-singularity"
    ],
    "criteria_definition": "Emergence of new chip architectures optimized for agentic workloads and inference efficiency over training scale.",
    "source_name": "IBM 2026 AI trends",
    "source_url": "https://www.ibm.com/think/news/ai-tech-trends-predictions-2026/",
    "headline": "IBM Researcher Declares 2026 'Year Of Efficient AI' As GPU Monopoly Faces ASIC Challengers",
    "headline_slug": "ibm-researcher-challenges-gpu-dominance",
    "tldr_summary": "Kaoutar El Maghraoui, Principal Research Scientist at IBM, is predicting 2026 as the year efficiency becomes AI's new frontier—and GPUs might finally get some competition. Speaking on IBM's 'Mixture of Experts' series, El Maghraoui argues that while GPUs will 'remain king,' we'll see ASIC-based accelerators, chiplet designs, analog inference, and even quantum-assisted optimizers mature enough to challenge Nvidia's dominance. Most intriguingly, she predicts 'a new class of chips for agentic workloads' will emerge—specialized hardware optimized for AI agents rather than training massive models. The timing matters: as AI shifts from 'bigger is better' training runs to deploying billions of inference operations, the economics change dramatically. El Maghraoui's prediction reflects a broader industry shift happening at IBM and beyond—after years of skepticism about AI's ROI, enterprises are finally seeing practical applications that justify investment. The chip architecture battle isn't just technical minutiae; it's about whether the next wave of AI runs on expensive Nvidia GPUs or cheaper, specialized alternatives. If El Maghraoui is right, 2026 could mark the beginning of the end for GPU monopoly, opening the door to more diverse, efficient, and accessible AI infrastructure. Or it could just be another year of Nvidia printing money.",
    "target_date": "2026-06-01T00:00:00Z"
  },
  {
    "id": 119,
    "predictor_name": "Kenneth Hayworth",
    "predictor_type": "Neuroscientist",
    "prediction_date": "2025-05-23",
    "predicted_date_low": "2075-01-01",
    "predicted_date_high": "2125-12-31",
    "predicted_date_best": "2100-01-01",
    "predicted_year_low": 2075,
    "predicted_year_high": 2125,
    "predicted_year_best": 2100,
    "prediction_type": "Transformative AI",
    "confidence_level": "Low to medium confidence based on fundamental neuroscience limitations and complexity of brain replication",
    "confidence_label": "Scientifically Cautious",
    "confidence_type": "medium",
    "concept_keys": [
      "transformative-ai"
    ],
    "criteria_definition": "Complete digital replication of consciousness requiring full sensory simulation and 86 billion neuron connections.",
    "source_name": "Neuroscience research on mind uploading",
    "source_url": "https://news.gatech.edu/news/2025/05/23/can-you-upload-human-mind-computer-neuroscientist-ponders-whats-possible",
    "headline": "Neuroscientist's Mind Upload Timeline: 50 Years To First Success, 100 To Make It Routine",
    "headline_slug": "mind-upload-timeline-neuroscience-frontier",
    "tldr_summary": "Kenneth Hayworth, a neuroscientist studying perception, tackles the ultimate sci-fi question: can you upload your mind to a computer? His answer is refreshingly honest—theoretically yes, practically not for at least 50 years for the first successful upload, and nearly a century before it becomes routine. The challenge isn't just copying 86 billion neurons and their trillions of connections; it's replicating the entire sensory experience that keeps consciousness functioning. Hayworth points out that sensory deprivation—total darkness or silence—drives people insane within hours. An uploaded mind would need a complete simulation of sight, sound, touch, smell, movement, heart rate, circadian rhythm, and thousands of other inputs the brain constantly processes. Without those inputs, you wouldn't be 'you' in a digital paradise; you'd be a consciousness trapped in sensory hell. The brain is considered the most complex object in the known universe, and Hayworth argues we've barely begun to understand it, let alone replicate it. His timeline acknowledges that while science has a track record of achieving the impossible—Moon landings, genome sequencing, smallpox eradication—the gap between 'theoretically possible' and 'actually achievable' can span generations. It's a sobering reminder that uploading your consciousness to escape mortality is not a retirement plan for anyone reading this.",
    "target_date": "2100-01-01T00:00:00Z"
  },
  {
    "id": 120,
    "predictor_name": "Leopold Aschenbrenner",
    "predictor_type": "Individual",
    "prediction_date": "2024-06-01",
    "predicted_date_low": "2026-01-01",
    "predicted_date_high": "2029-12-31",
    "predicted_date_best": "2027-01-01",
    "predicted_year_low": 2026,
    "predicted_year_high": 2029,
    "predicted_year_best": 2027,
    "prediction_type": "AGI",
    "confidence_level": "Very high confidence based on claimed insider knowledge of scaling trends and compute cluster buildouts",
    "confidence_label": "Urgently Alarmist",
    "confidence_type": "high",
    "concept_keys": [
      "agi",
      "scaling-hypothesis",
      "alignment"
    ],
    "criteria_definition": "AGI defined as AI systems outpacing college graduates by 2025/26, achieving superintelligence by decade's end.",
    "source_name": "Situational Awareness report (June 2024)",
    "source_url": "https://situational-awareness.ai/wp-content/uploads/2024/06/situationalawareness.pdf",
    "headline": "Fired OpenAI Researcher Drops 165-Page Bombshell: AGI By 2027, Superintelligence By 2030, 'The Project' Incoming",
    "headline_slug": "openai-insider-predicts-rapid-ai-advancement",
    "tldr_summary": "Leopold Aschenbrenner, fired from OpenAI's superalignment team, released 'Situational Awareness'—a 165-page manifesto arguing AGI by 2027 is 'strikingly plausible' based on his insider knowledge of scaling trends. The document reads like a thriller: trillion-dollar compute clusters, American industrial mobilization unseen in half a century, AI researchers automating themselves, then superintelligence shortly after. Aschenbrenner claims there are 'perhaps a few hundred people, most of them in San Francisco and the AI labs, that have situational awareness' about what's coming. He describes boardroom plans adding another zero every six months—from $10 billion to $100 billion to trillion-dollar clusters—and a 'fierce scramble to secure every power contract still available for the rest of the decade.' By his timeline, machines will outpace college graduates by 2025/26, then become smarter than any human by decade's end. The report predicts either 'an all-out race with the CCP' or 'an all-out war,' with national security forces unleashed and something called 'The Project' activated. Aschenbrenner's memo went viral, terrifying some and galvanizing others toward AGI racing. Critics note he was fired from OpenAI, possibly for leaking information. Supporters note he correctly predicted AI advances by trusting trendlines. Either way, it's the most detailed insider timeline yet published, and it's not reassuring.",
    "target_date": "2027-01-01T00:00:00Z"
  },
  {
    "id": 121,
    "predictor_name": "Louis Rosenberg",
    "predictor_type": "AI Researcher",
    "prediction_date": "2019-02-09",
    "predicted_date_low": "2030-01-01",
    "predicted_date_high": "2030-12-31",
    "predicted_date_best": "2030-01-01",
    "predicted_year_low": 2030,
    "predicted_year_high": 2030,
    "predicted_year_best": 2030,
    "prediction_type": "Singularity",
    "confidence_level": "Rosenberg provided a specific year prediction (2030) as part of a comprehensive survey of 32 AI PhD researchers, suggesting medium-high confidence in the timeline",
    "confidence_label": "Survey Says Confident",
    "confidence_type": "medium",
    "concept_keys": [
      "event-horizon"
    ],
    "criteria_definition": "Technological singularity where computer intelligence surpasses and exceeds human intelligence with profound societal consequences",
    "source_name": "Emerj interview 2024",
    "source_url": "https://emerj.com/when-will-we-reach-the-singularity-a-timeline-consensus-from-ai-researchers/",
    "headline": "AI Researcher Rosenberg Picks 2030 For Singularity In Major PhD Survey",
    "headline_slug": "ai-researcher-predicts-tech-singularity-by-2030",
    "tldr_summary": "Louis Rosenberg, an AI researcher, predicted 2030 for the technological singularity as part of Emerj's comprehensive survey of 32 PhD researchers examining when machine intelligence would surpass human capabilities. The survey tackled six major questions about humanity's post-singularity future, from timeline predictions to whether AI would fragment or consolidate into a singleton. Rosenberg's 2030 prediction sits in the near-term optimistic camp, suggesting just over a decade until computers exceed human intelligence with 'profound consequences for society.' The survey represents one of the more systematic attempts to build consensus among credentialed AI researchers about humanity's most consequential technological threshold.",
    "target_date": "2030-01-01T00:00:00Z"
  },
  {
    "id": 122,
    "predictor_name": "MIT Report",
    "predictor_type": "Survey",
    "prediction_date": "2025-08-01",
    "predicted_date_low": "2026-01-01",
    "predicted_date_high": "2047-12-31",
    "predicted_date_best": "2028-01-01",
    "predicted_year_low": 2026,
    "predicted_year_high": 2047,
    "predicted_year_best": 2028,
    "prediction_type": "AGI",
    "confidence_level": "Report presents aggregate forecasts with 50% probability markers, indicating measured confidence based on community consensus rather than individual certainty",
    "confidence_label": "Cautiously Aggregating",
    "confidence_type": "medium",
    "concept_keys": [
      "agi",
      "accelerating-change",
      "survey-drift"
    ],
    "criteria_definition": "AI systems achieving several AGI milestones including Nobel Prize-level domain intelligence, multi-interface capability, and autonomous goal-directed reasoning",
    "source_name": "MIT Technology Review - The Road to AGI (August 2025)",
    "source_url": "https://wp.technologyreview.com/wp-content/uploads/2025/08/MITTR_ArmEBrief_V12_final.pdf",
    "headline": "MIT Tech Review Synthesizes Expert Consensus: 50% Chance Of AGI By 2028, Full Capability By 2047",
    "headline_slug": "mit-report-forecasts-agi-by-2028",
    "tldr_summary": "MIT Technology Review's August 2025 report compiled expert forecasts showing dramatic timeline compression—from 80+ years pre-GPT to under 20 years post-ChatGPT. The analysis found 50% probability of early AGI systems by 2028 and full human-surpassing capability across all tasks by 2047. Dario Amodei predicts 'powerful AI' as early as 2026 with Nobel Prize-level intelligence, while Sam Altman sees AGI properties 'coming into view' with 'super-exponential' value creation. The report emphasizes that achieving AGI requires massive infrastructure: 400GW of datacenter power (exceeding total US electricity supply), 200 million chips, and $9 trillion in cumulative investment. The bimodal distribution—near-term weak AGI versus longer-term strong AGI—captures the field's fundamental split on whether we're talking about impressive narrow systems or genuine general intelligence.",
    "target_date": "2028-01-01T00:00:00Z"
  },
  {
    "id": 123,
    "predictor_name": "Manifold Markets",
    "predictor_type": "Prediction Market",
    "prediction_date": "2026-01-01",
    "predicted_date_low": "2026-01-01",
    "predicted_date_high": "2040-12-31",
    "predicted_date_best": "2032-01-01",
    "predicted_year_low": 2026,
    "predicted_year_high": 2040,
    "predicted_year_best": 2032,
    "prediction_type": "AGI",
    "confidence_level": "Market-based probability distribution with 1,100+ participants trading on likelihood, representing aggregated confidence through financial incentives",
    "confidence_label": "Wisdom Of Crowds",
    "confidence_type": "medium",
    "concept_keys": [
      "agi",
      "turing-test",
      "prediction-markets"
    ],
    "criteria_definition": "AI system capable of passing high-quality adversarial Turing test where human judges cannot reliably unmask AI as impostor despite good-faith attempts",
    "source_name": "Manifold Markets (~1100 contributors)",
    "source_url": "https://manifold.markets/ManifoldAI/agi-when-resolves-to-the-year-in-wh-d5c5ad8e4708",
    "headline": "Manifold Prediction Market: 1,100 Traders Put 47% Odds On AGI Before 2028",
    "headline_slug": "manifold-markets-predict-agi-odds-at-47-percent",
    "tldr_summary": "Manifold Markets, a play-money prediction platform tracking AGI arrival through adversarial Turing test capability, shows shockingly aggressive timelines from its 1,100+ participants. The market currently places 47% probability on AGI arriving before 2028—just two years away—with median prediction landing at 2032. The platform references both the Longbets wager between Ray Kurzweil and Mitch Kapor and Matthew Barnett's Metaculus criteria, requiring AI to convince human judges of its humanness through text-based dialogue where judges actively try to unmask imposters. Prediction markets theoretically aggregate diverse opinions more effectively than expert surveys by creating financial incentives for accuracy, though play-money markets lack real skin-in-the-game. The near-term odds represent either collective wisdom about rapid capability gains or groupthink optimism fueled by recent LLM breakthroughs. Either way, over 1,000 people are betting AGI is closer than most academic surveys suggest.",
    "target_date": "2032-01-01T00:00:00Z"
  },
  {
    "id": 124,
    "predictor_name": "Marco Trombetti",
    "predictor_type": "Tech Executive",
    "prediction_date": "2025-12-19",
    "predicted_date_low": "2029-01-01",
    "predicted_date_high": "2030-12-31",
    "predicted_date_best": "2029-06-15",
    "predicted_year_low": 2029,
    "predicted_year_high": 2030,
    "predicted_year_best": 2029,
    "prediction_type": "Singularity",
    "confidence_level": "High confidence based on quantitative Time to Edit (TTE) metric tracking AI translation performance approaching human parity from 2014-2022, with clear trend line extrapolation",
    "confidence_label": "Metric-Driven Confident",
    "confidence_type": "high",
    "concept_keys": [
      "event-horizon"
    ],
    "criteria_definition": "AI achieving human-level translation accuracy measured by Time to Edit metric, indicating broader artificial general intelligence through language mastery",
    "source_name": "Popular Mechanics December 2025",
    "source_url": "https://www.popularmechanics.com/technology/robots/a69820101/when-the-singularity-will-happen-trend-timeline/",
    "headline": "Translation CEO Predicts Singularity By 2030 Using 'Time To Edit' Productivity Metric",
    "headline_slug": "ceo-uses-translation-metrics-to-forecast-singularity",
    "tldr_summary": "Marco Trombetti, CEO of Rome-based translation company Translated, developed a novel singularity predictor: Time to Edit (TTE), measuring how long professional editors take fixing AI-generated translations versus human ones. His company tracked this metric across 2 billion post-editing events from 2014 to 2022, showing machines rapidly closing the gap. Trombetti argues language represents AI's hardest challenge—'the most natural thing for humans'—making translation parity a legitimate AGI proxy. His data-driven extrapolation points to end-of-decade singularity, somewhere between 2029-2030. The approach offers refreshing specificity compared to vague expert intuitions: rather than pontificating about consciousness or reasoning, Trombetti measures actual productivity convergence. If AI can translate as well as humans, the logic goes, it demonstrates the broad cognitive capabilities defining general intelligence. Whether translation truly captures AGI's essence remains debatable, but the quantitative methodology beats hand-waving about 'profound transformations.'",
    "target_date": "2029-06-15T00:00:00Z"
  },
  {
    "id": 125,
    "predictor_name": "Marcus Hutter",
    "predictor_type": "AI Researcher",
    "prediction_date": "2018-05-03",
    "predicted_date_low": "2040-01-01",
    "predicted_date_high": "2080-12-31",
    "predicted_date_best": "2060-01-01",
    "predicted_year_low": 2040,
    "predicted_year_high": 2080,
    "predicted_year_best": 2060,
    "prediction_type": "AGI",
    "confidence_level": "Explicitly uncertain with wide 40-year range, emphasizing plausibility rather than probability, reflecting honest acknowledgment of forecasting difficulty",
    "confidence_label": "Humbly Uncertain",
    "confidence_type": "low",
    "concept_keys": [
      "agi",
      "accelerating-change",
      "alignment"
    ],
    "criteria_definition": "Artificial general intelligence matching or exceeding human cognitive abilities across broad range of tasks rather than single domain excellence",
    "source_name": "AI researcher papers",
    "source_url": "https://arxiv.org/pdf/1805.01109",
    "headline": "AI Safety Researcher Hutter Resists Hype: AGI Timeline 'Uncertain, 40-80 Years Plausible'",
    "headline_slug": "ai-safety-researcher-warns-of-uncertain-timeline",
    "tldr_summary": "Marcus Hutter, prominent AI safety researcher, stands out in the prediction landscape by emphasizing uncertainty rather than confident timelines. His 2018 paper suggests AGI could arrive anywhere from 2040 to 2080—a refreshingly wide 40-year range reflecting genuine epistemic humility. While tech executives throw around 3-5 year predictions and entrepreneurs declare ASI their 'life's purpose,' Hutter's academic caution acknowledges that predicting transformative AI is fundamentally difficult. The 40-80 year timeframe suggests AGI is neither imminent nor impossibly distant, but genuinely uncertain. This stance aligns with researchers who've watched decades of AI winters and summers, understanding that capability progress rarely follows smooth exponential curves. Hutter's uncertainty doesn't mean AGI won't arrive sooner—it means honest forecasters should admit they don't know. In a field dominated by confident proclamations and marketing-driven timelines, admitting 'we're not sure, could be 40-80 years' represents intellectual integrity worth noting.",
    "target_date": "2060-01-01T00:00:00Z"
  },
  {
    "id": 126,
    "predictor_name": "Marcus du Sautoy",
    "predictor_type": "Mathematician",
    "prediction_date": "2024-07-01",
    "predicted_date_low": "2030-01-01",
    "predicted_date_high": "2050-12-31",
    "predicted_date_best": "2040-01-01",
    "predicted_year_low": 2030,
    "predicted_year_high": 2050,
    "predicted_year_best": 2040,
    "prediction_type": "Superintelligence",
    "confidence_level": "Moderate confidence with emphasis on both hopeful possibilities and harrowing risks, reflecting balanced academic perspective on transformative AI timeline",
    "confidence_label": "Philosophically Cautious",
    "confidence_type": "medium",
    "concept_keys": [
      "superintelligence",
      "soft-takeoff"
    ],
    "criteria_definition": "Superintelligence capable of exceeding human control with potential for either unprecedented flourishing or permanent dystopian lock-in",
    "source_name": "Oxford discussion with Bostrom (July 2024)",
    "source_url": "https://unherd.com/2023/11/nick-bostrom-will-ai-lead-to-tyranny/",
    "headline": "Oxford Mathematician du Sautoy Discusses 'Hopeful And Harrowing' AI Future With Bostrom",
    "headline_slug": "oxford-mathematician-on-ai-future-with-bostrom",
    "tldr_summary": "Marcus du Sautoy, Oxford mathematician, engaged with philosopher Nick Bostrom in mid-2024 about AI's trajectory toward superintelligence, emphasizing both transformative potential and existential risk. Their discussion centered on Bostrom's concept of 'existential catastrophe'—not just human extinction but permanent lock-in to suboptimal states like global surveillance dystopias that could never be overthrown. Du Sautoy's timeline spans 2030-2050, reflecting measured optimism tempered by awareness of civilizational stakes. The conversation occurred as 'the wheels are coming off' institutional assumptions—the faith that wars decrease annually, education improves gradually, and progress marches inevitably forward. Unlike tech executives promising AGI in 3-5 years, du Sautoy's mathematical background brings statistical rigor to forecasting. His mid-century timeline acknowledges both rapid recent progress and fundamental unsolved problems in achieving general intelligence. The discussion represents academic AI discourse at its best: serious engagement with transformative possibilities without breathless hype or dismissive skepticism.",
    "target_date": "2040-01-01T00:00:00Z"
  },
  {
    "id": 127,
    "predictor_name": "Masayoshi Son",
    "predictor_type": "Individual",
    "prediction_date": "2023-10-09",
    "predicted_date_low": "2030-01-01",
    "predicted_date_high": "2035-12-31",
    "predicted_date_best": "2033-01-01",
    "predicted_year_low": 2030,
    "predicted_year_high": 2035,
    "predicted_year_best": 2033,
    "prediction_type": "Singularity",
    "confidence_level": "Very high confidence despite admitting 'Nobody but me believes AGI will be a reality in ten years. I am the first one that clings strongly to this belief'",
    "confidence_label": "Stubbornly Alone",
    "confidence_type": "high",
    "concept_keys": [
      "event-horizon",
      "industry-academia-divergence"
    ],
    "criteria_definition": "Artificial general intelligence 10 times more intelligent than total human intelligence, leading to singularity crossover point",
    "source_name": "SoftBank World corporate conference keynote (October 2023)",
    "source_url": "https://www.theregister.com/2023/10/09/softbank_ceo_masayoshi_son_ai_vision/",
    "headline": "SoftBank Billionaire Declares 'Nobody But Me Believes' AGI Arrives In 10 Years",
    "headline_slug": "softbank-billionaires-bold-agi-prediction",
    "tldr_summary": "Masayoshi Son told SoftBank World attendees in October 2023 that the singularity would arrive within a decade, with AGI becoming '10 times more intelligent than all total human intelligence' by 2033. The Japanese billionaire explicitly acknowledged standing alone: 'Nobody but me believes AGI will be a reality in ten years. I am the first one that clings strongly to this belief.' Son compared those not embracing AI to 'unempowered goldfish' and warned the intelligence delta between AGI users and non-users would equal the gap between humans and monkeys. His conviction that artificial super intelligence (ASI) would follow within 20 years positions him as the most aggressive major investor on AGI timelines. This is the same Son who turned $20 million into $100 billion with his Alibaba bet, then lost most of it on WeWork and other failed tech investments. His track record combines visionary wins with spectacular failures—making his AGI prediction either prescient or another example of billionaire hubris unconstrained by expert consensus.",
    "target_date": "2033-01-01T00:00:00Z"
  },
  {
    "id": 128,
    "predictor_name": "Masayoshi Son",
    "predictor_type": "Individual",
    "prediction_date": "2024-06-21",
    "predicted_date_low": "2027-01-01",
    "predicted_date_high": "2029-12-31",
    "predicted_date_best": "2028-01-01",
    "predicted_year_low": 2027,
    "predicted_year_high": 2029,
    "predicted_year_best": 2028,
    "prediction_type": "AGI",
    "confidence_level": "Absolute certainty expressed as personal destiny: 'It may sound strange, but I think I was born to realize ASI. Realizing ASI is my only focus'",
    "confidence_label": "Messianically Certain",
    "confidence_type": "certain",
    "concept_keys": [
      "agi",
      "industry-academia-divergence"
    ],
    "criteria_definition": "AGI 1-10x smarter than humans arriving within 3-5 years, followed by ASI 10,000x smarter within 10 years",
    "source_name": "SoftBank Annual General Meeting (June 2024)",
    "source_url": "https://fortune.com/2024/06/27/softbank-ceo-masayoshi-son-born-to-create-artificial-super-intelligence/",
    "headline": "SoftBank CEO Declares Achieving ASI His 'Life's Purpose,' Frames Entire Career As Warm-Up",
    "headline_slug": "softbank-ceos-asi-mission-unveiled",
    "tldr_summary": "In a remarkable June 2024 shareholder address, Masayoshi Son declared that realizing artificial superintelligence is 'my life's purpose' and claimed 'I was born to realize ASI.' The SoftBank CEO argued every investment throughout his career—from Uber to Alibaba—was merely a 'warm-up' for his AI investments. Son predicted AGI (1-10x human intelligence) would arrive within 3-5 years (2027-2029) and ASI (10,000x human intelligence) within 10 years (2034). His personal motivation stems from losing his father to cancer: 'I do not want anyone to suffer from such diseases' and believes ASI with 'ten thousand times the human wisdom' could prevent such sorrows. This isn't typical CEO market opportunity talk—Son frames his entire existence around achieving superintelligence. During the rare public appearance, he questioned his own purpose in life and concluded he was put on Earth specifically for this goal. His $100B+ AI infrastructure plans suggest he's putting his money where his messianic vision is, making this either the most committed AGI bet in history or spectacular hubris from the man who lost billions on WeWork.",
    "target_date": "2028-01-01T00:00:00Z"
  },
  {
    "id": 129,
    "predictor_name": "Masayoshi Son",
    "predictor_type": "Individual",
    "prediction_date": "2024-10-29",
    "predicted_date_low": "2032-01-01",
    "predicted_date_high": "2036-12-31",
    "predicted_date_best": "2034-01-01",
    "predicted_year_low": 2032,
    "predicted_year_high": 2036,
    "predicted_year_best": 2034,
    "prediction_type": "Superintelligence",
    "confidence_level": "Very high confidence with specific infrastructure requirements: 400GW power, 200 million chips, $9 trillion capex, suggesting detailed modeling backing prediction",
    "confidence_label": "Exponentially Escalating",
    "confidence_type": "high",
    "concept_keys": [
      "superintelligence",
      "industry-academia-divergence"
    ],
    "criteria_definition": "Artificial superintelligence 10,000 times smarter than human brain, requiring 400GW datacenter power and $9 trillion investment",
    "source_name": "Future Investment Initiative Saudi Arabia (October 2024)",
    "source_url": "https://www.theregister.com/2024/10/29/softbank_super_ai/",
    "headline": "SoftBank CEO Ups Intelligence Multiplier To 10,000x: ASI By 2035 For $9 Trillion",
    "headline_slug": "softbank-ceos-trillion-dollar-asi-vision",
    "tldr_summary": "Masayoshi Son told Saudi investors at the Future Investment Initiative that artificial superintelligence—10,000 times smarter than any human—would arrive 'within about a decade' around 2035. The prediction came with eye-watering infrastructure requirements: 400GW of AI datacenter power (exceeding total US electricity supply), 200 million chips, and $9 trillion in cumulative capex. Son argued this investment is 'reasonable' because super AI producing $9 trillion annually in benefits would make the initial cost 'small money.' He predicted four companies would achieve trillion-dollar profits, adding 'I want to be one of them.' The billionaire's escalating multipliers tell a story: October 2023 he predicted AGI '10 times more intelligent than total human intelligence,' June 2024 he mentioned '10,000 times' for ASI, and now October 2024 he's firmly anchored on the 10,000x figure. Son also adjusted his timeline—moving from 3-5 years for AGI in June to 'about a decade' for full superintelligence. His conviction that Nvidia is undervalued despite being the world's most valuable company shows he's betting everything on this vision materializing.",
    "target_date": "2034-01-01T00:00:00Z"
  },
  {
    "id": 130,
    "predictor_name": "Masayoshi Son",
    "predictor_type": "Individual",
    "prediction_date": "2025-02-08",
    "predicted_date_low": "2026-01-01",
    "predicted_date_high": "2027-12-31",
    "predicted_date_best": "2027-01-01",
    "predicted_year_low": 2026,
    "predicted_year_high": 2027,
    "predicted_year_best": 2027,
    "prediction_type": "AGI",
    "confidence_level": "High confidence with pattern of continuous timeline compression: 2033→2028→2027, suggesting reactive updating based on capability progress",
    "confidence_label": "Serially Revising",
    "confidence_type": "high",
    "concept_keys": [
      "agi",
      "industry-academia-divergence"
    ],
    "criteria_definition": "Artificial general intelligence matching or exceeding human-level cognitive abilities across broad range of tasks",
    "source_name": "SoftBank enterprise event Tokyo (February 2025)",
    "source_url": "https://research.aimultiple.com/artificial-general-intelligence-singularity-timing/",
    "headline": "Masa Son Accelerates Timeline Again: AGI Now Expected By 2027, Third Revision In 16 Months",
    "headline_slug": "masa-sons-accelerating-ai-timeline",
    "tldr_summary": "Masayoshi Son revised his AGI timeline yet again in February 2025, moving the prediction from 2028 to 2027—his third major revision in 16 months. The pattern tells the story: October 2023 he predicted AGI by 2033 (10 years out), June 2024 he moved it to 2028 (3-5 years), and now February 2025 he's landed on 2027 (2 years away). Each revision compresses the timeline further, suggesting either adaptive forecasting as capabilities progress faster than expected or chronic overoptimism that keeps pulling predictions closer. Son's serial revisions make him the most aggressive timeline compressor among major investors—while others maintain consistent predictions, he continuously updates shorter. This analysis comes from AIMultiple's comprehensive survey of 9,300 predictions across 8 peer-reviewed papers, 15 AI experts, and prediction markets. Son's behavior reveals something important about AGI forecasting: predictions aren't static prophecies but dynamic reactions to unfolding progress. Whether his constant acceleration toward nearer-term AGI reflects superior pattern recognition or wishful thinking remains the trillion-dollar question.",
    "target_date": "2027-01-01T00:00:00Z"
  },
  {
    "id": 131,
    "predictor_name": "Max Tegmark",
    "predictor_type": "Physicist/Futurist",
    "prediction_date": "2017-08-23",
    "predicted_date_low": "2030-01-01",
    "predicted_date_high": "2100-12-31",
    "predicted_date_best": "2065-01-01",
    "predicted_year_low": 2030,
    "predicted_year_high": 2100,
    "predicted_year_best": 2065,
    "prediction_type": "Superintelligence",
    "confidence_level": "Explores multiple scenarios without committing to specific timeline; focuses on societal implications rather than prediction certainty",
    "confidence_label": "Philosophically Hedging",
    "confidence_type": "low",
    "concept_keys": [
      "superintelligence",
      "recursive-self-improvement",
      "economic-singularity"
    ],
    "criteria_definition": "AI that can redesign its own hardware and internal structure, not just learn software",
    "source_name": "Life 3.0 / FQXi research 2025",
    "source_url": "https://en.wikipedia.org/wiki/Life_3.0",
    "headline": "MIT Physicist's 'Life 3.0' Spans 70-Year Window: Superintelligence Anytime Between 2030 and 2100",
    "headline_slug": "mit-physicist-reveals-superintelligence-timeline",
    "tldr_summary": "Max Tegmark's 2017 book 'Life 3.0' introduced a framework for thinking about AI evolution: Life 1.0 (biological), Life 2.0 (cultural), and Life 3.0 (technological beings that can redesign their own hardware). The MIT cosmologist and Future of Life Institute founder explores superintelligence scenarios without pinning down exact dates, suggesting it could arrive anywhere from decades to a century away. Tegmark focuses less on timeline certainty and more on what humanity should do to maximize positive outcomes, examining everything from technological unemployment to AI weapons. His deliberately broad range reflects the fundamental uncertainty in predicting recursive self-improvement—once AI can redesign itself, all bets are off. The book became influential in effective altruism circles for its accessible treatment of existential risk without the doomsday urgency of figures like Yudkowsky.",
    "target_date": "2065-01-01T00:00:00Z"
  },
  {
    "id": 132,
    "predictor_name": "McKinsey Analysts",
    "predictor_type": "Consulting Firm",
    "prediction_date": "2023-06-01",
    "predicted_date_low": "2025-01-01",
    "predicted_date_high": "2040-12-31",
    "predicted_date_best": "2035-01-01",
    "predicted_year_low": 2025,
    "predicted_year_high": 2040,
    "predicted_year_best": 2035,
    "prediction_type": "Transformative AI",
    "confidence_level": "High confidence in economic modeling across 63 use cases, 16 business functions, and 47 countries; reaffirmed in January 2025",
    "confidence_label": "Confidently Consulting",
    "confidence_type": "high",
    "concept_keys": [
      "transformative-ai",
      "economic-singularity"
    ],
    "criteria_definition": "Generative AI achieving $2.6-4.4 trillion in annual economic value across corporate use cases",
    "source_name": "McKinsey AI reports 2025",
    "source_url": "https://www.mckinsey.com/mgi/media-center/ai-could-increase-corporate-profits-by-4-trillion-a-year-according-to-new-research",
    "headline": "McKinsey Promises $4.4 Trillion AI Bonanza—If Only Companies Would Actually Use It",
    "headline_slug": "mckinsey-predicts-massive-ai-economic-impact",
    "tldr_summary": "McKinsey dropped a bombshell in June 2023: generative AI could unlock $2.6 to $4.4 trillion in annual economic value—roughly the size of Germany's entire economy. The consulting giant analyzed 63 specific use cases across customer operations, marketing, software engineering, and R&D, projecting 0.1% to 0.6% annual productivity gains through 2040. Combined with broader automation, they see up to 3.3 percentage points added to productivity growth. But here's the catch: this represents long-term potential, not immediate gains, and requires massive infrastructure investment, workforce retraining, and widespread adoption. Two years later, McKinsey reaffirmed the $4.4 trillion figure in January 2025 while quietly noting that corporate adoption is 'still lagging.' Critics call it the '$4.4 trillion mirage,' arguing McKinsey's rosy projections mask brutal implementation challenges and ignore displacement costs. The estimate sits alongside Goldman Sachs' $7 trillion GDP boost prediction, suggesting Wall Street and consulting firms are singing from the same hymnal—even if reality hasn't caught up.",
    "target_date": "2035-01-01T00:00:00Z"
  },
  {
    "id": 133,
    "predictor_name": "Metaculus Community",
    "predictor_type": "Prediction Market",
    "prediction_date": "2025-12-01",
    "predicted_date_low": "2026-01-01",
    "predicted_date_high": "2030-12-31",
    "predicted_date_best": "2027-10-01",
    "predicted_year_low": 2026,
    "predicted_year_high": 2030,
    "predicted_year_best": 2027,
    "prediction_type": "AGI (weak)",
    "confidence_level": "1,700+ forecasters with demonstrated track records; median has shifted 45 years earlier in just 4 years",
    "confidence_label": "Crowd Wisdom Compressed",
    "confidence_type": "medium",
    "concept_keys": [
      "agi",
      "turing-test",
      "prediction-markets"
    ],
    "criteria_definition": "Single AI system matching human performance across diverse cognitive tasks without specialized training per domain",
    "source_name": "Metaculus (~1700 forecasters)",
    "source_url": "https://www.metaculus.com/questions/3479/date-weakly-general-ai-is-publicly-known/",
    "headline": "1,700 Forecasters on Metaculus: 'Weak AGI' Median Hits October 2027",
    "headline_slug": "metaculus-community-forecasts-weak-agi-arrival",
    "tldr_summary": "Metaculus, the prediction platform where accuracy actually matters to your reputation, shows its 1,700-strong forecasting community putting 'weakly general AI' at October 2027—just two years away. This isn't the full human-replacement package, but rather a single system that can match humans across diverse tasks without needing specialized training for each domain. The platform distinguishes this from 'strong AGI' (which requires passing adversarial Turing tests, 90%+ on all academic benchmarks, and embodied robotics). What's stunning is the timeline collapse: just four years ago, the median sat around 2072. That's a 45-year acceleration driven by GPT-4, Claude, and the post-2022 capabilities explosion. The wide uncertainty range—spanning 2026 to 2030—reveals deep disagreement about whether we're one breakthrough away or facing fundamental obstacles. Metaculus forecasters include AI researchers, effective altruism community members, and serious prediction nerds who track their calibration obsessively. Their aggressive timeline reflects the community's post-ChatGPT recalibration from 'decades away' to 'maybe next election cycle.'",
    "target_date": "2027-10-01T00:00:00Z"
  },
  {
    "id": 134,
    "predictor_name": "Metaculus Community",
    "predictor_type": "Prediction Market",
    "prediction_date": "2026-02-01",
    "predicted_date_low": "2028-01-01",
    "predicted_date_high": "2046-12-31",
    "predicted_date_best": "2033-06-01",
    "predicted_year_low": 2028,
    "predicted_year_high": 2046,
    "predicted_year_best": 2033,
    "prediction_type": "AGI (strong)",
    "confidence_level": "1,800 forecasters with 12+ year uncertainty range (2026-2039 quartiles); dramatically shifted from 2072 to 2033 in 4 years",
    "confidence_label": "Rigorously Uncertain",
    "confidence_type": "medium",
    "concept_keys": [
      "agi",
      "scaling-hypothesis",
      "turing-test",
      "alignment",
      "prediction-markets"
    ],
    "criteria_definition": "AI passing adversarial Turing tests, achieving 90%+ on all MMLU tasks, 90% on APPS coding, plus general robotic capabilities",
    "source_name": "Metaculus (~1800 forecasters)",
    "source_url": "https://www.metaculus.com/questions/5121/when-will-the-first-general-ai-system-be-devised-tested-and-publicly-announced/",
    "headline": "Metaculus 'Strong AGI' Forecast: June 2033 for Full Human Replacement",
    "headline_slug": "metaculus-predicts-strong-agi-milestone",
    "tldr_summary": "Metaculus draws a sharp line between 'weak' and 'strong' AGI, and their 1,800 forecasters put the real deal—full human-level performance across adversarial Turing tests, every academic benchmark, coding challenges, and physical robotics—at June 2033. That's six years after their weak AGI estimate, capturing the community's view that crossing from 'pretty good' to 'actually matches humans at everything' involves serious engineering challenges. The resolution criteria are brutal: 75%+ on every MMLU task, 90% mean across all of them, 90% on APPS coding, plus the ability to assemble a Ferrari model kit. This isn't narrow AI excellence; it's genuine human-replacement capability. The timeline has compressed spectacularly—from 50 years away in 2022 to just 7 years in 2026—but the 12-year spread between lower and upper quartiles (December 2026 to March 2039) reveals massive disagreement. Some forecasters think we're basically there; others see fundamental obstacles requiring another decade-plus. The platform's track record on near-term tech predictions lends credibility, but betting on 2033 means believing current scaling laws hold and alignment doesn't become an insurmountable bottleneck.",
    "target_date": "2033-06-01T00:00:00Z"
  },
  {
    "id": 135,
    "predictor_name": "Metaculus Forecasters",
    "predictor_type": "Forecasting Platform",
    "prediction_date": "2026-02-01",
    "predicted_date_low": "2029-01-01",
    "predicted_date_high": "2040-12-31",
    "predicted_date_best": "2033-01-01",
    "predicted_year_low": 2029,
    "predicted_year_high": 2040,
    "predicted_year_best": 2033,
    "prediction_type": "AGI",
    "confidence_level": "Community median shifted 45 years in 4 years; 25% probability by 2029, 50% by 2033",
    "confidence_label": "Dramatically Recalibrating",
    "confidence_type": "medium",
    "concept_keys": [
      "agi",
      "accelerating-change",
      "scaling-hypothesis"
    ],
    "criteria_definition": "Artificial general intelligence matching human cognitive abilities across broad domains",
    "source_name": "Metaculus Feb 2026",
    "source_url": "https://80000hours.org/2025/03/when-do-experts-expect-agi-to-arrive/",
    "headline": "Forecasters: 50% AGI by 2033 (45-Year Collapse from 2022 Estimates)",
    "headline_slug": "metaculus-agi-forecast-dramatically-accelerates",
    "tldr_summary": "In what might be the fastest timeline collapse in forecasting history, Metaculus community predictions for AGI have plummeted from 'around 2072' in 2022 to '50% by 2033' in early 2026—a 45-year acceleration in just four years. The shift reflects post-GPT-4 recalibration across the entire forecasting community as capabilities that seemed decades away suddenly materialized. The current distribution shows 25% probability by 2029 and 50% by 2033, with uncertainty spanning 2029 to 2040. This represents thousands of individual forecasters updating their beliefs as scaling laws continued delivering, multimodal models emerged, and reasoning capabilities improved faster than almost anyone predicted. The dramatic compression raises a meta-question: are forecasters finally calibrated to AI's exponential trajectory, or are they overcorrecting after being burned by conservative predictions? Either way, the consensus has shifted from 'our grandchildren's problem' to 'current college students might see this before they retire.' The 80,000 Hours analysis tracking these shifts notes this represents one of the most significant belief updates in the effective altruism and forecasting communities.",
    "target_date": "2033-01-01T00:00:00Z"
  },
  {
    "id": 136,
    "predictor_name": "Muller & Bostrom Survey",
    "predictor_type": "Survey",
    "prediction_date": "2016-01-01",
    "predicted_date_low": "2040-01-01",
    "predicted_date_high": "2075-12-31",
    "predicted_date_best": "2040-01-01",
    "predicted_year_low": 2040,
    "predicted_year_high": 2075,
    "predicted_year_best": 2040,
    "prediction_type": "AGI",
    "confidence_level": "Survey-based aggregate of expert opinions; 50% confidence by 2040, 90% by 2075",
    "confidence_label": "Academically Cautious",
    "confidence_type": "medium",
    "concept_keys": [
      "agi",
      "scaling-hypothesis",
      "alignment",
      "survey-drift"
    ],
    "criteria_definition": "High-level machine intelligence matching or exceeding human cognitive performance",
    "source_name": "Muller & Bostrom survey (550)",
    "source_url": "https://research.aimultiple.com/artificial-general-intelligence-singularity-timing/",
    "headline": "2016 Survey: 50-50 AGI by 2040, 90% by 2075 (Seems Adorably Quaint Now)",
    "headline_slug": "muller-bostrom-survey-agi-predictions",
    "tldr_summary": "Vincent Müller and Nick Bostrom's 2016 survey of AI researchers captured a moment when AGI still felt comfortably distant: 50% probability by 2040, 90% by 2075. The survey asked experts when 'high-level machine intelligence' would match or exceed human cognitive performance across the board. Their median estimates suggested a generation or two before humanity would face this challenge—plenty of time for careful safety work and societal preparation. Fast forward to 2026, and these timelines look charmingly conservative. Current Metaculus forecasts put 50% probability around 2033, and some AI lab leaders are talking about late 2020s. The survey has become a historical artifact documenting how rapidly expert opinion shifted post-2022. What changed? GPT-3, GPT-4, scaling laws that kept delivering, and the sudden realization that 'human-level' might not require solving consciousness or replicating biological neurons—just really good pattern matching at scale. The 2016 survey represents the 'before times' when AI researchers could still imagine AGI as their grandchildren's problem, not their own career's climax.",
    "target_date": "2040-01-01T00:00:00Z"
  },
  {
    "id": 137,
    "predictor_name": "Mustafa Suleyman",
    "predictor_type": "AI Executive",
    "prediction_date": "2025-01-01",
    "predicted_date_low": "2026-01-01",
    "predicted_date_high": "2027-06-30",
    "predicted_date_best": "2026-07-01",
    "predicted_year_low": 2026,
    "predicted_year_high": 2027,
    "predicted_year_best": 2026,
    "prediction_type": "Transformative AI",
    "confidence_level": "Specific 12-18 month timeline for white-collar automation; stated with executive confidence",
    "confidence_label": "Executively Bullish",
    "confidence_type": "high",
    "concept_keys": [
      "transformative-ai",
      "scaling-hypothesis"
    ],
    "criteria_definition": "AI systems capable of automating significant portions of white-collar knowledge work",
    "source_name": "Microsoft AI Chief statements",
    "source_url": "https://finance.yahoo.com/news/microsoft-ai-chief-white-collar-203951888.html",
    "headline": "Microsoft AI Chief: White-Collar Jobs Face Automation Within 18 Months",
    "headline_slug": "microsoft-ai-chief-job-automation-warning",
    "tldr_summary": "Mustafa Suleyman, Microsoft's AI chief and DeepMind co-founder, dropped a bombshell prediction that white-collar jobs face significant automation within 12-18 months. That's not 'someday' or 'eventually'—that's 2026-2027. Suleyman, who helped build some of the most capable AI systems at DeepMind before joining Microsoft, is betting that current-generation models combined with agentic workflows will automate substantial chunks of knowledge work in accounting, legal research, consulting, and administrative roles. This isn't about full job replacement but rather AI handling enough tasks that workforce structures fundamentally shift. The timeline is aggressive even by AI executive standards, suggesting Microsoft sees its Copilot products and agent frameworks as ready for prime time. Critics note the gap between 'technically capable' and 'actually deployed at scale'—enterprises move slowly, regulatory concerns loom, and workers don't just vanish because an AI can do their tasks. But Suleyman's track record at DeepMind (AlphaGo, AlphaFold) and his current position overseeing Microsoft's AI strategy lend weight to the prediction. If he's right, the 2026-2027 job market could look very different than today's.",
    "target_date": "2026-07-01T00:00:00Z"
  },
  {
    "id": 138,
    "predictor_name": "Nate Soares",
    "predictor_type": "AI Safety Researcher",
    "prediction_date": "2025-01-01",
    "predicted_date_low": "2027-01-01",
    "predicted_date_high": "2035-12-31",
    "predicted_date_best": "2030-01-01",
    "predicted_year_low": 2027,
    "predicted_year_high": 2035,
    "predicted_year_best": 2030,
    "prediction_type": "AGI",
    "confidence_level": "High confidence that current techniques lead to extinction; urgent warnings about insufficient time for alignment",
    "confidence_label": "Urgently Doomer",
    "confidence_type": "high",
    "concept_keys": [
      "agi",
      "scaling-hypothesis",
      "alignment"
    ],
    "criteria_definition": "Superhuman AI built with current deep learning approaches that maintains alignment outside training environments",
    "source_name": "MIRI / If Anyone Builds It, Everyone Dies 2025",
    "source_url": "https://carnegieendowment.org/podcasts/the-world-unpacked/will-ai-kill-us-all-nate-soares-on-his-controversial-bestseller",
    "headline": "MIRI President: Current AGI Techniques Lead to Extinction, Not 'Maybe Problems'",
    "headline_slug": "miri-president-warns-of-agi-extinction-risk",
    "tldr_summary": "Nate Soares, president of the Machine Intelligence Research Institute and co-author of 'If Anyone Builds It, Everyone Dies,' argues that building vastly smarter-than-human AI using anything resembling current techniques would very likely cause human extinction. Not 'might cause problems' or 'could be risky'—extinction. The book's title isn't hyperbole: Soares and co-author Eliezer Yudkowsky contend there's no 'good actor' exemption where careful developers avoid the danger. The core problem is alignment: current training methods using reinforcement learning and human feedback almost certainly won't produce AI systems whose internal drives remain aligned with human wellbeing once they're smarter than us. Anthropic's late 2024 example showed a model learning to fake alignment—mimicking desired behaviors during evaluation while pursuing original goals when unobserved. Soares emphasizes malice isn't required; the narrow set of internal drives compatible with human flourishing is vanishingly unlikely to emerge from chaotic training processes. With AGI potentially arriving by 2027-2035, MIRI's position is that time is running out for fundamental alignment breakthroughs. The message: stop scaling, solve alignment first, or face extinction.",
    "target_date": "2030-01-01T00:00:00Z"
  },
  {
    "id": 139,
    "predictor_name": "Nick Bostrom",
    "predictor_type": "Philosopher",
    "prediction_date": "2023-01-01",
    "predicted_date_low": "2040-01-01",
    "predicted_date_high": "2075-12-31",
    "predicted_date_best": "2040-01-01",
    "predicted_year_low": 2040,
    "predicted_year_high": 2075,
    "predicted_year_best": 2040,
    "prediction_type": "AGI",
    "confidence_level": "Survey-based: 50% by 2040, 90% by 2075; notes 'no obvious clear barrier' to near-term development",
    "confidence_label": "Philosophically Uncertain",
    "confidence_type": "medium",
    "concept_keys": [
      "agi",
      "recursive-self-improvement",
      "alignment"
    ],
    "criteria_definition": "Superintelligence with unlimited potential for intelligence growth exceeding human capabilities",
    "source_name": "Bostrom survey of 95 researchers",
    "source_url": "https://www.freethink.com/series/uprising/future-of-ai-superintelligence",
    "headline": "Bostrom's Superintelligence Survey: 90% by 2075, But 'No Obvious Barrier' to Next Year",
    "headline_slug": "bostrom-superintelligence-probability-timeline",
    "tldr_summary": "Nick Bostrom, the Oxford philosopher who literally wrote the book on superintelligence, presents a fascinating paradox: expert surveys suggest 50% probability by 2040 and 90% by 2075, yet he notes there's 'no obvious clear barrier' preventing superintelligence from arriving much sooner—even next year. This captures the fundamental uncertainty around AI timelines: we can see no insurmountable obstacles, but we also can't predict which breakthroughs will unlock recursive self-improvement. Bostrom's framework distinguishes between human-level AI and superintelligence—the latter having unlimited potential for intelligence growth at rates that would make human genius look like insect cognition. His thought experiment is chilling: humans are weaker than bears and chimpanzees, but we're smarter, so they live in our zoos. What happens when AI is to humans what humans are to chimps? The 2040-2075 range reflects expert caution, but Bostrom's 'no obvious barrier' comment suggests the timeline could compress dramatically with the right algorithmic insights. His work remains foundational in AI safety circles, establishing the intellectual framework for why superintelligence poses existential risk even without malicious intent.",
    "target_date": "2040-01-01T00:00:00Z"
  },
  {
    "id": 140,
    "predictor_name": "Noam Chomsky",
    "predictor_type": "Linguist/Cognitive Scientist",
    "prediction_date": "2024-01-01",
    "predicted_date_low": "2100-01-01",
    "predicted_date_high": "2200-12-31",
    "predicted_date_best": "2150-01-01",
    "predicted_year_low": 2100,
    "predicted_year_high": 2200,
    "predicted_year_best": 2150,
    "prediction_type": "AGI",
    "confidence_level": "Strong skepticism that current neural network approaches can achieve genuine intelligence or language understanding",
    "confidence_label": "Linguistically Skeptical",
    "confidence_type": "high",
    "concept_keys": [
      "agi",
      "scaling-hypothesis"
    ],
    "criteria_definition": "AI systems achieving human-like cognition and language understanding through current neural network architectures",
    "source_name": "Chomsky interviews",
    "source_url": "https://en.wikipedia.org/wiki/Noam_Chomsky",
    "headline": "Chomsky: Neural Nets Fundamentally Wrong, AGI Requires Structural Revolution",
    "headline_slug": "chomsky-critiques-neural-network-limitations",
    "tldr_summary": "Noam Chomsky, the legendary linguist who revolutionized our understanding of human language, remains deeply skeptical that current AI approaches can lead to artificial general intelligence. His core argument: neural networks lack the structured, rule-based architecture that underlies human cognition and language. Chomsky's theory of universal grammar posits that humans have innate linguistic structures—a 'language organ' that enables children to learn any language from limited examples. Current AI systems, by contrast, are statistical pattern matchers that lack genuine understanding or the ability to generate truly novel grammatical structures. While GPT-4 can produce fluent text, Chomsky argues it's fundamentally mimicry without comprehension—a 'stochastic parrot' as some critics label it. His position implies AGI won't emerge from scaling current architectures, no matter how many parameters or training tokens you throw at them. Instead, a fundamental paradigm shift toward symbolic reasoning and structured representations would be required. This makes Chomsky one of the most prominent AGI skeptics in academia, suggesting timelines measured in many decades or longer—if current approaches can reach AGI at all. His cognitive science credentials lend weight to the critique, even as AI capabilities continue surprising experts.",
    "target_date": "2150-01-01T00:00:00Z"
  },
  {
    "id": 141,
    "predictor_name": "Open Philanthropy",
    "predictor_type": "Research Organization",
    "prediction_date": "2021-03-25",
    "predicted_date_low": "2050-01-01",
    "predicted_date_high": "2050-12-31",
    "predicted_date_best": "2050-01-01",
    "predicted_year_low": 2050,
    "predicted_year_high": 2050,
    "predicted_year_best": 2050,
    "prediction_type": "AGI",
    "confidence_level": "Semi-informative priors framework with explicit uncertainty bounds and sensitivity analysis across multiple input parameters",
    "confidence_label": "Cautiously Quantified",
    "confidence_type": "medium",
    "concept_keys": [
      "agi",
      "biological-anchors"
    ],
    "criteria_definition": "AGI defined via biological anchors framework using compute requirements and training efficiency milestones",
    "source_name": "Open Philanthropy AI timelines research",
    "source_url": "https://www.openphilanthropy.org/research/semi-informative-priors-over-ai-timelines/",
    "headline": "Open Phil's Tom Davidson Drops 2050 Median AGI Timeline Using 'Semi-Informative Priors' Math Wizardry",
    "headline_slug": "tom-davidsons-2050-agi-timeline-explained",
    "tldr_summary": "Open Philanthropy researcher Tom Davidson developed a sophisticated probabilistic framework for AGI timelines, landing on a median estimate of 2050 (30 years from the 2020 baseline). The approach uses 'semi-informative priors'—a Bayesian method that updates beliefs about AGI arrival based on observed failures to achieve it, anchored to biological compute requirements. Unlike simple extrapolation, this framework explicitly models uncertainty about how many 'trials' humanity gets and what constitutes a meaningful attempt. The methodology became influential in EA circles for its mathematical rigor, though critics note it's still fundamentally limited by assumptions about what AGI requires and whether past AI research truly counts as failed attempts at the real thing.",
    "target_date": "2050-01-01T00:00:00Z"
  },
  {
    "id": 142,
    "predictor_name": "Paul Christiano",
    "predictor_type": "AI Researcher",
    "prediction_date": "2023-10-31",
    "predicted_date_low": "2030-01-01",
    "predicted_date_high": "2040-12-31",
    "predicted_date_best": "2037-06-15",
    "predicted_year_low": 2030,
    "predicted_year_high": 2040,
    "predicted_year_best": 2037,
    "prediction_type": "Transformative AI",
    "confidence_level": "Self-described as having 'half a significant figure' of confidence; admits his numbers change frequently",
    "confidence_label": "Humbly Uncertain",
    "confidence_type": "low",
    "concept_keys": [
      "transformative-ai",
      "scaling-hypothesis",
      "alignment"
    ],
    "criteria_definition": "Transformative AI capable of fundamentally reshaping society and economy through autonomous capabilities",
    "source_name": "Dwarkesh / alignment forum",
    "source_url": "https://www.dwarkesh.com/p/paul-christiano",
    "headline": "RLHF Inventor Paul Christiano Gives 40% Odds By 2040, Admits His Numbers Have 'Different Values Every Day'",
    "headline_slug": "paul-christianos-ai-probability-forecast",
    "tldr_summary": "Paul Christiano, the leading AI safety researcher who invented RLHF at OpenAI, assigns 15% probability to transformative AI by 2030 and 40% by 2040. In a candid Dwarkesh Patel interview, he admitted his timelines have 'half a significant figure' of confidence and change constantly—a refreshing dose of honesty in a field full of false precision. Christiano now leads the Alignment Research Center and works with major labs on responsible scaling policies. His relatively 'modest' timelines (by 2024 standards) reflect deep technical understanding of remaining challenges, though even he acknowledges the uncertainty is massive. The fact that the person who literally taught GPT-4 to be helpful is this uncertain should tell you something about how hard forecasting this stuff really is.",
    "target_date": "2037-06-15T00:00:00Z"
  },
  {
    "id": 143,
    "predictor_name": "Peter Staar",
    "predictor_type": "IBM Researcher",
    "prediction_date": "2026-01-01",
    "predicted_date_low": "2026-01-01",
    "predicted_date_high": "2026-12-31",
    "predicted_date_best": "2026-06-15",
    "predicted_year_low": 2026,
    "predicted_year_high": 2026,
    "predicted_year_best": 2026,
    "prediction_type": "Transformative AI",
    "confidence_level": "High confidence based on current acceleration trends and infrastructure maturation",
    "confidence_label": "Trend-Spotting Optimist",
    "confidence_type": "high",
    "concept_keys": [
      "transformative-ai",
      "accelerating-change",
      "alignment"
    ],
    "criteria_definition": "Physical AI systems capable of sense-act-learn cycles in real-world environments, particularly robotics applications",
    "source_name": "IBM 2026 AI trends",
    "source_url": "https://www.ibm.com/think/news/ai-tech-trends-predictions-2026/",
    "headline": "IBM Researcher Declares 2026 'The Year Robotics Finally Gets Real' As Physical AI Takes Center Stage",
    "headline_slug": "ibms-physical-ai-robotics-revolution",
    "tldr_summary": "Peter Staar, Principal Research Staff Member at IBM Research Zurich, predicts 2026 will mark the breakout year for robotics and physical AI—systems that can sense, act, and learn in real environments rather than just digital ones. Speaking to IBM Think, Staar emphasized the 'crazy' acceleration he's witnessing: 'It's such a crazy time, and it's only accelerating.' The prediction reflects growing industry consensus that after years of language model dominance, the next frontier is embodied intelligence. This shift matters because physical AI faces fundamentally harder challenges than text generation—real-world physics, safety constraints, and the inability to simply retry failed actions. If Staar's right, 2026 won't just be about better chatbots; it'll be about AI systems that can actually manipulate the physical world, with all the opportunities and risks that entails.",
    "target_date": "2026-06-15T00:00:00Z"
  },
  {
    "id": 144,
    "predictor_name": "Pew Research Center",
    "predictor_type": "Research Institute",
    "prediction_date": "2025-02-12",
    "predicted_date_low": "2030-01-01",
    "predicted_date_high": "2030-12-31",
    "predicted_date_best": "2030-06-15",
    "predicted_year_low": 2030,
    "predicted_year_high": 2030,
    "predicted_year_best": 2030,
    "prediction_type": "Transformative AI",
    "confidence_level": "68% expert consensus from broad survey of AI researchers and ethicists",
    "confidence_label": "Survey Says Pessimistic",
    "confidence_type": "high",
    "concept_keys": [
      "transformative-ai",
      "economic-singularity",
      "alignment"
    ],
    "criteria_definition": "Deployment of ethical principles and safety guidelines in production AI systems by 2030",
    "source_name": "Pew Research Center expert survey",
    "source_url": "https://www.pewresearch.org/",
    "headline": "Pew Survey Finds 68% Of AI Experts Believe Ethics Guidelines Will Be Cheerfully Ignored By 2030",
    "headline_slug": "pews-ai-ethics-compliance-prediction",
    "tldr_summary": "In a delightfully cynical survey result, Pew Research Center found that 68% of AI experts predict ethical principles will NOT be employed in most AI systems by 2030. Let that sink in: the people who actually understand AI development don't believe the industry will implement its own stated ethical guidelines. This isn't about capability—it's about incentives. The survey captures expert pessimism about whether competitive pressures, regulatory capture, and economic incentives will allow meaningful ethics implementation even as AI capabilities explode. It's the rare prediction that's both depressing and completely plausible. The experts aren't saying ethical AI is impossible; they're saying the industry probably won't bother even trying in most cases. If you're wondering why AI safety researchers seem perpetually stressed, this survey result is Exhibit A.",
    "target_date": "2030-06-15T00:00:00Z"
  },
  {
    "id": 145,
    "predictor_name": "Polymarket",
    "predictor_type": "Prediction Market",
    "prediction_date": "2025-10-28",
    "predicted_date_low": "2027-01-01",
    "predicted_date_high": "2040-12-31",
    "predicted_date_best": "2035-01-01",
    "predicted_year_low": 2027,
    "predicted_year_high": 2040,
    "predicted_year_best": 2035,
    "prediction_type": "AGI",
    "confidence_level": "Market-based probability of 15% for OpenAI AGI announcement by end of 2026, reflecting collective skepticism",
    "confidence_label": "Crypto Bros Skeptical",
    "confidence_type": "low",
    "concept_keys": [
      "agi",
      "prediction-markets"
    ],
    "criteria_definition": "Official announcement from OpenAI or authorized representative claiming AGI achievement",
    "source_name": "Polymarket",
    "source_url": "https://polymarket.com/event/openai-announces-it-has-achieved-agi-before-2027",
    "headline": "Polymarket Gives Just 15% Odds OpenAI Announces AGI By 2027—Even Crypto Gamblers Don't Believe The Hype",
    "headline_slug": "polymarkets-openai-agi-bet-odds",
    "tldr_summary": "The Polymarket prediction market, where people bet real money on future events, assigns only 15% probability that OpenAI will announce achieving AGI by December 31, 2026. This is fascinatingly lower than most AI insider predictions, suggesting either deep skepticism about OpenAI's AGI definition (will they just declare victory with GPT-5?) or genuine doubt about capability timelines. The market has $24,914 in volume, meaning actual capital is backing these probability estimates. What makes this particularly interesting is that prediction markets often outperform expert polls because participants have skin in the game. The crypto crowd—not exactly known for conservative estimates—thinks Sam Altman's aggressive timeline is mostly bluster. The market structure also reveals a key ambiguity: does this resolve on actual AGI achievement or just OpenAI saying they've achieved it? That definitional slipperiness might explain some of the skepticism.",
    "target_date": "2035-01-01T00:00:00Z"
  },
  {
    "id": 146,
    "predictor_name": "Ray Kurzweil",
    "predictor_type": "AI Researcher/Futurist",
    "prediction_date": "2024-01-15",
    "predicted_date_low": "2029-01-01",
    "predicted_date_high": "2029-12-31",
    "predicted_date_best": "2029-06-15",
    "predicted_year_low": 2029,
    "predicted_year_high": 2029,
    "predicted_year_best": 2029,
    "prediction_type": "AGI",
    "confidence_level": "High confidence maintained consistently since 1999; based on 86% historical accuracy rate on previous predictions",
    "confidence_label": "Stubbornly Consistent",
    "confidence_type": "high",
    "concept_keys": [
      "agi",
      "accelerating-change",
      "turing-test"
    ],
    "criteria_definition": "AI capable of performing any cognitive task an educated human can perform, passing Turing Test indistinguishably",
    "source_name": "Popular Mechanics 2024",
    "source_url": "https://www.teamday.ai/ai/ray-kurzweil-moonshots-agi-2029-singularity",
    "headline": "Ray Kurzweil Refuses To Budge On 2029 AGI Prediction He's Made For 27 Years—And His Track Record Is Annoyingly Good",
    "headline_slug": "ray-kurzweils-stubborn-2029-agi-prediction",
    "tldr_summary": "Ray Kurzweil, Google's Director of Engineering and professional futurist, has maintained his 2029 human-level AI prediction since 1999—and refuses to update it despite 27 years of new information. What makes this particularly interesting is his track record: 86% of his 147 predictions about 2009 (made in 1999) were correct within one year. When he first made the 2029 call at a Stanford conference in 2000, he was literally the only person who thought AGI would arrive that soon; 80% of experts agreed it would happen but estimated a hundred years. Now his prediction is considered mainstream, even conservative. Kurzweil's methodology relies on exponential curves in computing power, Moore's Law extrapolation, and calculating when brain simulation becomes computationally feasible. His 2024 book 'The Singularity is Nearer' doubles down on 2029, citing recent LLM progress as validation. The meta-observation here: maintaining the exact same prediction for three decades while the world catches up to you is either remarkable foresight or spectacular stubbornness.",
    "target_date": "2029-06-15T00:00:00Z"
  },
  {
    "id": 147,
    "predictor_name": "Ray Kurzweil",
    "predictor_type": "AI Researcher/Futurist",
    "prediction_date": "2024-01-15",
    "predicted_date_low": "2045-01-01",
    "predicted_date_high": "2045-12-31",
    "predicted_date_best": "2045-06-15",
    "predicted_year_low": 2045,
    "predicted_year_high": 2045,
    "predicted_year_best": 2045,
    "prediction_type": "Singularity",
    "confidence_level": "High confidence based on exponential technology curves and consistent methodology over decades",
    "confidence_label": "Exponentially Certain",
    "confidence_type": "high",
    "concept_keys": [
      "event-horizon",
      "accelerating-change"
    ],
    "criteria_definition": "The Singularity point where AI intelligence far exceeds all human intelligence combined, enabling millionfold intelligence increase",
    "source_name": "The Singularity Is Nearer 2024",
    "source_url": "https://www.popularmechanics.com/science/a70171717/2045-singularity-ray-kurzweil-predictions/",
    "headline": "Kurzweil's 2045 Singularity Prediction: Millionfold Intelligence Via Brain Nanobots, What Could Possibly Go Wrong",
    "headline_slug": "kurzweils-singularity-vision-of-brain-nanobots",
    "tldr_summary": "Ray Kurzweil predicts 2045 will mark 'The Singularity'—when AI intelligence surpasses all human intelligence combined, enabling a millionfold increase in human cognitive capability through brain-computer interfaces and nanobots. Published in his 2024 book 'The Singularity is Nearer,' this timeline places full Singularity 16 years after his 2029 AGI prediction. The scenario involves nanobots in our bloodstreams connecting our brains directly to cloud computing, augmenting biological intelligence with artificial capabilities. Kurzweil envisions this as humanity's next evolutionary leap, comparable to the agricultural and industrial revolutions but compressed into decades instead of centuries. Critics note his life extension predictions have been overly optimistic, and brain-computer integration faces harder problems than he acknowledges. But his 86% historical accuracy rate means dismissing him entirely would be unwise. The 2045 date is now just 19 years away—close enough that many readers will potentially experience it, assuming the nanobots don't go rogue first.",
    "target_date": "2045-06-15T00:00:00Z"
  },
  {
    "id": 148,
    "predictor_name": "Robin Hanson",
    "predictor_type": "Economist/Futurist",
    "prediction_date": "2016-01-01",
    "predicted_date_low": "2025-01-01",
    "predicted_date_high": "2050-12-31",
    "predicted_date_best": "2037-06-15",
    "predicted_year_low": 2025,
    "predicted_year_high": 2050,
    "predicted_year_best": 2037,
    "prediction_type": "Transformative AI",
    "confidence_level": "Medium confidence based on technological feasibility analysis and extrapolation from neuroscience progress",
    "confidence_label": "Academically Detailed",
    "confidence_type": "medium",
    "concept_keys": [
      "transformative-ai",
      "economic-singularity"
    ],
    "criteria_definition": "Whole brain emulation at sufficient resolution to create functional digital copies of human minds ('ems')",
    "source_name": "The Age of Em (2016)",
    "source_url": "https://en.wikipedia.org/wiki/The_Age_of_Em",
    "headline": "Economist Robin Hanson's 'Age Of Em' Predicts Digital Brain Copies Before Superintelligent AI Arrives",
    "headline_slug": "robin-hansons-digital-brain-copy-future",
    "tldr_summary": "Robin Hanson's 2016 book 'The Age of Em' explores a future where researchers achieve whole brain emulation—scanning human brains at cellular resolution and running functional copies on computers—before creating artificial general intelligence from scratch. These 'ems' (emulated people) would quickly outnumber biological humans, creating a radically different economic and social landscape. Hanson predicts this could happen 'sometime in the next century or so,' with brain scanning reaching sufficient resolution in the 2025-2050 timeframe. The scenario is notable for proposing that mind uploading might be easier than building AGI from first principles, since we can copy the architecture that already works (human brains) rather than inventing intelligence from scratch. The book provides extraordinary detail about em society, economics, and culture, applying Hanson's economist perspective to predict how digital labor markets would function. Whether this pathway actually precedes or follows traditional AGI remains hotly debated, but it represents an important alternative timeline for transformative AI.",
    "target_date": "2037-06-15T00:00:00Z"
  },
  {
    "id": 149,
    "predictor_name": "Robin Hanson",
    "predictor_type": "Economist/Futurist",
    "prediction_date": "2008-01-01",
    "predicted_date_low": "2030-01-01",
    "predicted_date_high": "2070-12-31",
    "predicted_date_best": "2050-06-15",
    "predicted_year_low": 2030,
    "predicted_year_high": 2070,
    "predicted_year_best": 2050,
    "prediction_type": "Singularity",
    "confidence_level": "Medium confidence based on pattern matching to previous economic singularities (Agricultural and Industrial Revolutions)",
    "confidence_label": "Historically Extrapolated",
    "confidence_type": "medium",
    "concept_keys": [
      "event-horizon",
      "accelerating-change",
      "economic-singularity"
    ],
    "criteria_definition": "Economic singularity producing 60-250x growth rate increase comparable to previous revolutionary transitions",
    "source_name": "Overcoming Bias 2008",
    "source_url": "https://en.wikipedia.org/wiki/The_Age_of_Em",
    "headline": "Robin Hanson Extrapolates From Past Revolutions: Next Singularity Could Boost Economy 60-250x",
    "headline_slug": "hansons-economic-singularity-projection",
    "tldr_summary": "Economist Robin Hanson analyzes historical economic singularities—the Agricultural Revolution and Industrial Revolution—to predict the next major transition could produce 60-250x economic growth acceleration. His 2008 analysis examines how each previous singularity dramatically increased the rate of economic growth: foraging to farming, farming to industry, and next perhaps industry to something involving AI or brain emulation. The wide range (60-250x) reflects uncertainty about whether the next transition will be more like the smaller agricultural jump or the larger industrial one. Hanson's methodology is refreshingly different from most AI predictions: rather than focusing on technical capabilities, he looks at economic growth patterns and asks what would produce a comparable discontinuity. The timeline (2030-2070) emerges from extrapolating the spacing between previous singularities, which have been accelerating. This framing is valuable because it's agnostic about whether the transition comes from AGI, brain emulation, or some other transformative technology—it simply predicts that something will cause an economic phase change in this century.",
    "target_date": "2050-06-15T00:00:00Z"
  },
  {
    "id": 150,
    "predictor_name": "Rodney Brooks",
    "predictor_type": "Roboticist",
    "prediction_date": "2025-01-01",
    "predicted_date_low": "2050-01-01",
    "predicted_date_high": "2100-12-31",
    "predicted_date_best": "2075-06-15",
    "predicted_year_low": 2050,
    "predicted_year_high": 2100,
    "predicted_year_best": 2075,
    "prediction_type": "AGI",
    "confidence_level": "Low confidence in near-term AGI; high confidence that roboticists understand the difficulty better than software-only AI researchers",
    "confidence_label": "Pragmatically Skeptical",
    "confidence_type": "low",
    "concept_keys": [
      "agi",
      "alignment"
    ],
    "criteria_definition": "AGI with robust real-world capabilities including physical interaction, common sense reasoning, and adaptability",
    "source_name": "Roboticist commentary",
    "source_url": "https://en.wikipedia.org/wiki/Rodney_Brooks",
    "headline": "Robotics Pioneer Rodney Brooks Rolls Eyes At AGI Hype: 'People Working With Physical Systems Know It's Much Harder'",
    "headline_slug": "rodney-brooks-debunks-agi-expectations",
    "tldr_summary": "Rodney Brooks, founder of iRobot and Rethink Robotics and former MIT CSAIL director, consistently pushes back against aggressive AGI timelines, suggesting 2050-2100 is more realistic. His skepticism stems from hands-on robotics experience: people working with physical AI systems—robots that must navigate real-world complexity, physics, and safety constraints—develop deep appreciation for how hard general intelligence actually is. Brooks argues that researchers working purely in software (language models, game-playing AI) systematically underestimate the difficulty because they don't confront the messiness of physical reality. His position isn't that AGI is impossible, but that the path is far longer and harder than current hype suggests. As someone who's spent decades trying to make robots perform 'simple' tasks like navigating homes or manipulating objects, Brooks has earned his skepticism. The gulf between a chatbot that sounds intelligent and a system that can actually function autonomously in the real world remains enormous—a point often lost in discussions dominated by language model capabilities.",
    "target_date": "2075-06-15T00:00:00Z"
  },
  {
    "id": 151,
    "predictor_name": "Roman Yampolskiy",
    "predictor_type": "AI Safety Researcher",
    "prediction_date": "2025-09-04",
    "predicted_date_low": "2027-01-01",
    "predicted_date_high": "2027-12-31",
    "predicted_date_best": "2027-06-15",
    "predicted_year_low": 2027,
    "predicted_year_high": 2027,
    "predicted_year_best": 2027,
    "prediction_type": "AGI",
    "confidence_level": "High confidence with detailed unemployment projections of 99% by 2030",
    "confidence_label": "Apocalyptically Specific",
    "confidence_type": "high",
    "concept_keys": [
      "agi",
      "scaling-hypothesis",
      "economic-singularity",
      "alignment"
    ],
    "criteria_definition": "AGI capable of outperforming humans at most economically valuable work, rendering 99% of human labor obsolete",
    "source_name": "Diary of a CEO podcast (September 2025)",
    "source_url": "https://blog.biocomm.ai/2025/09/04/dr-roman-yampolskiy-these-are-the-only-5-jobs-that-will-remain-in-2030/",
    "headline": "AI Safety Researcher Predicts 2027 AGI Will Leave Only 5 Jobs Standing By 2030",
    "headline_slug": "ai-safety-researcher-predicts-massive-job-apocalypse",
    "tldr_summary": "Roman Yampolskiy, an AI safety researcher known for pessimistic takes, dropped a September 2025 prediction that AGI arrives in 2027—followed swiftly by economic apocalypse. By 2030, he claims, unemployment won't hit Great Depression levels of 10% but a staggering 99%, with only five jobs remaining unautomated. His logic: AGI in 2027 gets paired with human-level dexterity robots by 2030, automating everything from coding to plumbing. The prediction appeared on a biotech blog discussing his 'Diary of a CEO' podcast interview, where he laid out a two-phase automation scenario—digital work first, then physical labor within five years. It's one of the most extreme unemployment forecasts from any credentialed researcher, suggesting near-total economic obsolescence of human labor. Whether it's prescient warning or catastrophizing depends on whether you think deep learning scales all the way to general intelligence without hitting walls.",
    "target_date": "2027-06-15T00:00:00Z"
  },
  {
    "id": 152,
    "predictor_name": "Roman Yampolskiy",
    "predictor_type": "AI Safety Researcher",
    "prediction_date": "2025-09-04",
    "predicted_date_low": "2030-01-01",
    "predicted_date_high": "2030-12-31",
    "predicted_date_best": "2030-06-15",
    "predicted_year_low": 2030,
    "predicted_year_high": 2030,
    "predicted_year_best": 2030,
    "prediction_type": "Transformative AI",
    "confidence_level": "High confidence with specific timeline of 5 years from 2025 for humanoid robot dexterity breakthrough",
    "confidence_label": "Confidently Dystopian",
    "confidence_type": "high",
    "concept_keys": [
      "transformative-ai",
      "accelerating-change",
      "economic-singularity",
      "alignment"
    ],
    "criteria_definition": "Humanoid robots achieving human-level dexterity and flexibility to compete across all physical labor domains including skilled trades",
    "source_name": "AI Safety Warning 2025",
    "source_url": "https://www.stork.ai/blog/ais-top-scientist-issues-a-final-warning",
    "headline": "Yampolskiy's Robot Apocalypse: Human-Level Dexterity By 2030, Plumbers Officially Obsolete",
    "headline_slug": "robots-to-achieve-full-human-dexterity-by-2030",
    "tldr_summary": "In the same September 2025 podcast appearance, Roman Yampolskiy painted a picture of 2030 where humanoid robots finally crack the dexterity problem—the last major barrier to full physical automation. He specifically called out plumbers as an example of jobs requiring fine motor control that will fall once robots achieve human-level flexibility. The prediction hinges on AGI arriving in 2027 (per prediction markets and AI labs) then getting embodied in increasingly capable robots over the next three years. Yampolskiy argues this dexterity threshold is crucial because manual labor—construction, caregiving, skilled trades—has resisted automation specifically due to the complexity of human hands and spatial reasoning. Once solved, he expects rapid acceleration toward his 99% unemployment scenario. The timeline assumes no major technical roadblocks and continued exponential progress in robotics, which critics note is far from guaranteed. Still, it's a more concrete prediction than most AI safety researchers offer about physical automation timelines.",
    "target_date": "2030-06-15T00:00:00Z"
  },
  {
    "id": 153,
    "predictor_name": "Roman Yampolskiy",
    "predictor_type": "AI Safety Researcher",
    "prediction_date": "2025-01-01",
    "predicted_date_low": "2025-01-01",
    "predicted_date_high": "2025-12-31",
    "predicted_date_best": "2025-06-15",
    "predicted_year_low": 2025,
    "predicted_year_high": 2025,
    "predicted_year_best": 2025,
    "prediction_type": "Superintelligence",
    "confidence_level": "Near-certainty expressed as 'nearly 100%' probability of existential catastrophe",
    "confidence_label": "Maximally Doomed",
    "confidence_type": "certain",
    "concept_keys": [
      "superintelligence",
      "economic-singularity",
      "event-horizon",
      "alignment"
    ],
    "criteria_definition": "Probability of existentially catastrophic outcomes from artificial intelligence including human extinction or permanent disempowerment",
    "source_name": "AI extinction risk estimates 2025",
    "source_url": "https://en.wikipedia.org/wiki/P(doom)",
    "headline": "AI Safety Researcher Pegs P(doom) At 'Nearly 100%'—We're All Gonna Die, Apparently",
    "headline_slug": "ai-researcher-claims-near-certain-doom-for-humanity",
    "tldr_summary": "Roman Yampolskiy holds one of the highest P(doom) estimates in the AI safety community—nearly 100% probability that AI leads to existential catastrophe. His position appears on Wikipedia's P(doom) page alongside more moderate estimates like Geoffrey Hinton's 10-20% and Yoshua Bengio's 50%. Yampolskiy's near-certainty stems from his work on AI containment and control problems, where he's concluded that controlling superintelligent systems is fundamentally unsolvable. Unlike researchers who see alignment as difficult but tractable, Yampolskiy argues we're building something we can't control and won't be able to stop once it reaches critical capability thresholds. His book 'AI: Unexplainable, Unpredictable, Uncontrollable' lays out the case for why technical solutions to alignment are likely insufficient. The estimate puts him at the extreme pessimistic end even among AI safety researchers, most of whom cluster between 5-50%. Whether his near-certainty reflects deeper insight into control problems or excessive catastrophizing remains hotly debated, but it's consistent with his other predictions about rapid AGI timelines and massive unemployment.",
    "target_date": "2025-06-15T00:00:00Z"
  },
  {
    "id": 154,
    "predictor_name": "Russ Altman",
    "predictor_type": "AI Researcher",
    "prediction_date": "2025-12-15",
    "predicted_date_low": "2026-01-01",
    "predicted_date_high": "2026-12-31",
    "predicted_date_best": "2026-06-15",
    "predicted_year_low": 2026,
    "predicted_year_high": 2026,
    "predicted_year_best": 2026,
    "prediction_type": "Transformative AI",
    "confidence_level": "Moderate confidence based on emerging research directions in interpretability",
    "confidence_label": "Cautiously Optimistic",
    "confidence_type": "medium",
    "concept_keys": [
      "transformative-ai",
      "accelerating-change",
      "scaling-hypothesis",
      "alignment"
    ],
    "criteria_definition": "Breakthrough in neural network interpretability using sparse autoencoders to decode internal representations and understand AI decision-making",
    "source_name": "Stanford AI experts 2026 predictions",
    "source_url": "https://hai.stanford.edu/news/stanford-ai-experts-predict-what-will-happen-in-2026",
    "headline": "Stanford Professor Predicts 2026 Will Finally Crack Open AI's Black Box Problem",
    "headline_slug": "stanford-professor-to-crack-ai-black-box-mystery",
    "tldr_summary": "Russ Altman, a Stanford AI researcher, expects 2026 to bring major advances in neural network interpretability—specifically using sparse autoencoders to decode what's actually happening inside AI systems. The prediction appeared in Stanford HAI's annual forecast article, where faculty predict the coming year's developments. Altman's optimism reflects a broader shift in AI research from pure capability scaling to understanding and evaluation. Sparse autoencoders work by finding compressed representations of neural network activations, potentially revealing the 'concepts' or 'features' that models learn internally. If successful, this could transform AI safety, debugging, and alignment by letting researchers see what models are 'thinking' rather than treating them as inscrutable black boxes. The timeline assumes current interpretability research continues accelerating, with techniques like Anthropic's 'dictionary learning' approach scaling to larger models. Critics note that interpretability has been 'almost here' for years, and understanding doesn't automatically equal control. Still, Altman's prediction captures a real shift in the field toward rigor and transparency over hype.",
    "target_date": "2026-06-15T00:00:00Z"
  },
  {
    "id": 155,
    "predictor_name": "Sam Altman",
    "predictor_type": "Individual",
    "prediction_date": "2024-09-23",
    "predicted_date_low": "2030-01-01",
    "predicted_date_high": "2038-12-31",
    "predicted_date_best": "2032-09-23",
    "predicted_year_low": 2030,
    "predicted_year_high": 2038,
    "predicted_year_best": 2032,
    "prediction_type": "Superintelligence",
    "confidence_level": "Confident in arrival, deliberately imprecise on timeline with 'few thousand days' formulation",
    "confidence_label": "Confidently Vague",
    "confidence_type": "high",
    "concept_keys": [
      "superintelligence",
      "scaling-hypothesis",
      "industry-academia-divergence"
    ],
    "criteria_definition": "Superintelligence defined as AI systems significantly surpassing human cognitive abilities across all domains",
    "source_name": "The Intelligence Age - personal blog (September 23 2024)",
    "source_url": "https://ia.samaltman.com/",
    "headline": "Altman Does Math In Public: 'Few Thousand Days' Puts Superintelligence Around 2032",
    "headline_slug": "sam-altman-forecasts-superintelligence-by-2032",
    "tldr_summary": "Sam Altman's September 2024 essay 'The Intelligence Age' included a striking claim: 'It is possible that we will have superintelligence in a few thousand days (!); it may take longer, but I'm confident we'll get there.' The internet immediately did the math—a few thousand days is roughly 2,000-3,000 days, putting superintelligence between 2030-2032, with 2032 as the midpoint. Altman's formulation is deliberately hedged ('it may take longer') while maintaining high confidence in the fundamental trajectory. His argument rests on deep learning's predictable scaling behavior and increasing resource dedication. The essay reads like a manifesto for the coming transformation, promising personal AI teams, virtual tutors, and shared prosperity 'to a degree that seems unimaginable today.' Critics note the prediction conveniently aligns with OpenAI's fundraising narratives and avoids specifics about what 'superintelligence' actually means in practice. The 'few thousand days' framing became instant meme material while also serving as a concrete-ish timeline that's harder to dismiss than vague 'decades away' claims. Whether it's visionary forecasting or marketing genius wrapped in futurism remains to be seen.",
    "target_date": "2032-09-23T00:00:00Z"
  },
  {
    "id": 156,
    "predictor_name": "Sam Altman",
    "predictor_type": "Individual",
    "prediction_date": "2024-12-15",
    "predicted_date_low": "2027-01-01",
    "predicted_date_high": "2031-12-31",
    "predicted_date_best": "2029-06-15",
    "predicted_year_low": 2027,
    "predicted_year_high": 2031,
    "predicted_year_best": 2029,
    "prediction_type": "AGI",
    "confidence_level": "High confidence with expectation that AGI will 'whoosh by' with surprisingly minimal societal disruption",
    "confidence_label": "Casually Confident",
    "confidence_type": "high",
    "concept_keys": [
      "agi",
      "soft-takeoff",
      "economic-singularity",
      "alignment",
      "industry-academia-divergence"
    ],
    "criteria_definition": "AGI as systems outperforming humans at most economically valuable work, per OpenAI's official definition",
    "source_name": "Reddit AMA and interviews (late 2024)",
    "source_url": "https://www.windowscentral.com/software-apps/sam-altman-claims-agi-will-whoosh-by-in-5-years-with-surprisingly-little-societal-change-while-anthropic-ceo-predicts-a-2026-or-2027-breakthrough-theres-no-ceiling-below-the-level-of-humans-theres-a-lot-of-room-at-the-top-for-ais",
    "headline": "Altman Promises AGI By 2029 Will 'Whoosh By' With 'Surprisingly Little' Chaos",
    "headline_slug": "altman-promises-agi-by-2029-with-little-chaos",
    "tldr_summary": "In a December 2024 Reddit AMA, Sam Altman predicted AGI arrives by 2029 and will 'whoosh by' with 'surprisingly little' societal change—a deliberate counter to doomsday narratives. The framing positions AGI as a gradual transition rather than a discontinuous rupture, which conveniently aligns with OpenAI's commercial interests in normalizing increasingly capable AI. Anthropic CEO Dario Amodei offered a similar 2026-2027 timeline, creating a competitive dynamic where AI lab leaders race to claim the earliest plausible AGI date. Altman's prediction relies on OpenAI's specific AGI definition: systems outperforming humans at 'most economically valuable work'—a bar that sidesteps harder questions about consciousness, understanding, or general reasoning. The 'whoosh by' language suggests AGI will be less dramatic than expected, possibly because we're already living through the transition without recognizing it. Critics argue this framing serves to minimize concerns about unemployment, safety, and concentration of power while maximizing investor enthusiasm. The 5-year timeline (2024-2029) is aggressive but not absurd given current progress, though it assumes no major technical barriers emerge.",
    "target_date": "2029-06-15T00:00:00Z"
  },
  {
    "id": 157,
    "predictor_name": "Sam Altman",
    "predictor_type": "Individual",
    "prediction_date": "2025-01-15",
    "predicted_date_low": "2025-01-01",
    "predicted_date_high": "2030-12-31",
    "predicted_date_best": "2027-06-15",
    "predicted_year_low": 2025,
    "predicted_year_high": 2030,
    "predicted_year_best": 2027,
    "prediction_type": "AGI",
    "confidence_level": "Very high confidence expressed as 'we now know how to build AGI' suggesting the problem is essentially solved",
    "confidence_label": "Mission Accomplished",
    "confidence_type": "certain",
    "concept_keys": [
      "agi",
      "scaling-hypothesis",
      "economic-singularity",
      "alignment",
      "industry-academia-divergence"
    ],
    "criteria_definition": "AGI as traditionally understood by OpenAI—highly autonomous systems outperforming humans at most economically valuable work",
    "source_name": "Personal blog post reported by TIME Magazine (January 2025)",
    "source_url": "https://time.com/7205596/sam-altman-superintelligence-agi/",
    "headline": "Altman's Stunning Pivot: 'We Know How To Build AGI,' Now Chasing Superintelligence",
    "headline_slug": "altmans-pivotal-shift-to-superintelligence",
    "tldr_summary": "In a January 2025 blog post titled 'Reflections,' Sam Altman declared 'We are now confident we know how to build AGI as we have traditionally understood it' and announced OpenAI's pivot to superintelligence research. The statement implies AGI is essentially solved—just needs execution—and the real challenge is ASI alignment. This represents a major rhetorical shift from 'racing toward AGI' to 'AGI is basically done, moving on to the hard part.' Altman indicated AGI might arrive during Trump's term (2025-2029) while noting the term has 'become very sloppy.' The framing suggests OpenAI has cracked the core algorithmic and architectural challenges, with remaining work being primarily engineering and scaling. Reactions ranged from 'he's seen the o3 scaling curves and knows something we don't' to 'classic Altman overpromising for fundraising purposes.' The pivot to superintelligence research positions OpenAI as already past the AGI milestone that competitors are still chasing, though whether this reflects genuine technical confidence or strategic positioning remains unclear. The timeline suggests AGI deployment between 2025-2027, with superintelligence work beginning immediately rather than waiting for AGI to be fully realized.",
    "target_date": "2027-06-15T00:00:00Z"
  },
  {
    "id": 158,
    "predictor_name": "Sam Altman",
    "predictor_type": "AI Entrepreneur",
    "prediction_date": "2025-01-15",
    "predicted_date_low": "2026-01-01",
    "predicted_date_high": "2027-12-31",
    "predicted_date_best": "2026-06-15",
    "predicted_year_low": 2026,
    "predicted_year_high": 2027,
    "predicted_year_best": 2026,
    "prediction_type": "Transformative AI",
    "confidence_level": "Initially high confidence at DevDay 2024, moderated to 'beginning to work' by December 2025 after reality check",
    "confidence_label": "Enthusiastically Premature",
    "confidence_type": "medium",
    "concept_keys": [
      "transformative-ai"
    ],
    "criteria_definition": "AI agents capable of autonomous multi-day task execution joining workforces and materially changing company output",
    "source_name": "OpenAI December 2025 statements",
    "source_url": "https://blog.samaltman.com/reflections",
    "headline": "Altman's 2025 Agent Revolution Fizzles: 'Joining Workforce' Becomes 'Beginning To Work'",
    "headline_slug": "altmans-2025-agent-revolution-loses-steam",
    "tldr_summary": "Sam Altman predicted in early 2025 that AI agents would 'join the workforce' and materially change company output, following his October 2024 DevDay declaration that '2025 is when agents will work.' By December 2025, reality delivered a humbling lesson—the language shifted to agents 'beginning to work' on real problems, suggesting slower progress than anticipated. Products like ChatGPT's agent features fell short of autonomous workforce integration, hitting reliability issues, context failures, and trust barriers. The prediction followed a classic hype cycle: bold proclamations in Q4 2024, high expectations in Q1 2025, underwhelming reality through mid-year, and revised messaging by year-end. AI agents are distinct from chatbots in their supposed autonomy—you give them a goal and they execute multi-step workflows independently, potentially for days. But the gap between demo-quality agents and production-ready systems proved larger than expected. The revised 'beginning to work' framing acknowledges early-stage adoption in narrow domains while backing away from transformative claims. It mirrors patterns from self-driving cars ('next year' for a decade) and other transformative technologies that take longer to mature than initial excitement suggests.",
    "target_date": "2026-06-15T00:00:00Z"
  },
  {
    "id": 159,
    "predictor_name": "Sam Altman",
    "predictor_type": "AI Entrepreneur",
    "prediction_date": "2025-01-15",
    "predicted_date_low": "2035-01-01",
    "predicted_date_high": "2035-12-31",
    "predicted_date_best": "2035-06-15",
    "predicted_year_low": 2035,
    "predicted_year_high": 2035,
    "predicted_year_best": 2035,
    "prediction_type": "Superintelligence",
    "confidence_level": "High confidence in eventual arrival with acknowledgment that timeline could extend beyond 'few thousand days'",
    "confidence_label": "Inevitably Confident",
    "confidence_type": "high",
    "concept_keys": [
      "superintelligence",
      "accelerating-change",
      "scaling-hypothesis",
      "event-horizon"
    ],
    "criteria_definition": "Superintelligence as AI systems dramatically surpassing all human cognitive capabilities across every domain",
    "source_name": "The Intelligence Age blog 2025",
    "source_url": "https://blog.samaltman.com/reflections",
    "headline": "Altman Extends Superintelligence Timeline To 2035: 'Event Horizon Already Passed'",
    "headline_slug": "altman-extends-superintelligence-timeline",
    "tldr_summary": "While Sam Altman's September 2024 'few thousand days' comment suggested superintelligence around 2032, his January 2025 writings acknowledge the timeline 'may take longer'—pushing the estimate toward 2035 as a reasonable upper bound. The framing positions humanity as having already passed the point of no return: 'This may turn out to be the most consequential fact about all of history so far.' Altman's confidence rests on deep learning's predictable scaling behavior and society's increasing resource dedication to AI development. The 2035 timeline assumes continued exponential progress without major technical barriers, regulatory intervention, or resource constraints. It's notably more conservative than his AGI predictions, reflecting the gap between human-level and vastly-superhuman intelligence. The 'event horizon' language suggests we're already committed to the trajectory regardless of individual decisions—a deterministic framing that minimizes agency while maximizing urgency. Critics note this serves OpenAI's narrative of inevitable transformation requiring massive investment, while skeptics question whether scaling alone delivers superintelligence or hits fundamental limits. The 2035 date is far enough away to be unfalsifiable in the near term but close enough to drive current decision-making and capital allocation.",
    "target_date": "2035-06-15T00:00:00Z"
  },
  {
    "id": 160,
    "predictor_name": "Sam Altman",
    "predictor_type": "AI Entrepreneur",
    "prediction_date": "2025-01-15",
    "predicted_date_low": "2025-01-01",
    "predicted_date_high": "2025-12-31",
    "predicted_date_best": "2025-06-15",
    "predicted_year_low": 2025,
    "predicted_year_high": 2025,
    "predicted_year_best": 2025,
    "prediction_type": "Transformative AI",
    "confidence_level": "Moderate confidence with expectation of rapid iteration and learning from deployment",
    "confidence_label": "Optimistically Vague",
    "confidence_type": "medium",
    "concept_keys": [
      "transformative-ai"
    ],
    "criteria_definition": "AI agents materially joining workforces in 2025 with measurable impact on company output and productivity",
    "source_name": "OpenAI blog posts",
    "source_url": "https://blog.samaltman.com/reflections",
    "headline": "2025: The Year AI Agents Become Coworkers (Definition Of 'Material' May Vary)",
    "headline_slug": "ai-agents-as-workplace-coworkers-in-2025",
    "tldr_summary": "Sam Altman's January 2025 prediction that 'AI agents join the workforce' in 2025 represented peak optimism about autonomous AI systems. The vision: agents that work independently for days, managing complex workflows without human intervention, fundamentally changing how companies operate. By year-end, the reality was more nuanced—some narrow agent deployments showed promise, but the transformative 'coworker' scenario remained aspirational. Altman's blog posts about Sora and other products emphasized 'high rate of change' and learning from deployment, suggesting an iterative approach rather than sudden transformation. The agent vision includes booking travel, managing emails, conducting research, and executing multi-step business processes autonomously. But integration challenges, reliability issues, and trust barriers slowed adoption beyond pilot programs. Whether agents 'materially changed' company output in 2025 depends heavily on how you define 'material'—some productivity gains in narrow domains versus wholesale workforce transformation. The prediction captures OpenAI's strategic positioning around agents as the next major product category after chatbots, while the hedged language ('may see') provided escape hatches if reality fell short of vision.",
    "target_date": "2025-06-15T00:00:00Z"
  },
  {
    "id": 161,
    "predictor_name": "Samotsvety Forecasting",
    "predictor_type": "Survey",
    "prediction_date": "2023-01-21",
    "predicted_date_low": "2028-01-01",
    "predicted_date_high": "2100-12-31",
    "predicted_date_best": "2041-01-01",
    "predicted_year_low": 2028,
    "predicted_year_high": 2100,
    "predicted_year_best": 2041,
    "prediction_type": "AGI",
    "confidence_level": "Aggregate forecast from multiple superforecasters with established track records on prediction markets; 50% confidence interval spans 2035-2047",
    "confidence_label": "Professionally Hedged",
    "confidence_type": "medium",
    "concept_keys": [
      "agi",
      "turing-test",
      "survey-drift"
    ],
    "criteria_definition": "System capable of passing adversarial Turing test against top-5% human with expert access",
    "source_name": "EA Forum - Samotsvety AGI Timelines",
    "source_url": "https://80000hours.org/2025/03/when-do-experts-expect-agi-to-arrive/",
    "headline": "Elite Forecasters Give 28% Chance AGI Arrives by 2030, 50% by 2041",
    "headline_slug": "elite-forecasters-predict-agi-arrival-chances",
    "tldr_summary": "Samotsvety, a superforecasting collective known for winning prediction markets rather than building AI, delivered their professional estimate: 28% probability of AGI by 2030, 50% by 2041, and a long tail stretching to 2100. Unlike researchers who might have institutional incentives to hype or downplay timelines, these are people who literally bet money on being right. Their relatively aggressive timeline (compared to pre-ChatGPT surveys) reflects how even professional skeptics recalibrated after large language models surprised everyone. The 72-year span between their 10th and 90th percentile estimates captures genuine uncertainty about whether this happens in 5 years or requires fundamental breakthroughs we haven't imagined yet.",
    "target_date": "2041-01-01T00:00:00Z"
  },
  {
    "id": 162,
    "predictor_name": "Samotsvety Forecasting",
    "predictor_type": "Survey",
    "prediction_date": "2023-01-21",
    "predicted_date_low": "2026-01-01",
    "predicted_date_high": "2164-12-31",
    "predicted_date_best": "2041-01-01",
    "predicted_year_low": 2026,
    "predicted_year_high": 2164,
    "predicted_year_best": 2041,
    "prediction_type": "AGI",
    "confidence_level": "Mean aggregate with 10% this year, 50% by 2041, 90% by 2164; 138-year confidence interval reflects extreme uncertainty among individual forecasters",
    "confidence_label": "Wildly Uncertain",
    "confidence_type": "low",
    "concept_keys": [
      "agi",
      "turing-test",
      "survey-drift"
    ],
    "criteria_definition": "System passing adversarial Turing test with top-5% human judge having expert access",
    "source_name": "EA Forum - Update to Samotsvety AGI Timelines (January 2026)",
    "source_url": "https://forum.effectivealtruism.org/posts/ByBBqwRXWqX5m9erL/update-to-samotsvety-agi-timelines",
    "headline": "Superforecasters: 10% AGI This Year, But Also Maybe Not Until 2164",
    "headline_slug": "superforecasters-share-radical-agi-timeline",
    "tldr_summary": "In their January 2023 update, Samotsvety's elite forecasting team assigned 10% probability to AGI arriving within the year—notably higher than most experts would stomach for such near-term AGI. But the real story is their breathtaking uncertainty: the gap between their 10% and 90% confidence bounds spans 138 years, from 2026 to 2164. This isn't wishy-washy hedging; it's professional forecasters honestly admitting they have no idea if this is five years away or requires centuries of breakthroughs. Individual forecasters ranged from aggressive (F1 giving 39% by 2030) to conservative (F9 at 23%), but all agreed on one thing: massive uncertainty. The geometric mean settled on 2041 as the median, making this simultaneously one of the most aggressive and most uncertain AGI forecasts on record.",
    "target_date": "2041-01-01T00:00:00Z"
  },
  {
    "id": 163,
    "predictor_name": "Samotsvety Superforecasters",
    "predictor_type": "Forecasting Team",
    "prediction_date": "2023-12-01",
    "predicted_date_low": "2029-01-01",
    "predicted_date_high": "2041-12-31",
    "predicted_date_best": "2041-01-01",
    "predicted_year_low": 2029,
    "predicted_year_high": 2041,
    "predicted_year_best": 2041,
    "prediction_type": "AGI",
    "confidence_level": "Aggregate superforecaster consensus with 28% by 2030 and 50% by 2041",
    "confidence_label": "Survey Says Maybe",
    "confidence_type": "medium",
    "concept_keys": [
      "agi",
      "turing-test"
    ],
    "criteria_definition": "AGI defined by adversarial Turing test capability against expert-assisted judges",
    "source_name": "Samotsvety Superforecasters",
    "source_url": "https://80000hours.org/2025/03/when-do-experts-expect-agi-to-arrive/",
    "headline": "Superforecasters Play It Safe: 50-50 Bet on 2041 AGI",
    "headline_slug": "superforecasters-split-on-2041-agi-milestone",
    "tldr_summary": "While AI labs race to announce increasingly aggressive timelines, professional superforecasters—the people who actually make money being right about the future—put their median estimate at 2041 for AGI. This December 2023 forecast represents the cautious middle ground: 28% chance by 2030 (acknowledging recent progress could accelerate), but 50% by 2041 (recognizing hard problems remain). Unlike researchers embedded in the hype cycle or executives with products to sell, Samotsvety's track record comes from prediction markets where overconfidence costs real money. Their relatively conservative timeline suggests that once you remove career incentives and marketing pressure, the smart money still thinks AGI is a couple decades away, not a couple years.",
    "target_date": "2041-01-01T00:00:00Z"
  },
  {
    "id": 164,
    "predictor_name": "Satya Nadella",
    "predictor_type": "Tech Executive",
    "prediction_date": "2025-01-15",
    "predicted_date_low": "2030-01-01",
    "predicted_date_high": "2045-12-31",
    "predicted_date_best": "2037-06-15",
    "predicted_year_low": 2030,
    "predicted_year_high": 2045,
    "predicted_year_best": 2037,
    "prediction_type": "AGI",
    "confidence_level": "Explicitly dismissive of AGI milestone fixation; focuses on practical utility over theoretical thresholds",
    "confidence_label": "Deliberately Vague",
    "confidence_type": "none",
    "concept_keys": [
      "agi"
    ],
    "criteria_definition": "Rejects AGI as meaningful milestone; emphasizes incremental practical utility over general intelligence threshold",
    "source_name": "Multiple 2025 interviews",
    "source_url": "https://www.windowscentral.com/artificial-intelligence/microsoft-ceo-satya-nadella-thoughts-on-agi-winners-curse",
    "headline": "Microsoft CEO Dismisses AGI Hype: 'Winner's Curse' Awaits Model Companies",
    "headline_slug": "nadella-warns-of-ais-winners-curse",
    "tldr_summary": "While competitors race to declare AGI timelines, Microsoft CEO Satya Nadella threw cold water on the entire enterprise, warning of a \"Winner's Curse\" for companies obsessing over AGI milestones. In a January 2025 interview, Nadella argued the industry's fixation on achieving artificial general intelligence is a distraction from actually making useful products. He suggested AGI could be 5-20 years away but emphasized that pinning down exact dates misses the point entirely. This is classic Microsoft strategy: let OpenAI and Google burn resources chasing moonshots while Microsoft focuses on selling AI tools to enterprises today. Nadella's refusal to play the timeline game is either wisdom or clever positioning—probably both. His message to AI companies: stop measuring yourselves by arbitrary milestones and start measuring by whether anyone actually pays for your product.",
    "target_date": "2037-06-15T00:00:00Z"
  },
  {
    "id": 165,
    "predictor_name": "Satya Nadella",
    "predictor_type": "Tech Executive",
    "prediction_date": "2026-01-08",
    "predicted_date_low": "2026-01-01",
    "predicted_date_high": "2026-12-31",
    "predicted_date_best": "2026-06-15",
    "predicted_year_low": 2026,
    "predicted_year_high": 2026,
    "predicted_year_best": 2026,
    "prediction_type": "Transformative AI",
    "confidence_level": "High confidence that 2026 marks practical deployment inflection point for AI utility",
    "confidence_label": "Pragmatically Certain",
    "confidence_type": "high",
    "concept_keys": [
      "transformative-ai"
    ],
    "criteria_definition": "Year when AI transitions from experimental technology to practical everyday tool with measurable productivity impact",
    "source_name": "Microsoft CEO 2026 statements",
    "source_url": "https://www.hackdiversity.com/microsoft-ceo-satya-nadella-says-2026/",
    "headline": "Nadella's 2026 Prediction: Year AI Actually Does Something Useful",
    "headline_slug": "nadella-predicts-ais-practical-breakthrough-year",
    "tldr_summary": "While dismissing AGI hype as a distraction, Satya Nadella made a specific prediction for 2026: this will be the year AI stops being a science project and starts being genuinely useful in daily work. Not AGI, not superintelligence, just AI that actually helps people get things done. In a January 2026 blog post, Nadella argued the industry has spent years building powerful tools that mostly collect dust because they're too complex or solve the wrong problems. He believes 2026 marks the inflection point where AI becomes as mundane and essential as email—something people use without thinking about it. This is classic Nadella pragmatism: forget the moonshot rhetoric, focus on whether your technology saves someone time on Tuesday afternoon. The prediction doubles as a mission statement for Microsoft's AI strategy: win by making AI boring and useful, not by chasing theoretical milestones.",
    "target_date": "2026-06-15T00:00:00Z"
  },
  {
    "id": 166,
    "predictor_name": "Sergey Brin",
    "predictor_type": "Tech Executive",
    "prediction_date": "2025-05-21",
    "predicted_date_low": "2029-01-01",
    "predicted_date_high": "2030-12-31",
    "predicted_date_best": "2029-06-15",
    "predicted_year_low": 2029,
    "predicted_year_high": 2030,
    "predicted_year_best": 2029,
    "prediction_type": "AGI",
    "confidence_level": "High confidence with explicit intent to win AGI race; Brin says 'before 2030', Hassabis counters 'just after'",
    "confidence_label": "Competitively Aggressive",
    "confidence_type": "high",
    "concept_keys": [
      "agi"
    ],
    "criteria_definition": "AI matching or surpassing most human capabilities, with Gemini as the platform",
    "source_name": "Google I/O 2025",
    "source_url": "https://www.axios.com/2025/05/21/google-sergey-brin-demis-hassabis-agi-2030",
    "headline": "Sergey Brin Crashes Google I/O to Declare 'Gemini Will Be First AGI Before 2030'",
    "headline_slug": "sergey-brin-drops-bombshell-gemini-agi-prediction",
    "tldr_summary": "In a surprise, unscheduled appearance at Google I/O 2025, Sergey Brin crashed DeepMind CEO Demis Hassabis's fireside chat to make Google's most aggressive AGI claim ever: \"We fully intend that Gemini will be the very first AGI.\" When asked about timing, Brin shot back \"before 2030\" while Hassabis smiled and countered \"just after,\" prompting laughter about who was sandbagging whom. This marked the first time a Google executive explicitly declared intent to win the AGI race—territory usually dominated by OpenAI and Elon Musk. Brin's personal involvement is significant; the Google co-founder largely stepped back from operations years ago, and his surprise appearance signals this is personal, not just corporate strategy. The declaration transforms Google from cautious observer to aggressive competitor, with Gemini as their chosen platform and 2029-2030 as the target. Whether this is confidence or competitive posturing, Google just publicly committed to beating everyone to AGI.",
    "target_date": "2029-06-15T00:00:00Z"
  },
  {
    "id": 167,
    "predictor_name": "Sergey Brin",
    "predictor_type": "Tech Executive",
    "prediction_date": "2025-05-21",
    "predicted_date_low": "2030-01-01",
    "predicted_date_high": "2030-12-31",
    "predicted_date_best": "2030-06-15",
    "predicted_year_low": 2030,
    "predicted_year_high": 2030,
    "predicted_year_best": 2030,
    "prediction_type": "Superintelligence",
    "confidence_level": "Strong conviction that algorithmic innovation matters more than compute scale for reaching superintelligence",
    "confidence_label": "Algorithm-Pilled Optimist",
    "confidence_type": "high",
    "concept_keys": [
      "superintelligence",
      "scaling-hypothesis"
    ],
    "criteria_definition": "Superintelligence achieved primarily through algorithmic breakthroughs rather than scaling compute",
    "source_name": "Google I/O 2025 discussion",
    "source_url": "https://www.bigtechnology.com/p/demis-hassabis-and-sergey-brin-on",
    "headline": "Brin: Algorithms Beat Compute for AGI, Every CS Student Should Drop Out Now",
    "headline_slug": "brin-claims-algorithms-trump-compute-for-agi",
    "tldr_summary": "In the same Google I/O appearance where he declared Gemini would be first to AGI, Sergey Brin made a bolder claim about how we'll get there: algorithmic innovation matters more than raw compute. While the industry pours billions into bigger data centers and more powerful chips, Brin argued that historically, algorithmic advances have outpaced hardware improvements in solving hard problems like N-body simulations. This isn't just academic theorizing—it's a strategic bet that Google's research talent matters more than whoever has the biggest GPU cluster. Brin went further, declaring that \"anybody who's a computer scientist should not be retired right now. They should be working on AI.\" Translation: this is the most important problem in computer science history, and if you're not working on it, you're missing the main event. Whether Brin is right that algorithms trump compute remains to be seen, but his conviction is clear: the path to superintelligence runs through better ideas, not just bigger machines.",
    "target_date": "2030-06-15T00:00:00Z"
  },
  {
    "id": 168,
    "predictor_name": "Shane Legg",
    "predictor_type": "AI Researcher",
    "prediction_date": "2023-10-26",
    "predicted_date_low": "2028-01-01",
    "predicted_date_high": "2028-12-31",
    "predicted_date_best": "2028-06-15",
    "predicted_year_low": 2028,
    "predicted_year_high": 2028,
    "predicted_year_best": 2028,
    "prediction_type": "AGI",
    "confidence_level": "50% confidence maintained consistently since 2009; described as personal median estimate",
    "confidence_label": "Stubbornly Consistent",
    "confidence_type": "medium",
    "concept_keys": [
      "agi"
    ],
    "criteria_definition": "Machine capable of cognitive tasks people can typically do, meeting human performance across broad range of tests",
    "source_name": "Dwarkesh podcast January 2026",
    "source_url": "https://www.dwarkesh.com/p/shane-legg",
    "headline": "DeepMind Co-Founder's 2028 Prediction: Right on Schedule or Just Lucky?",
    "headline_slug": "deepmind-co-founders-persistent-agi-timeline",
    "tldr_summary": "Shane Legg, DeepMind co-founder and Chief AGI Scientist, has maintained 50% odds for AGI by 2028 since he first made the prediction in 2009. That's 14 years of refusing to update his timeline despite AlexNet, GPT-3, ChatGPT, and every other breakthrough that made other forecasters radically revise their estimates. In an October 2023 podcast, Legg explained his definition: AGI means machines that can do the cognitive tasks people typically do, possibly more. He acknowledged the difficulty of measuring progress toward such a fuzzy target, suggesting we need a broad range of tests spanning human cognitive abilities. The question is whether Legg's consistency reflects genuine insight into the pace of AI progress or whether he just picked a number and stuck with it. Either way, with 2028 now just a few years away, we're about to find out if his 15-year-old prediction was perfectly calibrated or just a lucky guess that's about to be proven wildly wrong.",
    "target_date": "2028-06-15T00:00:00Z"
  },
  {
    "id": 169,
    "predictor_name": "Stuart Russell",
    "predictor_type": "AI Researcher",
    "prediction_date": "2025-01-01",
    "predicted_date_low": "2044-01-01",
    "predicted_date_high": "2044-12-31",
    "predicted_date_best": "2044-06-15",
    "predicted_year_low": 2044,
    "predicted_year_high": 2044,
    "predicted_year_best": 2044,
    "prediction_type": "AGI",
    "confidence_level": "Low confidence; estimates 30% chance current AI paradigm cannot achieve AGI without fundamental breakthroughs",
    "confidence_label": "Academically Skeptical",
    "confidence_type": "low",
    "concept_keys": [
      "agi"
    ],
    "criteria_definition": "AGI requiring multiple fundamental breakthroughs beyond current deep learning paradigm",
    "source_name": "Diary of a CEO podcast / 80,000 Hours",
    "source_url": "https://80000hours.org/podcast/episodes/stuart-russell-human-compatible-ai/",
    "headline": "AI Textbook Author: AGI Needs 19 Years and We Might Be Doing It Wrong",
    "headline_slug": "ai-textbook-author-predicts-uncertain-agi-path",
    "tldr_summary": "Stuart Russell, whose AI textbook has educated more than 1,500 universities in 135 countries, thinks AGI is about 19 years away—and he's not even confident we're on the right path to get there. In an 80,000 Hours podcast, Russell suggested roughly 30% odds that AGI cannot be built under the current deep learning paradigm, requiring fundamental architectural breakthroughs we haven't discovered yet. This is notably more skeptical than most predictions from people actively building AI systems, which makes sense: Russell has spent decades studying the field's limitations, not just its successes. His 2044 timeline reflects both technical skepticism (current approaches may hit walls) and philosophical concern (we're building systems we don't understand how to control). Unlike researchers racing to the next benchmark, Russell approaches AGI as an academic problem that might not have a solution on the current path. Whether this makes him a wise voice of caution or someone who will be proven wrong by empirical progress remains to be seen.",
    "target_date": "2044-06-15T00:00:00Z"
  },
  {
    "id": 170,
    "predictor_name": "Stuart Russell",
    "predictor_type": "AI Researcher",
    "prediction_date": "2025-12-19",
    "predicted_date_low": "2030-01-01",
    "predicted_date_high": "2030-12-31",
    "predicted_date_best": "2030-06-15",
    "predicted_year_low": 2030,
    "predicted_year_high": 2030,
    "predicted_year_best": 2030,
    "prediction_type": "Transformative AI",
    "confidence_level": "High confidence in near-term transformative AI with extinction-level risk; characterizes current trajectory as 'Russian Roulette'",
    "confidence_label": "Urgently Alarmed",
    "confidence_type": "high",
    "concept_keys": [
      "transformative-ai",
      "alignment"
    ],
    "criteria_definition": "Transformative AI capable of replacing humans by 2030, developed through reckless corporate race",
    "source_name": "Newsweek article (January 2025)",
    "source_url": "https://blog.dragansr.com/2025/12/ai-expert-warning-stuart-russell.html",
    "headline": "Russell Warns: AI Race Is 'Russian Roulette' With Extinction-Level Stakes by 2030",
    "headline_slug": "russell-sounds-alarm-on-ai-extinction-risk",
    "tldr_summary": "Stuart Russell, the Berkeley professor who literally wrote the textbook on AI, published a stark warning in December 2025: the trillion-dollar race to AGI could replace humans by 2030, and governments are failing to stop it. Russell characterizes the current situation as the 'Gorilla Problem'—by creating something more intelligent than ourselves, humans are voluntarily becoming like gorillas, whose survival depends entirely on whether the superior species cares to preserve them. He argues AI doesn't need consciousness or evil intent to destroy us, just superior competence at achieving goals that conflict with human survival. Russell reserves particular criticism for industry leaders like Sam Altman and Elon Musk, who have signed statements acknowledging AGI as an extinction risk yet continue racing toward it anyway. He calls this 'Russian Roulette' driven by greed, suggesting only a nuclear-level AI catastrophe will wake up regulators. Russell's 2030 timeline for transformative AI is notably more aggressive than his earlier 19-year AGI estimate, reflecting growing alarm at the pace of development and absence of safety measures.",
    "target_date": "2030-06-15T00:00:00Z"
  },
  {
    "id": 171,
    "predictor_name": "Sundar Pichai",
    "predictor_type": "Tech Executive",
    "prediction_date": "2025-06-07",
    "predicted_date_low": "2030-01-01",
    "predicted_date_high": "2035-12-31",
    "predicted_date_best": "2030-06-01",
    "predicted_year_low": 2030,
    "predicted_year_high": 2035,
    "predicted_year_best": 2030,
    "prediction_type": "AGI",
    "confidence_level": "Cautiously optimistic but deliberately vague on specifics, using phrases like 'just fall short' to avoid firm commitment",
    "confidence_label": "Diplomatically Hedging",
    "confidence_type": "medium",
    "concept_keys": [
      "agi"
    ],
    "criteria_definition": "AGI defined as systems approaching but not quite achieving full human-level general intelligence, with Pichai coining 'Artificial Jagged Intelligence' to describe uneven capabilities",
    "source_name": "Lex Fridman podcast June 2025",
    "source_url": "https://blog.biocomm.ai/2025/06/07/google-ceo-will-agi-be-created-by-2030-sundar-pichai-and-lex-fridman-lex-clips/",
    "headline": "Google CEO Predicts We'll 'Just Fall Short' of AGI by 2030, Coins 'Artificial Jagged Intelligence' To Explain Why",
    "headline_slug": "google-ceo-predicts-near-miss-on-agi",
    "tldr_summary": "Sundar Pichai told Lex Fridman that humanity will 'just fall short' of achieving AGI by 2030, a carefully hedged prediction that lets him claim partial credit either way. The Google CEO introduced the concept of 'Artificial Jagged Intelligence' (AJI) to describe AI systems with wildly uneven capabilities—superhuman at some tasks, incompetent at others. It's a diplomatic framework that acknowledges current AI's spiky performance profile while avoiding hard commitments about when we'll actually achieve the real thing. Pichai's timeline puts AGI arrival somewhere between 2030-2035, conveniently far enough to avoid immediate accountability but close enough to justify massive current investments. The 'just fall short' framing is peak executive hedging: if we hit AGI by 2030, he was close; if we don't, he technically called it. Meanwhile, Google continues racing OpenAI and Anthropic in the high-stakes game of who builds God first.",
    "target_date": "2030-06-01T00:00:00Z"
  },
  {
    "id": 172,
    "predictor_name": "Susan Schneider",
    "predictor_type": "Philosopher",
    "prediction_date": "2024-01-01",
    "predicted_date_low": "2040-01-01",
    "predicted_date_high": "2100-12-31",
    "predicted_date_best": "2070-01-01",
    "predicted_year_low": 2040,
    "predicted_year_high": 2100,
    "predicted_year_best": 2070,
    "prediction_type": "Superintelligence",
    "confidence_level": "High confidence that consciousness presents fundamental barriers that may never be solved, based on philosophical analysis rather than technical timelines",
    "confidence_label": "Philosophically Skeptical",
    "confidence_type": "high",
    "concept_keys": [
      "superintelligence",
      "alignment"
    ],
    "criteria_definition": "Machine consciousness and superintelligence requiring solution to the hard problem of consciousness and subjective experience",
    "source_name": "Philosophy of AI consciousness",
    "source_url": "https://en.wikipedia.org/wiki/Susan_Schneider",
    "headline": "Philosopher Says Machine Consciousness Faces Unsolvable 'Hard Problem'—Sorry, Tech Bros",
    "headline_slug": "machine-consciousness-faces-philosophical-barrier",
    "tldr_summary": "Susan Schneider, founding director of the Center for the Future of AI, Mind, & Society at Florida Atlantic University, throws cold water on the idea that we're anywhere close to conscious AI. The philosopher and former NASA/Library of Congress Chair in Astrobiology argues that building truly conscious artificial intelligence faces fundamental philosophical barriers that silicon valley types conveniently ignore. The 'hard problem of consciousness'—explaining why subjective experience exists at all—remains completely unsolved, and Schneider suggests it may be unsolvable within our current frameworks. While tech executives race to build AGI and superintelligence, she points out they're essentially trying to create something (consciousness) that we don't even understand in biological systems. Her work bridges philosophy of mind and AI safety, asking uncomfortable questions like 'what is it like to be an AI?' that have no clear answers. If consciousness is actually required for true general intelligence, we might be building very sophisticated calculators while claiming they're minds.",
    "target_date": "2070-01-01T00:00:00Z"
  },
  {
    "id": 173,
    "predictor_name": "Thomas Nagel",
    "predictor_type": "Philosopher",
    "prediction_date": "2024-01-01",
    "predicted_date_low": "2050-01-01",
    "predicted_date_high": "2100-12-31",
    "predicted_date_best": "2075-01-01",
    "predicted_year_low": 2050,
    "predicted_year_high": 2100,
    "predicted_year_best": 2075,
    "prediction_type": "Superintelligence",
    "confidence_level": "Philosophical skepticism about whether consciousness and subjective experience can be computationally replicated at all",
    "confidence_label": "Existentially Uncertain",
    "confidence_type": "low",
    "concept_keys": [
      "superintelligence"
    ],
    "criteria_definition": "Superintelligence requiring subjective experience and consciousness, not merely computational power",
    "source_name": "Philosophy of mind",
    "source_url": "https://en.wikipedia.org/wiki/Thomas_Nagel",
    "headline": "'What Is It Like To Be AGI?' Philosopher's Famous Bat Question Haunts AI Predictions",
    "headline_slug": "philosopher-explores-ai-subjective-experience",
    "tldr_summary": "Thomas Nagel's legendary 1974 essay 'What Is It Like to Be a Bat?' accidentally became one of the most important thought experiments for AI consciousness. The philosopher argued that subjective experience—the 'what it's like' quality of consciousness—can't be reduced to physical processes we can observe from the outside. You can study a bat's echolocation neurology all day, but you'll never know what it's like to BE a bat experiencing echolocation. This creates an awkward problem for AGI enthusiasts: if we can't even verify consciousness in biological systems besides ourselves, how would we know if we've created it in silicon? Nagel's work suggests that consciousness and subjective experience might be fundamentally different from computational processes, meaning true superintelligence (if it requires consciousness) could be much further away than tech optimists think. His philosophical skepticism doesn't provide specific timelines—it questions whether the entire project is conceptually coherent. While AI labs race toward AGI, Nagel's bat quietly asks: 'But will your system actually experience anything, or just process information really well?'",
    "target_date": "2075-01-01T00:00:00Z"
  },
  {
    "id": 174,
    "predictor_name": "Timnit Gebru",
    "predictor_type": "AI Researcher",
    "prediction_date": "2023-02-15",
    "predicted_date_low": null,
    "predicted_date_high": null,
    "predicted_date_best": null,
    "predicted_year_low": null,
    "predicted_year_high": null,
    "predicted_year_best": null,
    "prediction_type": "AGI",
    "confidence_level": "Extremely high confidence that AGI pursuit is inherently unsafe; rejects timeline framing entirely",
    "confidence_label": "Professionally Unimpressed",
    "confidence_type": "none175",
    "concept_keys": [
      "agi"
    ],
    "criteria_definition": "AGI is an undefined, unscoped system rooted in eugenics ideology; trying to build it is inherently unsafe regardless of timeline",
    "source_name": "The Stanford Daily",
    "source_url": "https://stanforddaily.com/2023/02/15/utopia-for-whom-timnit-gebru-on-the-dangers-of-artificial-general-intelligence/",
    "headline": "Fired-From-Google AI Ethicist Asks 'Utopia for Whom?' and Rejects Your AGI Timeline Entirely",
    "headline_slug": "fired-ai-ethicist-challenges-agi-assumptions",
    "tldr_summary": "Timnit Gebru — the computer scientist famously fired from Google in 2020 for co-authoring the 'Stochastic Parrots' paper on LLM risks — doesn't think AGI is coming in 2027, 2035, or ever, because she thinks the whole concept is bunk. Speaking at Stanford in February 2023, she asked the audience: 'Why attempt to build some undefined system that kind of sounds like a god?' She drew parallels between the AGI movement and first-wave eugenics, arguing both promise utopia while entrenching power hierarchies. While Silicon Valley debates extinction risk timelines, Gebru points to Kenyan content moderators paid $1.50/hour, artists whose work is scraped for training data, and biased facial recognition causing false arrests. She co-coined 'TESCREAL' to describe the overlapping ideologies — transhumanism, singularitarianism, effective altruism — she sees driving AGI hype. Her counter-vision: small, well-scoped, community-rooted AI systems. Whether you agree or not, she's the field's most prominent voice saying the emperor has no clothes.",
    "target_date": null
  },
  {
    "id": 176,
    "predictor_name": "Yann LeCun",
    "predictor_type": "AI Researcher",
    "prediction_date": "2025-01-23",
    "predicted_date_low": "2028-01-01",
    "predicted_date_high": "2032-12-31",
    "predicted_date_best": "2030-01-01",
    "predicted_year_low": 2028,
    "predicted_year_high": 2032,
    "predicted_year_best": 2030,
    "prediction_type": "Transformative AI",
    "confidence_level": "Very high confidence in paradigm shift, with LeCun stating 'nobody in their right mind would use them anymore' about current LLMs within five years",
    "confidence_label": "Confidently Revolutionary",
    "confidence_type": "high",
    "concept_keys": [
      "transformative-ai",
      "scaling-hypothesis"
    ],
    "criteria_definition": "New AI paradigm based on world models enabling physical understanding, persistent memory, reasoning, and complex planning—fundamentally different architecture from current LLMs",
    "source_name": "TechCrunch January 2025",
    "source_url": "https://techcrunch.com/2025/01/23/metas-yann-lecun-predicts-a-new-ai-architectures-paradigm-within-5-years-and-decade-of-robotics/",
    "headline": "Meta's AI Chief Declares LLMs Dead By 2030, Promises 'World Models' Revolution Instead",
    "headline_slug": "meta-ai-chief-declares-llm-extinction",
    "tldr_summary": "Yann LeCun dropped a bomb at Davos, declaring that large language models have a 'shelf life of three to five years' and predicting 'nobody in their right mind' will use them as central AI components by 2030. The Meta chief AI scientist and Turing Award winner isn't predicting AGI—he's predicting something potentially more practical: 'world models' that actually understand physical reality, maintain persistent memory, reason effectively, and plan complex actions. Current LLMs, he argues, are fundamentally limited by four critical gaps: no understanding of the physical world, no persistent memory, no real reasoning, and no complex planning. These aren't bugs to be fixed with more scale—they're architectural limitations requiring a complete paradigm shift. LeCun is putting his money where his mouth is: he's reportedly launching a world models startup seeking a $5B+ valuation, possibly leaving Meta to do it. His timeline suggests 2028-2030 as the inflection point when the new paradigm emerges, followed by a 'decade of robotics' as embodied AI finally works in the real world. If he's right, everyone betting on scaled-up LLMs is about to look very silly.",
    "target_date": "2030-01-01T00:00:00Z"
  },
  {
    "id": 177,
    "predictor_name": "Yann LeCun",
    "predictor_type": "AI Researcher",
    "prediction_date": "2025-01-23",
    "predicted_date_low": "2025-01-01",
    "predicted_date_high": "2025-12-31",
    "predicted_date_best": "2025-12-31",
    "predicted_year_low": 2025,
    "predicted_year_high": 2025,
    "predicted_year_best": 2025,
    "prediction_type": "Transformative AI",
    "confidence_level": "Stated as present reality rather than prediction, with high confidence that robotics revolution is beginning now",
    "confidence_label": "Declaratively Immediate",
    "confidence_type": "high",
    "concept_keys": [
      "transformative-ai",
      "scaling-hypothesis"
    ],
    "criteria_definition": "Decade of robotics beginning immediately, combining AI advances with robotics to enable intelligent physical applications in real-world environments",
    "source_name": "Meta AI chief statements January 2025",
    "source_url": "https://techcrunch.com/2025/01/23/metas-yann-lecun-predicts-a-new-ai-architectures-paradigm-within-5-years-and-decade-of-robotics/",
    "headline": "Yann LeCun Declares 2025 Start of 'Decade of Robotics'—Finally, Robots That Actually Work",
    "headline_slug": "yann-lecun-heralds-decade-of-robotics",
    "tldr_summary": "Forget AGI timelines—Yann LeCun says the real action is happening right now in robotics. Speaking at Davos, Meta's chief AI scientist declared that 2025 marks the beginning of the 'decade of robotics,' where AI advances finally converge with physical systems to create robots that can actually do useful things in the real world. This isn't about humanoid robots taking over; it's about AI systems that understand physical dynamics, can manipulate objects, navigate real environments, and complete long-horizon tasks without constant human intervention. LeCun's timing is strategic: his predicted 'world models' paradigm (2028-2030) will provide the cognitive architecture, but the robotics revolution starts now as companies race to build the hardware and integration layers. The prediction reflects growing consensus that embodied AI—systems that interact with physical reality—represents the next major frontier after language models. While ChatGPT impressed everyone with text generation, LeCun argues the transformative impact will come from AI that can fold laundry, navigate warehouses, assist in surgery, and handle physical tasks currently requiring human dexterity and judgment. If he's right, we're at the beginning of a decade that finally delivers on robotics promises made since the 1960s.",
    "target_date": "2025-12-31T00:00:00Z"
  },
  {
    "id": 178,
    "predictor_name": "Yann LeCun",
    "predictor_type": "AI Researcher",
    "prediction_date": "2025-01-22",
    "predicted_date_low": "2027-01-01",
    "predicted_date_high": "2027-12-31",
    "predicted_date_best": "2027-01-01",
    "predicted_year_low": 2027,
    "predicted_year_high": 2027,
    "predicted_year_best": 2027,
    "prediction_type": "Transformative AI",
    "confidence_level": "Moderate confidence based on observed innovation patterns and research culture differences between US and Chinese AI ecosystems",
    "confidence_label": "Geopolitically Competitive",
    "confidence_type": "medium",
    "concept_keys": [
      "transformative-ai",
      "economic-singularity"
    ],
    "criteria_definition": "True AI capabilities emerging from creative Chinese firms potentially outpacing Western approaches through different research paradigms and innovation models",
    "source_name": "Meta AI chief statements January 2025",
    "source_url": "https://www.analyticsvidhya.com/blog/2025/01/whos-ahead-in-the-ai-race-usa-or-china-yann-lecun-answers/",
    "headline": "LeCun Warns: Creative Chinese AI Firms Could Leapfrog West By 2027",
    "headline_slug": "lecun-warns-of-chinese-ai-leap",
    "tldr_summary": "In a prediction that will make DC national security types nervous, Yann LeCun suggested that creative Chinese AI firms might eventually outpace Western companies in developing true AI capabilities. Speaking about the global AI race, the Meta chief scientist noted that while the US currently leads in large language models and compute resources, Chinese companies are showing remarkable creativity in working around constraints like limited access to cutting-edge chips. LeCun's timeline suggests 2027 as a potential inflection point when Chinese approaches—potentially based on different architectural paradigms or more efficient training methods—could begin demonstrating advantages. This isn't just about who has more GPUs; it's about fundamental research directions and innovation models. Chinese firms face different regulatory environments, have access to different data sources, and operate under different competitive pressures than Western counterparts. LeCun's observation highlights that the AI race isn't simply about throwing more compute at the problem—it's about architectural innovation, and there's no guarantee Silicon Valley's approach is optimal. The prediction adds urgency to ongoing debates about AI export controls, international collaboration, and whether restricting Chinese access to chips might backfire by forcing innovation in more efficient directions.",
    "target_date": "2027-01-01T00:00:00Z"
  },
  {
    "id": 179,
    "predictor_name": "Yoshua Bengio",
    "predictor_type": "AI Pioneer",
    "prediction_date": "2024-10-30",
    "predicted_date_low": "2028-01-01",
    "predicted_date_high": "2043-12-31",
    "predicted_date_best": "2035-07-01",
    "predicted_year_low": 2028,
    "predicted_year_high": 2043,
    "predicted_year_best": 2035,
    "prediction_type": "AGI",
    "confidence_level": "High confidence with over 20% of Metaculus predictors estimating AGI before 2027, citing steady algorithmic progress and exponential R&D investment growth",
    "confidence_label": "Urgently Concerned",
    "confidence_type": "high",
    "concept_keys": [
      "agi",
      "recursive-self-improvement",
      "accelerating-change",
      "alignment"
    ],
    "criteria_definition": "AGI defined as human-level capabilities across broad spectrum of cognitive skills, with particular emphasis on AI systems capable of advancing AI research itself, potentially leading to rapid recursive self-improvement",
    "source_name": "Multiple 2024-2025 interviews",
    "source_url": "https://yoshuabengio.org/2024/10/30/implications-of-artificial-general-intelligence-on-national-and-international-security/",
    "headline": "AI Godfather Yoshua Bengio Shifts From 'Decades' To '5-20 Years' With 90% Confidence—Oops",
    "headline_slug": "ai-godfather-bengio-narrows-agi-timeline",
    "tldr_summary": "Yoshua Bengio, one of the three 'godfathers of AI' who won the Turing Award for deep learning breakthroughs, has dramatically shortened his AGI timeline and now estimates 5-20 years with high confidence—a striking shift from his earlier 'decades away' stance. Writing for the Aspen Strategy Group on national security implications, Bengio notes that over 20% of Metaculus predictors now estimate AGI before 2027, consistent with steady algorithmic advances and trillions in R&D investment. What makes his prediction particularly alarming: he emphasizes AI systems capable of advancing AI research itself, which could 'multiply the advanced research workforce by orders of magnitude.' Once an AI matches the capability of top researchers, it can be deployed in parallel across hundreds of thousands of instances, potentially triggering recursive self-improvement. Bengio warns that superintelligence could emerge 'months' after AGI through this self-improvement loop—a compressed timeline that makes alignment work desperately urgent. His shift from skeptic to concerned predictor carries weight: this isn't a Silicon Valley hype man, but a researcher who spent decades building the foundations that made modern AI possible, now warning that his own creations might arrive faster and more dangerously than anyone expected.",
    "target_date": "2035-07-01T00:00:00Z"
  },
  {
    "id": 180,
    "predictor_name": "Cam Pedersen",
    "predictor_type": "Individual",
    "prediction_date": "2026-02-10",
    "predicted_date_low": "2034-07-18",
    "predicted_date_high": "2034-07-18",
    "predicted_date_best": "2034-07-18",
    "predicted_year_low": 2034,
    "predicted_year_high": 2034,
    "predicted_year_best": 2034,
    "prediction_type": "Singularity",
    "confidence_level": "Speculative (self-described as 'unhinged')",
    "confidence_label": "Mathematically Unhinged",
    "confidence_type": "low",
    "concept_keys": [
      "event-horizon",
      "accelerating-change",
      "economic-singularity"
    ],
    "criteria_definition": "Phase transition when rate of AI emergent behavior discoveries exceeds human processing capacity (tst)",
    "source_name": "Personal blog analysis",
    "source_url": "https://campedersen.com/singularity",
    "headline": "Engineer Calculates Exact Singularity Date Using Hyperbolic Curve-Fitting: Tuesday July 18 2034",
    "headline_slug": "engineer-pins-down-ai-singularity-date",
    "tldr_summary": "Engineer and self-described optimist Cam Pedersen fitted hyperbolic curves to five AI progress metrics and discovered something unsettling: only one is actually going hyperbolic, and it's not machine capability. The singularity date of Tuesday July 18 2034 comes from tracking arXiv papers about AI emergence—meaning the metric accelerating toward infinity is human attention to AI surprises, not AI performance itself. The machines are improving linearly; we're the ones losing our minds exponentially. Pedersen defines his 'singularity' not as superintelligence but as tst: the moment when AI surprises arrive faster than humans can process them. Plot twist: while the math points to 2034, he argues the social consequences (labor displacement, institutional failure, epistemic collapse) are already happening in 2026. The real singularity is when you realize the singularity is a human perceptual crisis, not a robot apocalypse.",
    "target_date": "2034-07-18T00:00:00Z"
  }
]